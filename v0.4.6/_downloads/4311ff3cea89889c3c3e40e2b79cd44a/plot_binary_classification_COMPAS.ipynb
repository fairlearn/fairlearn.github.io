{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Binary Classification on COMPAS dataset\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting and preparing the data\n------------------------------\n\nTo demonstrate the post processing algorithm we use the \"COMPAS\" dataset from\n`ProPublica\n<https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv>`_.\nThe labels represent the two-year recidivism ID, i.e. whether a person got\nrearrested within two years (label 1) or not (label 0). The features include\nsex, age, as well as information on prior incidents.\n\nTo start, let's download the dataset using `tempeh`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\nfrom tempeh.configurations import datasets\n\ncompas_dataset = datasets[\"compas\"]()\nX_train, X_test = compas_dataset.get_X(format=pd.DataFrame)\ny_train, y_test = compas_dataset.get_y(format=pd.Series)\n(\n    sensitive_features_train,\n    sensitive_features_test,\n) = compas_dataset.get_sensitive_features(\"race\", format=pd.Series)\nX_train.loc[0], y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a fairness-unaware model\n-------------------------------\nFirst we set up a helper function that will help in analyzing the dataset as\nwell as predictions from our models. Feel free to skip to the next cell for\nthe actual logic.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\n\n# show_proportions is only a helper function for plotting\ndef show_proportions(\n    X, sensitive_features, y_pred, y=None, description=None, plot_row_index=1\n):\n    print(\"\\n\" + description)\n    plt.figure(plot_row_index)\n    plt.title(description)\n    plt.ylabel(\"P[recidivism predicted | conditions]\")\n\n    indices = {}\n    positive_indices = {}\n    negative_indices = {}\n    recidivism_count = {}\n    recidivism_pct = {}\n    groups = np.unique(sensitive_features.values)\n    n_groups = len(groups)\n    max_group_length = max([len(group) for group in groups])\n    color = cm.rainbow(np.linspace(0, 1, n_groups))\n    x_tick_labels_basic = []\n    x_tick_labels_by_label = []\n    for index, group in enumerate(groups):\n        indices[group] = sensitive_features.index[sensitive_features == group]\n        recidivism_count[group] = sum(y_pred[indices[group]])\n        recidivism_pct[group] = recidivism_count[group] / len(indices[group])\n        print(\n            \"P[recidivism predicted | {}]                {}= {}\".format(\n                group, \" \" * (max_group_length - len(group)), recidivism_pct[group]\n            )\n        )\n\n        plt.bar(index + 1, recidivism_pct[group], color=color[index])\n        x_tick_labels_basic.append(group)\n\n        if y is not None:\n            positive_indices[group] = sensitive_features.index[\n                (sensitive_features == group) & (y == 1)\n            ]\n            negative_indices[group] = sensitive_features.index[\n                (sensitive_features == group) & (y == 0)\n            ]\n            prob_1 = sum(y_pred[positive_indices[group]]) / len(positive_indices[group])\n            prob_0 = sum(y_pred[negative_indices[group]]) / len(negative_indices[group])\n            print(\n                \"P[recidivism predicted | {}, recidivism]    {}= {}\".format(\n                    group, \" \" * (max_group_length - len(group)), prob_1\n                )\n            )\n            print(\n                \"P[recidivism predicted | {}, no recidivism] {}= {}\".format(\n                    group, \" \" * (max_group_length - len(group)), prob_0\n                )\n            )\n\n            plt.bar(n_groups + 1 + 2 * index, prob_1, color=color[index])\n            plt.bar(n_groups + 2 + 2 * index, prob_0, color=color[index])\n            x_tick_labels_by_label.extend(\n                [\"{} recidivism\".format(group), \"{} no recidivism\".format(group)]\n            )\n\n    x_tick_labels = x_tick_labels_basic + x_tick_labels_by_label\n    plt.xticks(\n        range(1, len(x_tick_labels) + 1),\n        x_tick_labels,\n        rotation=45,\n        horizontalalignment=\"right\",\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get started we look at a very basic Logistic Regression model. We fit it\nto the training data and plot some characteristics of training and test data\nas well as the predictions of the model on those datasets.\n\nWe notice a stark contrast in the predictions with African-Americans being a\nlot more likely to be predicted to reoffend, similar to the original training\ndata. However, there's even a disparity between the subgroup of\nAfrican-Americans and Caucasians with recidivism. When considering only the\nsamples labeled with \"no recidivism\" African-Americans are much more likely\nto be predicted to reoffend than Caucasians. The test data shows a similar\ndisparity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n\nestimator = LogisticRegression(solver=\"liblinear\")\nestimator.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print and plot data from training and test set as well as predictions with\nfairness-unaware classifier on both sets show only test data related plots by\ndefault - uncomment the next two lines to see training data plots as well\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# show_proportions(X_train, sensitive_features_train, y_train,\n#                  description=\"original training data:\", plot_row_index=1)\n# show_proportions(X_train, sensitive_features_train,\n#                  estimator.predict(X_train), y_train,\n#                  description=\"fairness-unaware prediction on training data:\",\n#                  plot_row_index=2)\nshow_proportions(\n    X_test,\n    sensitive_features_test,\n    y_test,\n    description=\"original test data:\",\n    plot_row_index=3,\n)\nshow_proportions(\n    X_test,\n    sensitive_features_test,\n    estimator.predict(X_test),\n    y_test,\n    description=\"fairness-unaware prediction on test data:\",\n    plot_row_index=4,\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Postprocessing the model to get a fair model\n--------------------------------------------\n\nThe idea behind postprocessing is to alter the output of the fairness-unaware\nmodel to achieve fairness. The postprocessing algorithm requires three input\narguments:\n- the matrix of samples X\n- the vector of predictions y from the fairness-unaware model\n- the vector of group attribute values A (in the code we refer to it as `sensitive_features`)\n\nThe goal is to make the output fair with respect to constraints. The\npostprocessing algorithm uses one of\n\n- Demographic Parity (DP):\n  $P\\ [\\ h(X)=\\hat{y}\\ |\\ A=a] = P\\ [\\ h(X)=\\hat{y}\\ ] \\qquad \\forall a, \\hat{y}$\n- Equalized Odds (EO):\n  $P\\ [\\ h(X)=\\hat{y}\\ |\\ A=a, Y=y] = P\\ [\\ h(X)=\\hat{y}\\ |\\ Y=y\\ ] \\qquad \\forall a, \\hat{y}$\n\nwhere $h(X)$ is the prediction based on the input $X$,\n$\\hat{y}$ and $y$ are labels, and $a$ is a sensitive feature\nvalue. In this example, we'd expect the postprocessed model with DP to be\nbalanced between races. In this particular scenario it makes more sense to\naim at fairness through accuracy like EO. EO does not make the same\nguarantees. Instead, it ensures parity between the subgroups of each race\nwith label 1 in the training set, and parity between the subgroups of each\nrace with label 0 in the training set. Applied to this scenario, this means\nthat the subgroups of each race who reoffended in the past are equally likely\nto be predicted to reoffend (and therefore also equally likely not to).\nSimilarly, there is parity between subgroups of each race without recidivism,\nbut we have no parity between the groups with different training labels. In\nmathematical terms at the example of African-American and Caucasian:\n\n\\begin{align}P\\ [\\ \\text{recidivism predicted}\\ |\\ \\text{African-American, recidivism}\\ ] = P\\ [\\ \\text{recidivism predicted}\\ |\\ \\text{Caucasian, recidivism}\\ ], \\text{e.g. } 0.95\\end{align}\n\n\\begin{align}P\\ [\\ \\text{recidivism predicted}\\ |\\ \\text{African-American, no recidivism}\\ ] = P\\ [\\ \\text{recidivism predicted}\\ |\\ \\text{Caucasian, no recidivism}\\ ], \\text{e.g. } 0.15\\end{align}\n\nbut that also means that African-Americans (and Caucasians) of different\nsubgroup based on training labels don't necessarily have parity:\n\n\\begin{align}P[\\text{recidivism predicted} | \\text{African-American, recidivism}] = 0.95 \\neq 0.15 = P[\\text{recidivism predicted} | \\text{African-American, no recidivism}]\\end{align}\n\nAssessing which disparity metric is indeed fair varies by application\nscenario. In this case the evaluation focuses on Equalized Odds, because the\nrecidivism prediction should be accurate for each race, and for each subgroup\nwithin. The plot for the training data shows the intended outcome, while the\nplot for the test data exhibits slight variation which is likely due to\nrandomized predictions as well as a slightly different data distribution.\n\nThis wrapper around the unconstrained estimator serves the purpose of mapping\nthe predict method to ``predict_proba``` so that we can use real values to get\nmore accurate estimates.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import clone\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\n\nclass LogisticRegressionAsRegression(BaseEstimator, ClassifierMixin):\n    def __init__(self, logistic_regression_estimator):\n        self.logistic_regression_estimator = logistic_regression_estimator\n\n    def fit(self, X, y):\n        try:\n            check_is_fitted(self.logistic_regression_estimator)\n            self.logistic_regression_estimator_ = self.logistic_regression_estimator\n        except NotFittedError:\n            self.logistic_regression_estimator_ = clone(\n                self.logistic_regression_estimator\n            ).fit(X, y)\n        return self\n\n    def predict(self, X):\n        # use predict_proba to get real values instead of 0/1, select only prob for 1\n        scores = self.logistic_regression_estimator_.predict_proba(X)[:, 1]\n        return scores\n\n\nfrom fairlearn.postprocessing import ThresholdOptimizer\n\nestimator_wrapper = LogisticRegressionAsRegression(estimator).fit(X_train, y_train)\npostprocessed_predictor_EO = ThresholdOptimizer(\n    estimator=estimator_wrapper, constraints=\"equalized_odds\", prefit=True\n)\n\npostprocessed_predictor_EO.fit(\n    X_train, y_train, sensitive_features=sensitive_features_train\n)\n\nfairness_aware_predictions_EO_train = postprocessed_predictor_EO.predict(\n    X_train, sensitive_features=sensitive_features_train\n)\nfairness_aware_predictions_EO_test = postprocessed_predictor_EO.predict(\n    X_test, sensitive_features=sensitive_features_test\n)\n\n# show only test data related plot by default - uncomment the next line to see\n# training data plot as well\n\n# show_proportions(\n#     X_train, sensitive_features_train, fairness_aware_predictions_EO_train,\n#     y_train,\n#     description=\"equalized odds with postprocessed model on training data:\",\n#     plot_row_index=1)\nshow_proportions(\n    X_test,\n    sensitive_features_test,\n    fairness_aware_predictions_EO_test,\n    y_test,\n    description=\"equalized odds with postprocessed model on test data:\",\n    plot_row_index=2,\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Post Processing in Detail\n-------------------------\n\nWhile this worked as the numbers show, it's not entirely obvious how it found\nits solution. The following section will provide a deep dive on post\nprocessing for Equalized Odds (EO).\n\nThe post processing method (based on work by `Hardt, Price,\nSrebro <https://arxiv.org/pdf/1610.02413.pdf>`_) takes a fairness-unaware model\nand disparity constraints (such as EO) in the constructor and features (X),\nlabels (y), and a sensitive feature (sensitive_features) in the fit method.\nIt subsequently uses the model to make predictions for all samples in X. Note\nthat these predictions could be 0/1 (as in this example), or more categories,\nor even real valued scores. In this case, this looks as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = estimator_wrapper.predict(X_train)\nscores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding threshold rules\n***********************\n\nThe algorithm then tries to find all thresholding rules with which it could\ndivide the data. Any score for which the thresholding rule evaluates to true\nis predicted to be 1. It does that for each group, so in this case for each\nrace. Depending on the unconstrained predictor's scores you could have lots\nof thresholding rules, between each set of such values. For each rule we just\nevaluate the following two probabilities empirically:\n\n\\begin{align}P[\\hat{Y} = 1 | Y = 0] \\text{ which is labeled x below to indicate that it'll be plotted on the x-axis}\\end{align}\n\n\\begin{align}P[\\hat{Y} = 1 | Y = 1] \\text{ which is labeled y below to indicate that it'll be plotted on the y-axis}\\end{align}\n\nThe former is the false negative rate (of that group), while the latter is\nthe true positive rate (of that group). In this example, the threshold rules\nwould be the ones shown below:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fairlearn.postprocessing._threshold_optimizer import _reformat_and_group_data\n\ndata_grouped_by_sensitive_feature = _reformat_and_group_data(\n    sensitive_features_train, y_train.astype(int), scores\n)\ndata_grouped_by_sensitive_feature.describe()\n\nfrom fairlearn.postprocessing._roc_curve_utilities import _calculate_roc_points\n\nroc_points = {}\nfor group_name, group in data_grouped_by_sensitive_feature:\n    roc_points[group_name] = _calculate_roc_points(\n        data_grouped_by_sensitive_feature.get_group(group_name), 0\n    )\nprint(\"Thresholding rules:\")\nroc_points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The base points with (x,y) as (0,0) and (1,1) always exist, because that\nessentially just means that we're predicting everything as 0 or everything as\n1 regardless of the scores from the fairness-unaware model. Let's look at\nboth cases:\n\n- x=0, y=0, threshold rule \">inf\": more than infinity is impossible, which\n  means every sample is predicted as 0. That means $P[\\hat{Y} = 1 | Y =\n  0] = 0$ (represented as x) because the predictions $\\hat{Y}$ are\n  never 1, and similarly $P[\\hat{Y} = 1 | Y = 1] = 0$ (represented as\n  y).\n- x=1, y=1, threshold rule \">-inf\": more than infinity is always true, which\n  means every sample is predicted as 1. That means $P[\\hat{Y} = 1 | Y =\n  0] = 1$ (represented as x) because the predictions $\\hat{Y}$ are\n  always 1, and similarly $P[\\hat{Y} = 1 | Y = 1] = 1$ (represented as\n  y).\n\nThe more interesting logic happens in between. The x and y values were\ncalculated as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_group_0 = {}\nn_group_1 = {}\nfor group_name, group in data_grouped_by_sensitive_feature:\n    print(\"{}:\".format(group_name))\n    n_group_1[group_name] = sum(group[\"label\"])\n    n_group_0[group_name] = len(group) - n_group_1[group_name]\n\n    print(\"    number of samples with label 1: {}\".format(n_group_1[group_name]))\n    print(\"    number of samples with label 0: {}\".format(n_group_0[group_name]))\n\nthreshold = 0.5\nfor group_name, group in data_grouped_by_sensitive_feature:\n    x_group_0_5 = (\n        sum((group[\"score\"] > threshold) & (group[\"label\"] == 0))\n        / n_group_0[group_name]\n    )\n    y_group_0_5 = (\n        sum((group[\"score\"] > threshold) & (group[\"label\"] == 1))\n        / n_group_1[group_name]\n    )\n    print(\"{}:\".format(group_name))\n    print(\"    P[\u0176 = 1 | Y = 0] = {}\".format(x_group_0_5))\n    print(\"    P[\u0176 = 1 | Y = 1] = {}\".format(y_group_0_5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that it never makes sense to have $x>y$ because in that case you're\nbetter off flipping labels, i.e. completely turning around the meaning of the\nscores. The method automatically does that unless specified otherwise.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpolated Predictions and Probabilistic Classifiers\n******************************************************\n\nThis way you end up with a set of points above the diagonal line connecting\n(0,0) and (1,1). We calculate the convex hull based on that, because we can\nreach any point in between two known thresholding points by interpolation. An\ninterpolation could be $p_0 (x_0, y_0) + p_1 (x_1, y_1)$. For the post\nprocessing algorithm that would mean that we use the rule defined by\n$(x_0, y_0, \\text{operation}_0)$ $\\quad p_0$ percent of the time,\nand the rule defined by $(x_1, y_1, \\text{operation}_1)$ $\\quad\np_1$ percent of the time, thus resulting in a probabilistic classifier.\nDepending on the data certain fairness objectives can only be accomplished\nwith probabilistic classifiers. However, not every use case lends itself to\nprobabilistic classifiers, since it could mean that two people with identical\nfeatures are classified differently.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding the Equalized Odds solution\n***********************************\n\nFrom all the ROC points (see below) we need to extract the convex hull for\nboth groups/curves.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for group_name, group in data_grouped_by_sensitive_feature:\n    plt.plot(roc_points[group_name].x, roc_points[group_name].y)\nplt.xlabel(\"$P [ \\\\hat{Y}=1 | Y=0 ]$\")\nplt.ylabel(\"$P [ \\\\hat{Y}=1 | Y=1 ]$\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Equalized Odds case, we need to find a point where the presented\nprobabilities (false positive rate, true positive rate, and thereby also true\nnegative rate and false negative rate) for the corresponding groups match\nwhile minimizing the error, which is equivalent to finding the minimum error\noverlap of the convex hulls of the ROC curves. The error in the chart is\nsmallest in the top left corner. This is done as part of the `fit` step\nabove, and we'll repeat it here for completeness. The yellow area is the\noverlap between the areas under the curve that are reachable with\ninterpolation for both groups. Of course, this works for more than two groups\nas well. The result is that we have interpolated solutions for each group,\ni.e. every prediction is calculated as the weighted result of two threshold\nrules.\n\nIn this particular case the Caucasian curve is always below or matching the\nAfrican-American curve. That means that the area under the Caucasian curve is\nalso identical to the overlap. That does not always happen, though, and\noverlaps can be fairly complex with multiple intersecting curves defining its\noutline.\n\nWe can actually even look up the specific interpolations and interpret the\nresults. Keep in mind that these interpolations come up with a floating point\nnumber between 0 and 1, and represent the probability of getting 0 or 1 in\nthe predicted outcome.\n\nThe result for African-Americans is a combination of two thresholding rules.\nThe first rule checks whether the score is above 0.542, the other whether it\nis above 0.508. The first rule has a weight of 0.19, which means it\ndetermines 19% of the resulting probability. The second rule determines the\nremaining 81%. In the chart the Caucasian curve is below the African-American\ncurve at the EO solution. This means that there is a slight adjustment\naccording to the formula presented below.\n\nThe Caucasian rules have somewhat lower thresholds: The first rule's\nthreshold is >0.421 and it is basically the deciding factor with a weight of\n99.3%, while the second rule's threshold is >0.404.\n\nOverall, this means that the postprocessing algorithm learned to get\nprobabilities consistent with Equalized Odds and minimal error by setting\nlower thresholds for Caucasians than for African-Americans. The resulting\nprobability from the formula below is then the probability to get label 1\nfrom the probabilistic classifier.\n\nNote that this does not necessarily mean it's fair. It simply enforced the\nconstraints we asked it to enforce, as described by Equalized Odds. The\nsocietal context plays a crucial role in determining whether this is fair.\n\nThe parameters `p_ignore` and `prediction_constant` are irrelevant for cases\nwhere the curves intersect in the minimum error point. When that doesn't\nhappen, and the minimum error point is only part of one curve, then the\ninterpolation is adjusted as follows::\n\n  p_ignore * prediction_constant + (1 - p_ignore) * (p0 * operation0(score) + p1 * operation1(score))\n\nThe adjustment should happen to the higher one of the curves and essentially\nbrings it closer to the diagonal as represented by `prediction_constant`. In\nthis case this is not required since the curves intersect, but we are\nactually slightly inaccurate because we only determine the minimum error\npoint on a grid of x values, instead of calculating the intersection point\nanalytically. By choosing a large `grid_size` this can be alleviated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "postprocessed_predictor_EO._plot = True\npostprocessed_predictor_EO.fit(\n    X_train, y_train, sensitive_features=sensitive_features_train\n)\n\nfor (\n    group,\n    interpolation,\n) in postprocessed_predictor_EO._post_processed_predictor_by_sensitive_feature.items():\n    print(\"{}:\".format(group))\n    print(\"\\n \".join(interpolation.__repr__().split(\",\")))\n    print(\"-----------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}