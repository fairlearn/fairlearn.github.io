{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Binary Classification on COMPAS dataset\n=======================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting and preparing the data\n==============================\n\nTo demonstrate the post processing algorithm we use the \\\"COMPAS\\\"\ndataset from\n[ProPublica](https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv).\nThe labels represent the two-year recidivism ID, i.e. whether a person\ngot rearrested within two years (label 1) or not (label 0). The features\ninclude sex, age, as well as information on prior incidents.\n\nTo start, let\\'s download the dataset using [tempeh]{.title-ref}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\nfrom tempeh.configurations import datasets\n\ncompas_dataset = datasets[\"compas\"]()\nX_train, X_test = compas_dataset.get_X(format=pd.DataFrame)\ny_train, y_test = compas_dataset.get_y(format=pd.Series)\n(\n    sensitive_features_train,\n    sensitive_features_test,\n) = compas_dataset.get_sensitive_features(\"race\", format=pd.Series)\nX_train.loc[0], y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a fairness-unaware model\n===============================\n\nFirst we set up a helper function that will help in analyzing the\ndataset as well as predictions from our models. Feel free to skip to the\nnext cell for the actual logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\n\n# show_proportions is only a helper function for plotting\ndef show_proportions(\n    X, sensitive_features, y_pred, y=None, description=None, plot_row_index=1\n):\n    print(\"\\n\" + description)\n    plt.figure(plot_row_index)\n    plt.title(description)\n    plt.ylabel(\"P[recidivism predicted | conditions]\")\n\n    indices = {}\n    positive_indices = {}\n    negative_indices = {}\n    recidivism_count = {}\n    recidivism_pct = {}\n    groups = np.unique(sensitive_features.values)\n    n_groups = len(groups)\n    max_group_length = max([len(group) for group in groups])\n    color = cm.rainbow(np.linspace(0, 1, n_groups))\n    x_tick_labels_basic = []\n    x_tick_labels_by_label = []\n    for index, group in enumerate(groups):\n        indices[group] = sensitive_features.index[sensitive_features == group]\n        recidivism_count[group] = sum(y_pred[indices[group]])\n        recidivism_pct[group] = recidivism_count[group] / len(indices[group])\n        print(\n            \"P[recidivism predicted | {}]                {}= {}\".format(\n                group, \" \" * (max_group_length - len(group)), recidivism_pct[group]\n            )\n        )\n\n        plt.bar(index + 1, recidivism_pct[group], color=color[index])\n        x_tick_labels_basic.append(group)\n\n        if y is not None:\n            positive_indices[group] = sensitive_features.index[\n                (sensitive_features == group) & (y == 1)\n            ]\n            negative_indices[group] = sensitive_features.index[\n                (sensitive_features == group) & (y == 0)\n            ]\n            prob_1 = sum(y_pred[positive_indices[group]]) / len(positive_indices[group])\n            prob_0 = sum(y_pred[negative_indices[group]]) / len(negative_indices[group])\n            print(\n                \"P[recidivism predicted | {}, recidivism]    {}= {}\".format(\n                    group, \" \" * (max_group_length - len(group)), prob_1\n                )\n            )\n            print(\n                \"P[recidivism predicted | {}, no recidivism] {}= {}\".format(\n                    group, \" \" * (max_group_length - len(group)), prob_0\n                )\n            )\n\n            plt.bar(n_groups + 1 + 2 * index, prob_1, color=color[index])\n            plt.bar(n_groups + 2 + 2 * index, prob_0, color=color[index])\n            x_tick_labels_by_label.extend(\n                [\"{} recidivism\".format(group), \"{} no recidivism\".format(group)]\n            )\n\n    x_tick_labels = x_tick_labels_basic + x_tick_labels_by_label\n    plt.xticks(\n        range(1, len(x_tick_labels) + 1),\n        x_tick_labels,\n        rotation=45,\n        horizontalalignment=\"right\",\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get started we look at a very basic Logistic Regression model. We fit\nit to the training data and plot some characteristics of training and\ntest data as well as the predictions of the model on those datasets.\n\nWe notice a stark contrast in the predictions with African-Americans\nbeing a lot more likely to be predicted to reoffend, similar to the\noriginal training data. However, there\\'s even a disparity between the\nsubgroup of African-Americans and Caucasians with recidivism. When\nconsidering only the samples labeled with \\\"no recidivism\\\"\nAfrican-Americans are much more likely to be predicted to reoffend than\nCaucasians. The test data shows a similar disparity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n\nestimator = LogisticRegression(solver=\"liblinear\")\nestimator.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print and plot data from training and test set as well as predictions\nwith fairness-unaware classifier on both sets show only test data\nrelated plots by default - uncomment the next two lines to see training\ndata plots as well\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# show_proportions(X_train, sensitive_features_train, y_train,\n#                  description=\"original training data:\", plot_row_index=1)\n# show_proportions(X_train, sensitive_features_train,\n#                  estimator.predict(X_train), y_train,\n#                  description=\"fairness-unaware prediction on training data:\",\n#                  plot_row_index=2)\nshow_proportions(\n    X_test,\n    sensitive_features_test,\n    y_test,\n    description=\"original test data:\",\n    plot_row_index=3,\n)\nshow_proportions(\n    X_test,\n    sensitive_features_test,\n    estimator.predict(X_test),\n    y_test,\n    description=\"fairness-unaware prediction on test data:\",\n    plot_row_index=4,\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Postprocessing the model to get a fair model\n============================================\n\nThe idea behind postprocessing is to alter the output of the\nfairness-unaware model to achieve fairness. The postprocessing algorithm\nrequires three input arguments: - the matrix of samples X - the vector\nof predictions y from the fairness-unaware model - the vector of group\nattribute values A (in the code we refer to it as\n[sensitive\\_features]{.title-ref})\n\nThe goal is to make the output fair with respect to constraints. The\npostprocessing algorithm uses one of\n\n-   Demographic Parity (DP):\n    $P\\ [\\ h(X)=\\hat{y}\\ |\\ A=a] = P\\ [\\ h(X)=\\hat{y}\\ ] \\qquad \\forall a, \\hat{y}$\n-   Equalized Odds (EO):\n    $P\\ [\\ h(X)=\\hat{y}\\ |\\ A=a, Y=y] = P\\ [\\ h(X)=\\hat{y}\\ |\\ Y=y\\ ] \\qquad \\forall a, \\hat{y}$\n\nwhere $h(X)$ is the prediction based on the input $X$, $\\hat{y}$ and $y$\nare labels, and $a$ is a sensitive feature value. In this example, we\\'d\nexpect the postprocessed model with DP to be balanced between races. In\nthis particular scenario it makes more sense to aim at fairness through\naccuracy like EO. EO does not make the same guarantees. Instead, it\nensures parity between the subgroups of each race with label 1 in the\ntraining set, and parity between the subgroups of each race with label 0\nin the training set. Applied to this scenario, this means that the\nsubgroups of each race who reoffended in the past are equally likely to\nbe predicted to reoffend (and therefore also equally likely not to).\nSimilarly, there is parity between subgroups of each race without\nrecidivism, but we have no parity between the groups with different\ntraining labels. In mathematical terms at the example of\nAfrican-American and Caucasian:\n\n$$P\\ [\\ \\text{recidivism predicted}\\ |\\ \\text{African-American, recidivism}\\ ] = P\\ [\\ \\text{recidivism predicted}\\ |\\ \\text{Caucasian, recidivism}\\ ], \\text{e.g. } 0.95$$\n\n$$P\\ [\\ \\text{recidivism predicted}\\ |\\ \\text{African-American, no recidivism}\\ ] = P\\ [\\ \\text{recidivism predicted}\\ |\\ \\text{Caucasian, no recidivism}\\ ], \\text{e.g. } 0.15$$\n\nbut that also means that African-Americans (and Caucasians) of different\nsubgroup based on training labels don\\'t necessarily have parity:\n\n$$P[\\text{recidivism predicted} | \\text{African-American, recidivism}] = 0.95 \\neq 0.15 = P[\\text{recidivism predicted} | \\text{African-American, no recidivism}]$$\n\nAssessing which disparity metric is indeed fair varies by application\nscenario. In this case the evaluation focuses on Equalized Odds, because\nthe recidivism prediction should be accurate for each race, and for each\nsubgroup within. The plot for the training data shows the intended\noutcome, while the plot for the test data exhibits slight variation\nwhich is likely due to randomized predictions as well as a slightly\ndifferent data distribution.\n\nThis wrapper around the unconstrained estimator serves the purpose of\nmapping the predict method to `predict_proba`\\` so that we can use real\nvalues to get more accurate estimates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import clone\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\n\nclass LogisticRegressionAsRegression(BaseEstimator, ClassifierMixin):\n    def __init__(self, logistic_regression_estimator):\n        self.logistic_regression_estimator = logistic_regression_estimator\n\n    def fit(self, X, y):\n        try:\n            check_is_fitted(self.logistic_regression_estimator)\n            self.logistic_regression_estimator_ = self.logistic_regression_estimator\n        except NotFittedError:\n            self.logistic_regression_estimator_ = clone(\n                self.logistic_regression_estimator\n            ).fit(X, y)\n        return self\n\n    def predict(self, X):\n        # use predict_proba to get real values instead of 0/1, select only prob for 1\n        scores = self.logistic_regression_estimator_.predict_proba(X)[:, 1]\n        return scores\n\n\nfrom fairlearn.postprocessing import ThresholdOptimizer\n\nestimator_wrapper = LogisticRegressionAsRegression(estimator).fit(X_train, y_train)\npostprocessed_predictor_EO = ThresholdOptimizer(\n    estimator=estimator_wrapper, constraints=\"equalized_odds\", prefit=True\n)\n\npostprocessed_predictor_EO.fit(\n    X_train, y_train, sensitive_features=sensitive_features_train\n)\n\nfairness_aware_predictions_EO_train = postprocessed_predictor_EO.predict(\n    X_train, sensitive_features=sensitive_features_train\n)\nfairness_aware_predictions_EO_test = postprocessed_predictor_EO.predict(\n    X_test, sensitive_features=sensitive_features_test\n)\n\n# show only test data related plot by default - uncomment the next line to see\n# training data plot as well\n\n# show_proportions(\n#     X_train, sensitive_features_train, fairness_aware_predictions_EO_train,\n#     y_train,\n#     description=\"equalized odds with postprocessed model on training data:\",\n#     plot_row_index=1)\nshow_proportions(\n    X_test,\n    sensitive_features_test,\n    fairness_aware_predictions_EO_test,\n    y_test,\n    description=\"equalized odds with postprocessed model on test data:\",\n    plot_row_index=2,\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Post Processing in Detail\n=========================\n\nWhile this worked as the numbers show, it\\'s not entirely obvious how it\nfound its solution. The following section will provide a deep dive on\npost processing for Equalized Odds (EO).\n\nThe post processing method (based on work by [Hardt, Price,\nSrebro](https://arxiv.org/pdf/1610.02413.pdf)) takes a fairness-unaware\nmodel and disparity constraints (such as EO) in the constructor and\nfeatures (X), labels (y), and a sensitive feature (sensitive\\_features)\nin the fit method. It subsequently uses the model to make predictions\nfor all samples in X. Note that these predictions could be 0/1 (as in\nthis example), or more categories, or even real valued scores. In this\ncase, this looks as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = estimator_wrapper.predict(X_train)\nscores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding threshold rules\n=======================\n\nThe algorithm then tries to find all thresholding rules with which it\ncould divide the data. Any score for which the thresholding rule\nevaluates to true is predicted to be 1. It does that for each group, so\nin this case for each race. Depending on the unconstrained predictor\\'s\nscores you could have lots of thresholding rules, between each set of\nsuch values. For each rule we just evaluate the following two\nprobabilities empirically:\n\n$$P[\\hat{Y} = 1 | Y = 0] \\text{ which is labeled x below to indicate that it'll be plotted on the x-axis}$$\n\n$$P[\\hat{Y} = 1 | Y = 1] \\text{ which is labeled y below to indicate that it'll be plotted on the y-axis}$$\n\nThe former is the false negative rate (of that group), while the latter\nis the true positive rate (of that group). In this example, the\nthreshold rules would be the ones shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fairlearn.postprocessing._threshold_optimizer import _reformat_and_group_data\n\ndata_grouped_by_sensitive_feature = _reformat_and_group_data(\n    sensitive_features_train, y_train.astype(int), scores\n)\ndata_grouped_by_sensitive_feature.describe()\n\nfrom fairlearn.postprocessing._roc_curve_utilities import _calculate_roc_points\n\nroc_points = {}\nfor group_name, group in data_grouped_by_sensitive_feature:\n    roc_points[group_name] = _calculate_roc_points(\n        data_grouped_by_sensitive_feature.get_group(group_name), 0\n    )\nprint(\"Thresholding rules:\")\nroc_points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The base points with (x,y) as (0,0) and (1,1) always exist, because that\nessentially just means that we\\'re predicting everything as 0 or\neverything as 1 regardless of the scores from the fairness-unaware\nmodel. Let\\'s look at both cases:\n\n-   x=0, y=0, threshold rule \\\"\\>inf\\\": more than infinity is\n    impossible, which means every sample is predicted as 0. That means\n    $P[\\hat{Y} = 1 | Y =\n    0] = 0$ (represented as x) because the predictions $\\hat{Y}$ are\n    never 1, and similarly $P[\\hat{Y} = 1 | Y = 1] = 0$ (represented as\n    y).\n-   x=1, y=1, threshold rule \\\"\\>-inf\\\": more than infinity is always\n    true, which means every sample is predicted as 1. That means\n    $P[\\hat{Y} = 1 | Y =\n    0] = 1$ (represented as x) because the predictions $\\hat{Y}$ are\n    always 1, and similarly $P[\\hat{Y} = 1 | Y = 1] = 1$ (represented as\n    y).\n\nThe more interesting logic happens in between. The x and y values were\ncalculated as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_group_0 = {}\nn_group_1 = {}\nfor group_name, group in data_grouped_by_sensitive_feature:\n    print(\"{}:\".format(group_name))\n    n_group_1[group_name] = sum(group[\"label\"])\n    n_group_0[group_name] = len(group) - n_group_1[group_name]\n\n    print(\"    number of samples with label 1: {}\".format(n_group_1[group_name]))\n    print(\"    number of samples with label 0: {}\".format(n_group_0[group_name]))\n\nthreshold = 0.5\nfor group_name, group in data_grouped_by_sensitive_feature:\n    x_group_0_5 = (\n        sum((group[\"score\"] > threshold) & (group[\"label\"] == 0))\n        / n_group_0[group_name]\n    )\n    y_group_0_5 = (\n        sum((group[\"score\"] > threshold) & (group[\"label\"] == 1))\n        / n_group_1[group_name]\n    )\n    print(\"{}:\".format(group_name))\n    print(\"    P[\u0176 = 1 | Y = 0] = {}\".format(x_group_0_5))\n    print(\"    P[\u0176 = 1 | Y = 1] = {}\".format(y_group_0_5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that it never makes sense to have $x>y$ because in that case\nyou\\'re better off flipping labels, i.e. completely turning around the\nmeaning of the scores. The method automatically does that unless\nspecified otherwise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpolated Predictions and Probabilistic Classifiers\n======================================================\n\nThis way you end up with a set of points above the diagonal line\nconnecting (0,0) and (1,1). We calculate the convex hull based on that,\nbecause we can reach any point in between two known thresholding points\nby interpolation. An interpolation could be\n$p_0 (x_0, y_0) + p_1 (x_1, y_1)$. For the post processing algorithm\nthat would mean that we use the rule defined by\n$(x_0, y_0, \\text{operation}_0)$ $\\quad p_0$ percent of the time, and\nthe rule defined by $(x_1, y_1, \\text{operation}_1)$ $\\quad\np_1$ percent of the time, thus resulting in a probabilistic classifier.\nDepending on the data certain fairness objectives can only be\naccomplished with probabilistic classifiers. However, not every use case\nlends itself to probabilistic classifiers, since it could mean that two\npeople with identical features are classified differently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding the Equalized Odds solution\n===================================\n\nFrom all the ROC points (see below) we need to extract the convex hull\nfor both groups/curves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for group_name, group in data_grouped_by_sensitive_feature:\n    plt.plot(roc_points[group_name].x, roc_points[group_name].y)\nplt.xlabel(\"$P [ \\\\hat{Y}=1 | Y=0 ]$\")\nplt.ylabel(\"$P [ \\\\hat{Y}=1 | Y=1 ]$\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Equalized Odds case, we need to find a point where the presented\nprobabilities (false positive rate, true positive rate, and thereby also\ntrue negative rate and false negative rate) for the corresponding groups\nmatch while minimizing the error, which is equivalent to finding the\nminimum error overlap of the convex hulls of the ROC curves. The error\nin the chart is smallest in the top left corner. This is done as part of\nthe [fit]{.title-ref} step above, and we\\'ll repeat it here for\ncompleteness. The yellow area is the overlap between the areas under the\ncurve that are reachable with interpolation for both groups. Of course,\nthis works for more than two groups as well. The result is that we have\ninterpolated solutions for each group, i.e. every prediction is\ncalculated as the weighted result of two threshold rules.\n\nIn this particular case the Caucasian curve is always below or matching\nthe African-American curve. That means that the area under the Caucasian\ncurve is also identical to the overlap. That does not always happen,\nthough, and overlaps can be fairly complex with multiple intersecting\ncurves defining its outline.\n\nWe can actually even look up the specific interpolations and interpret\nthe results. Keep in mind that these interpolations come up with a\nfloating point number between 0 and 1, and represent the probability of\ngetting 0 or 1 in the predicted outcome.\n\nThe result for African-Americans is a combination of two thresholding\nrules. The first rule checks whether the score is above 0.542, the other\nwhether it is above 0.508. The first rule has a weight of 0.19, which\nmeans it determines 19% of the resulting probability. The second rule\ndetermines the remaining 81%. In the chart the Caucasian curve is below\nthe African-American curve at the EO solution. This means that there is\na slight adjustment according to the formula presented below.\n\nThe Caucasian rules have somewhat lower thresholds: The first rule\\'s\nthreshold is \\>0.421 and it is basically the deciding factor with a\nweight of 99.3%, while the second rule\\'s threshold is \\>0.404.\n\nOverall, this means that the postprocessing algorithm learned to get\nprobabilities consistent with Equalized Odds and minimal error by\nsetting lower thresholds for Caucasians than for African-Americans. The\nresulting probability from the formula below is then the probability to\nget label 1 from the probabilistic classifier.\n\nNote that this does not necessarily mean it\\'s fair. It simply enforced\nthe constraints we asked it to enforce, as described by Equalized Odds.\nThe societal context plays a crucial role in determining whether this is\nfair.\n\nThe parameters [p\\_ignore]{.title-ref} and\n[prediction\\_constant]{.title-ref} are irrelevant for cases where the\ncurves intersect in the minimum error point. When that doesn\\'t happen,\nand the minimum error point is only part of one curve, then the\ninterpolation is adjusted as follows:\n\n    p_ignore * prediction_constant + (1 - p_ignore) * (p0 * operation0(score) + p1 * operation1(score))\n\nThe adjustment should happen to the higher one of the curves and\nessentially brings it closer to the diagonal as represented by\n[prediction\\_constant]{.title-ref}. In this case this is not required\nsince the curves intersect, but we are actually slightly inaccurate\nbecause we only determine the minimum error point on a grid of x values,\ninstead of calculating the intersection point analytically. By choosing\na large [grid\\_size]{.title-ref} this can be alleviated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "postprocessed_predictor_EO._plot = True\npostprocessed_predictor_EO.fit(\n    X_train, y_train, sensitive_features=sensitive_features_train\n)\n\nfor (\n    group,\n    interpolation,\n) in postprocessed_predictor_EO._post_processed_predictor_by_sensitive_feature.items():\n    print(\"{}:\".format(group))\n    print(\"\\n \".join(interpolation.__repr__().split(\",\")))\n    print(\"-----------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}