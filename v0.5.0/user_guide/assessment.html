
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Assessment &#8212; Fairlearn 0.5.0 documentation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"E": "{\\mathbb{E}}", "P": "{\\mathbb{P}}", "given": "\\mathbin{\\vert}"}}}</script>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Mitigation" href="mitigation.html" />
    <link rel="prev" title="1. Fairness in Machine Learning" href="fairness_in_machine_learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="https://fairlearn.org">
  <img src="../_static/_static/images/fairlearn_full_color.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../about/index.html">
  About
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../quickstart.html">
  Quickstart
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="index.html">
  User Guide
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api_reference/index.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../auto_examples/index.html">
  Example Notebooks
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../contributor_guide/index.html">
  Contributor Guide
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../faq.html">
  FAQ
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/fairlearn/fairlearn" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/fairlearn" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://stackoverflow.com/questions/tagged/fairlearn" rel="noopener" target="_blank" title="StackOverflow">
            <span><i class="fab fa-stack-overflow"></i></span>
            <label class="sr-only">StackOverflow</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://discord.gg/R22yCfgsRn" rel="noopener" target="_blank" title="Discord">
            <span><i class="fab fa-discord"></i></span>
            <label class="sr-only">Discord</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><div class="sidebar-message">
  
  <h4>Versions</h4>
  <ul>
      
        
          <li><a href="../../v0.4.6/user_guide/assessment.html">v0.4.6</a></li>
        
      
      
        
          <li><strong><a href="assessment.html">v0.5.0</a></strong></li>
        
      
      
        
          <li><a href="../../v0.6.2/user_guide/assessment.html">v0.6.2</a></li>
        
      
      
        
          <li><a href="../../v0.7.0/user_guide/assessment.html">v0.7.0</a></li>
        
      
  </ul>
  
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="fairness_in_machine_learning.html">
   1. Fairness in Machine Learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Assessment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mitigation.html">
   3. Mitigation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="migrating_versions/index.html">
   4. Migrating from prior versions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="migrating_versions/migrating_to_v0_5_0.html">
     4.1. Migrating to v0.5.0 from v0.4.6
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="further_resources.html">
   5. Further Resources
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metrics">
   2.1. Metrics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ungrouped-metrics">
     2.1.1. Ungrouped Metrics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metrics-with-grouping">
     2.1.2. Metrics with Grouping
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fairlearn-dashboard">
   2.2. Fairlearn dashboard
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setup-and-a-single-model-assessment">
     2.2.1. Setup and a single-model assessment
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparing-multiple-models">
     2.2.2. Comparing multiple models
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="assessment">
<h1><span class="section-number">2. </span>Assessment<a class="headerlink" href="#assessment" title="Permalink to this headline">¶</a></h1>
<div class="section" id="metrics">
<h2><span class="section-number">2.1. </span>Metrics<a class="headerlink" href="#metrics" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="../api_reference/fairlearn.metrics.html#module-fairlearn.metrics" title="fairlearn.metrics"><code class="xref py py-mod docutils literal notranslate"><span class="pre">fairlearn.metrics</span></code></a> module provides the means to assess fairness-related
metrics for models. This applies for any kind of model that users may already
use, but also for models created with mitigation techniques from the
<a class="reference internal" href="mitigation.html#mitigation"><span class="std std-ref">Mitigation</span></a> section. The <a class="reference internal" href="#dashboard"><span class="std std-ref">Fairlearn dashboard</span></a> provides a visual way to
compare metrics between models as well as compare metrics for different groups
on a single model.</p>
<div class="section" id="ungrouped-metrics">
<h3><span class="section-number">2.1.1. </span>Ungrouped Metrics<a class="headerlink" href="#ungrouped-metrics" title="Permalink to this headline">¶</a></h3>
<p>At their simplest, metrics take a set of ‘true’ values <span class="math notranslate nohighlight">\(Y_{true}\)</span> (from
the input data) and predicted values <span class="math notranslate nohighlight">\(Y_{pred}\)</span> (by applying the model
to the input data), and use these to compute a measure. For example, the
<em>recall</em> or <em>true positive rate</em> is given by</p>
<div class="math notranslate nohighlight">
\[P( Y_{pred}=1 \given Y_{true}=1 )\]</div>
<p>That is, a measure of whether the model finds all the positive cases in the
input data. The <cite>scikit-learn</cite> package implements this in
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="(in scikit-learn v0.24)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.recall_score()</span></code></a>.</p>
<p>Suppose we have the following data we can see that the prediction is <cite>1</cite> in five
of the ten cases where the true value is <cite>1</cite>, so we expect the recall to be 0.5:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">skm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">skm</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">Y_true</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
</pre></div>
</div>
</div>
<div class="section" id="metrics-with-grouping">
<h3><span class="section-number">2.1.2. </span>Metrics with Grouping<a class="headerlink" href="#metrics-with-grouping" title="Permalink to this headline">¶</a></h3>
<p>When considering fairness, each row of input data will have an associated
group label <span class="math notranslate nohighlight">\(g \in G\)</span>, and we will want to know how the metric behaves
for each <span class="math notranslate nohighlight">\(g\)</span>. To help with this, Fairlearn provides a class, which takes
an existing (ungrouped) metric function, and applies it to each group within a
set of data.</p>
<p>Suppose in addition to the <span class="math notranslate nohighlight">\(Y_{true}\)</span> and <span class="math notranslate nohighlight">\(Y_{pred}\)</span> above, we had
the following set of labels:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_membership_data</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span>
<span class="gp">... </span>                         <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_columns&#39;</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.width&#39;</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span> <span class="s1">&#39;Y_true&#39;</span><span class="p">:</span> <span class="n">Y_true</span><span class="p">,</span>
<span class="gp">... </span>               <span class="s1">&#39;Y_pred&#39;</span><span class="p">:</span> <span class="n">Y_pred</span><span class="p">,</span>
<span class="gp">... </span>               <span class="s1">&#39;group_membership_data&#39;</span><span class="p">:</span> <span class="n">group_membership_data</span><span class="p">})</span>
<span class="go">    Y_true  Y_pred group_membership_data</span>
<span class="go">0        0       0                     d</span>
<span class="go">1        1       0                     a</span>
<span class="go">2        1       1                     c</span>
<span class="go">3        1       0                     b</span>
<span class="go">4        1       1                     b</span>
<span class="go">5        0       1                     c</span>
<span class="go">6        1       1                     c</span>
<span class="go">7        0       0                     c</span>
<span class="go">8        1       0                     b</span>
<span class="go">9        0       1                     d</span>
<span class="go">10       0       1                     c</span>
<span class="go">11       0       1                     a</span>
<span class="go">12       1       1                     b</span>
<span class="go">13       1       0                     d</span>
<span class="go">14       1       0                     c</span>
<span class="go">15       1       1                     c</span>
</pre></div>
</div>
<p>We then calculate a metric which shows the subgroups:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">fairlearn.metrics</span> <span class="kn">import</span> <span class="n">MetricFrame</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grouped_metric</span> <span class="o">=</span> <span class="n">MetricFrame</span><span class="p">(</span><span class="n">skm</span><span class="o">.</span><span class="n">recall_score</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">Y_true</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">sensitive_features</span><span class="o">=</span><span class="n">group_membership_data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Overall recall = &quot;</span><span class="p">,</span> <span class="n">grouped_metric</span><span class="o">.</span><span class="n">overall</span><span class="p">)</span>
<span class="go">Overall recall =  0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;recall by groups = &quot;</span><span class="p">,</span> <span class="n">grouped_metric</span><span class="o">.</span><span class="n">by_group</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
<span class="go">recall by groups =  {&#39;a&#39;: 0.0, &#39;b&#39;: 0.5, &#39;c&#39;: 0.75, &#39;d&#39;: 0.0}</span>
</pre></div>
</div>
<p>Note that the overall recall is the same as that calculated above in the
Ungrouped Metric section, while the ‘by group’ dictionary can be checked
against the table above.</p>
<p>In addition to these basic scores, Fairlearn also provides
convenience functions to recover the maximum and minimum values of the metric
across groups and also the difference and ratio between the maximum and minimum:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;min recall over groups = &quot;</span><span class="p">,</span> <span class="n">grouped_metric</span><span class="o">.</span><span class="n">group_min</span><span class="p">())</span>
<span class="go">min recall over groups =  0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;max recall over groups = &quot;</span><span class="p">,</span> <span class="n">grouped_metric</span><span class="o">.</span><span class="n">group_max</span><span class="p">())</span>
<span class="go">max recall over groups =  0.75</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;difference in recall = &quot;</span><span class="p">,</span> <span class="n">grouped_metric</span><span class="o">.</span><span class="n">difference</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;between_groups&#39;</span><span class="p">))</span>
<span class="go">difference in recall =  0.75</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ratio in recall = &quot;</span><span class="p">,</span> <span class="n">grouped_metric</span><span class="o">.</span><span class="n">ratio</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;between_groups&#39;</span><span class="p">))</span>
<span class="go">ratio in recall =  0.0</span>
</pre></div>
</div>
<p>A single instance of <a class="reference internal" href="../api_reference/fairlearn.metrics.html#fairlearn.metrics.MetricFrame" title="fairlearn.metrics.MetricFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">fairlearn.metrics.MetricFrame</span></code></a> can evaluate multiple
metrics simultaneously:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">multi_metric</span> <span class="o">=</span> <span class="n">MetricFrame</span><span class="p">({</span><span class="s1">&#39;precision&#39;</span><span class="p">:</span><span class="n">skm</span><span class="o">.</span><span class="n">precision_score</span><span class="p">,</span> <span class="s1">&#39;recall&#39;</span><span class="p">:</span><span class="n">skm</span><span class="o">.</span><span class="n">recall_score</span><span class="p">},</span>
<span class="gp">... </span>                            <span class="n">Y_true</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span>
<span class="gp">... </span>                            <span class="n">sensitive_features</span><span class="o">=</span><span class="n">group_membership_data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multi_metric</span><span class="o">.</span><span class="n">overall</span>
<span class="go">precision    0.555556</span>
<span class="go">recall            0.5</span>
<span class="go">dtype: object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multi_metric</span><span class="o">.</span><span class="n">by_group</span>
<span class="go">     precision recall</span>
<span class="go">sensitive_feature_0</span>
<span class="go">a            0      0</span>
<span class="go">b            1    0.5</span>
<span class="go">c          0.6   0.75</span>
<span class="go">d            0      0</span>
</pre></div>
</div>
<p>If there are per-sample arguments (such as sample weights), these can also be provided
in a dictionary via the <code class="docutils literal notranslate"><span class="pre">sample_params</span></code> argument.:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s_w</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s_p</span> <span class="o">=</span> <span class="p">{</span> <span class="s1">&#39;sample_weight&#39;</span><span class="p">:</span><span class="n">s_w</span> <span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weighted</span> <span class="o">=</span> <span class="n">MetricFrame</span><span class="p">(</span><span class="n">skm</span><span class="o">.</span><span class="n">recall_score</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">Y_true</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">sensitive_features</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">group_membership_data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;SF 0&#39;</span><span class="p">),</span>
<span class="gp">... </span>                       <span class="n">sample_params</span><span class="o">=</span><span class="n">s_p</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weighted</span><span class="o">.</span><span class="n">overall</span>
<span class="go">0.45</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weighted</span><span class="o">.</span><span class="n">by_group</span>
<span class="go">SF 0</span>
<span class="go">a               0</span>
<span class="go">b             0.5</span>
<span class="go">c        0.714286</span>
<span class="go">d               0</span>
<span class="go">Name: recall_score, dtype: object</span>
</pre></div>
</div>
<p>If mutiple metrics are being evaluated, then <code class="docutils literal notranslate"><span class="pre">sample_params</span></code> becomes a dictionary of
dictionaries, with the first key corresponding matching that in the dictionary holding
the desired underlying metric functions.</p>
<p>We do not support non-sample parameters at the current time. If these are required, then
use <a class="reference external" href="https://docs.python.org/3/library/functools.html#functools.partial" title="(in Python v3.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">functools.partial()</span></code></a> to prebind the required arguments to the metric
function:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">functools</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fbeta_06</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">skm</span><span class="o">.</span><span class="n">fbeta_score</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric_beta</span> <span class="o">=</span> <span class="n">MetricFrame</span><span class="p">(</span><span class="n">fbeta_06</span><span class="p">,</span>
<span class="gp">... </span>                          <span class="n">Y_true</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span>
<span class="gp">... </span>                          <span class="n">sensitive_features</span><span class="o">=</span><span class="n">group_membership_data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric_beta</span><span class="o">.</span><span class="n">overall</span>
<span class="go">0.5396825396825397</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric_beta</span><span class="o">.</span><span class="n">by_group</span>
<span class="go">sensitive_feature_0</span>
<span class="go">a            0</span>
<span class="go">b     0.790698</span>
<span class="go">c      0.63354</span>
<span class="go">d            0</span>
<span class="go">Name: metric, dtype: object</span>
</pre></div>
</div>
<p>Finally, multiple sensitive features can be specified. The <code class="docutils literal notranslate"><span class="pre">by_groups</span></code> property then
holds the intersections of these groups:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">g_2</span> <span class="o">=</span> <span class="p">[</span> <span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s_f_frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">group_membership_data</span><span class="p">,</span> <span class="n">g_2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>                         <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;SF 0&#39;</span><span class="p">,</span> <span class="s1">&#39;SF 1&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric_2sf</span> <span class="o">=</span> <span class="n">MetricFrame</span><span class="p">(</span><span class="n">skm</span><span class="o">.</span><span class="n">recall_score</span><span class="p">,</span>
<span class="gp">... </span>                         <span class="n">Y_true</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span>
<span class="gp">... </span>                         <span class="n">sensitive_features</span><span class="o">=</span><span class="n">s_f_frame</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric_2sf</span><span class="o">.</span><span class="n">overall</span>  <span class="c1"># Same as before</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric_2sf</span><span class="o">.</span><span class="n">by_group</span>
<span class="go">SF 0  SF 1</span>
<span class="go">a     6         0</span>
<span class="go">      8       NaN</span>
<span class="go">b     6       0.5</span>
<span class="go">      8       0.5</span>
<span class="go">c     6         1</span>
<span class="go">      8       0.5</span>
<span class="go">d     6         0</span>
<span class="go">      8         0</span>
<span class="go">Name: recall_score, dtype: object</span>
</pre></div>
</div>
<p>With such a small number of samples, we are obviously running into cases where
there are no members in a particular combination of sensitive features. In this
case we see that the subgroup <code class="docutils literal notranslate"><span class="pre">(a,</span> <span class="pre">8)</span></code> has a result of <code class="docutils literal notranslate"><span class="pre">NaN</span></code>, indicating
that there were no samples in it.</p>
</div>
</div>
<div class="section" id="fairlearn-dashboard">
<span id="dashboard"></span><h2><span class="section-number">2.2. </span>Fairlearn dashboard<a class="headerlink" href="#fairlearn-dashboard" title="Permalink to this headline">¶</a></h2>
<p>The Fairlearn dashboard is a Jupyter notebook widget for assessing how a
model’s predictions impact different groups (e.g., different ethnicities), and
also for comparing multiple models along different fairness and performance
metrics.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="code docutils literal notranslate"><span class="pre">FairlearnDashboard</span></code> will move from Fairlearn to the
<code class="code docutils literal notranslate"><span class="pre">raiwidgets</span></code> package after the v0.5.0 release. Instead, Fairlearn
will provide some of the existing functionality through
<code class="code docutils literal notranslate"><span class="pre">matplotlib</span></code>-based visualizations.</p>
</div>
<div class="section" id="setup-and-a-single-model-assessment">
<h3><span class="section-number">2.2.1. </span>Setup and a single-model assessment<a class="headerlink" href="#setup-and-a-single-model-assessment" title="Permalink to this headline">¶</a></h3>
<p>To assess a single model’s fairness and performance, the dashboard widget can
be launched within a Jupyter notebook as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fairlearn.widget</span> <span class="kn">import</span> <span class="n">FairlearnDashboard</span>

<span class="c1"># A_test containts your sensitive features (e.g., age, binary gender)</span>
<span class="c1"># sensitive_feature_names contains your sensitive feature names</span>
<span class="c1"># y_true contains ground truth labels</span>
<span class="c1"># y_pred contains prediction labels</span>

<span class="n">FairlearnDashboard</span><span class="p">(</span><span class="n">sensitive_features</span><span class="o">=</span><span class="n">A_test</span><span class="p">,</span>
                   <span class="n">sensitive_feature_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;BinaryGender&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">],</span>
                   <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                   <span class="n">y_pred</span><span class="o">=</span><span class="p">[</span><span class="n">y_pred</span><span class="o">.</span><span class="n">tolist</span><span class="p">()])</span>
</pre></div>
</div>
<p>After the launch, the widget walks the user through the assessment setup,
where the user is asked to select</p>
<ol class="arabic simple">
<li><p>the sensitive feature of interest (e.g., binary gender or age), and</p></li>
<li><p>the performance metric (e.g., model precision) along which to evaluate the
overall model performance as well as any disparities across groups. These
selections are then used to obtain the visualization of the model’s impact
on the subgroups (e.g., model precision for females and model precision for
males).</p></li>
</ol>
<p>The following figures illustrate the setup steps, where <em>binary gender</em> is
selected as a sensitive feature and <em>accuracy rate</em> is selected as the
performance metric.</p>
<img alt="../_images/fairlearn-dashboard-start.png" src="../_images/fairlearn-dashboard-start.png" />
<img alt="../_images/fairlearn-dashboard-multiple-sensitive-features.png" src="../_images/fairlearn-dashboard-multiple-sensitive-features.png" />
<img alt="../_images/fairlearn-dashboard-performance-multiple-sensitive-features.png" src="../_images/fairlearn-dashboard-performance-multiple-sensitive-features.png" />
<p>After the setup, the dashboard presents the model assessment in two panels:</p>
<table class="colwidths-given table">
<colgroup>
<col style="width: 25%" />
<col style="width: 75%" />
</colgroup>
<tbody>
<tr class="row-odd"><th class="stub"><p>Disparity in performance</p></th>
<td><p>This panel shows: (1) the performance of your model with respect to
your selected performance metric (e.g., <em>accuracy rate</em>) overall as
well as on different subgroups based on your selected sensitive
feature (e.g., <em>accuracy rate</em> for females, <em>accuracy rate</em> for
males); (2) the disparity (difference) in the values of the selected
performance metric across different subgroups; (3) the distribution of
errors in each subgroup (e.g., female, male). For binary
classification, the errors are further split into overprediction
(predicting 1 when the true label is 0), and underprediction
(predicting 0 when the true label is 1).</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Disparity in predictions</p></th>
<td><p>This panel shows a bar chart that contains the selection rate in each
group, meaning the fraction of data classified as 1 (in binary
classification) or distribution of prediction values (in regression).</p></td>
</tr>
</tbody>
</table>
<img alt="../_images/fairlearn-dashboard-disparity-performance-multiple-sensitive-features.png" src="../_images/fairlearn-dashboard-disparity-performance-multiple-sensitive-features.png" />
<img alt="../_images/fairlearn-dashboard-disparity-predictions-multiple-sensitive-features.png" src="../_images/fairlearn-dashboard-disparity-predictions-multiple-sensitive-features.png" />
</div>
<div class="section" id="comparing-multiple-models">
<h3><span class="section-number">2.2.2. </span>Comparing multiple models<a class="headerlink" href="#comparing-multiple-models" title="Permalink to this headline">¶</a></h3>
<p>The dashboard also enables comparison of multiple models, such as the models
produced by different learning algorithms and different mitigation approaches,
including <code class="code docutils literal notranslate"><span class="pre">fairlearn.reductions.GridSearch</span></code>,
<code class="code docutils literal notranslate"><span class="pre">fairlearn.reductions.ExponentiatedGradient</span></code>, and
<code class="code docutils literal notranslate"><span class="pre">fairlearn.postprocessing.ThresholdOptimizer</span></code>.</p>
<p>As before, the user is first asked to select the sensitive feature and the
performance metric. The <em>model comparison</em> view then depicts the performance
and disparity of all the provided models in a scatter plot. This allows the
user to examine trade-offs between performance and fairness. Each of the dots
can be clicked to open the assessment of the corresponding model. The figure
below shows the model comparison view with <em>binary gender</em> selected as a
sensitive feature and <em>accuracy rate</em> selected as the performance metric.</p>
<img alt="../_images/fairlearn-dashboard-models.png" src="../_images/fairlearn-dashboard-models.png" />
</div>
</div>
</div>


              </div>
              
              
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2018 - 2021, Fairlearn contributors.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>