
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_make_derived_metric.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_make_derived_metric.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_make_derived_metric.py:


======================
Making Derived Metrics
======================

.. GENERATED FROM PYTHON SOURCE LINES 10-31

This notebook demonstrates the use of the :func:`fairlearn.metrics.make_derived_metric`
function.
Many higher-order machine learning algorithms (such as hyperparameter tuners) make use
of scalar metrics when deciding how to proceed.
While the :class:`fairlearn.metrics.MetricFrame` has the ability to produce such
scalars through its aggregation functions, its API does not conform to that usually
expected by these algorithms.
The :func:`~fairlearn.metrics.make_derived_metric` function exists to bridge this gap.

Getting the Data
================

*This section may be skipped. It simply creates a dataset for
illustrative purposes*

We will use the well-known UCI 'Adult' dataset as the basis of this
demonstration. This is not for a lending scenario, but we will regard
it as one for the purposes of this example. We will use the existing
'race' and 'sex' columns (trimming the former to three unique values),
and manufacture credit score bands and loan sizes from other columns.
We start with some uncontroversial `import` statements:

.. GENERATED FROM PYTHON SOURCE LINES 31-47

.. code-block:: default


    import functools
    import numpy as np

    import sklearn.metrics as skm
    from sklearn.compose import ColumnTransformer
    from sklearn.datasets import fetch_openml
    from sklearn.impute import SimpleImputer
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    from sklearn.compose import make_column_selector as selector
    from sklearn.pipeline import Pipeline
    from fairlearn.metrics import MetricFrame, make_derived_metric
    from fairlearn.metrics import accuracy_score_group_min








.. GENERATED FROM PYTHON SOURCE LINES 48-49

Next, we import the data, dropping any rows which are missing data:

.. GENERATED FROM PYTHON SOURCE LINES 49-55

.. code-block:: default


    data = fetch_openml(data_id=1590, as_frame=True)
    X_raw = data.data
    y = (data.target == ">50K") * 1
    A = X_raw[["race", "sex"]]








.. GENERATED FROM PYTHON SOURCE LINES 56-63

We are now going to preprocess the data. Before applying any transforms,
we first split the data into train and test sets. All the transforms we
apply will be trained on the training set, and then applied to the test
set. This ensures that data doesn't leak between the two sets (this is
a serious but subtle
`problem in machine learning <https://en.wikipedia.org/wiki/Leakage_(machine_learning)>`_).
So, first we split the data:

.. GENERATED FROM PYTHON SOURCE LINES 63-79

.. code-block:: default


    (X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(
        X_raw, y, A, test_size=0.3, random_state=12345, stratify=y
    )

    # Ensure indices are aligned between X, y and A,
    # after all the slicing and splitting of DataFrames
    # and Series

    X_train = X_train.reset_index(drop=True)
    X_test = X_test.reset_index(drop=True)
    y_train = y_train.reset_index(drop=True)
    y_test = y_test.reset_index(drop=True)
    A_train = A_train.reset_index(drop=True)
    A_test = A_test.reset_index(drop=True)








.. GENERATED FROM PYTHON SOURCE LINES 80-89

Next, we build two :class:`~sklearn.pipeline.Pipeline` objects
to process the columns, one for numeric data, and the other
for categorical data. Both impute missing values; the difference
is whether the data are scaled (numeric columns) or
one-hot encoded (categorical columns). Imputation of missing
values should generally be done with care, since it could
potentially introduce biases. Of course, removing rows with
missing data could also cause trouble, if particular subgroups
have poorer data quality.

.. GENERATED FROM PYTHON SOURCE LINES 89-109

.. code-block:: default


    numeric_transformer = Pipeline(
        steps=[
            ("impute", SimpleImputer()),
            ("scaler", StandardScaler()),
        ]
    )
    categorical_transformer = Pipeline(
        [
            ("impute", SimpleImputer(strategy="most_frequent")),
            ("ohe", OneHotEncoder(handle_unknown="ignore")),
        ]
    )
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, selector(dtype_exclude="category")),
            ("cat", categorical_transformer, selector(dtype_include="category")),
        ]
    )








.. GENERATED FROM PYTHON SOURCE LINES 110-112

With our preprocessor defined, we can now build a
new pipeline which includes an Estimator:

.. GENERATED FROM PYTHON SOURCE LINES 112-123

.. code-block:: default


    unmitigated_predictor = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            (
                "classifier",
                LogisticRegression(solver="liblinear", fit_intercept=True),
            ),
        ]
    )








.. GENERATED FROM PYTHON SOURCE LINES 124-127

With the pipeline fully defined, we can first train it
with the training data, and then generate predictions
from the test data.

.. GENERATED FROM PYTHON SOURCE LINES 127-131

.. code-block:: default


    unmitigated_predictor.fit(X_train, y_train)
    y_pred = unmitigated_predictor.predict(X_test)








.. GENERATED FROM PYTHON SOURCE LINES 132-139

Creating a derived metric
=========================

Suppose our key metric is the accuracy score, and we are most interested in
ensuring that it exceeds some threshold for all subgroups
We might use the :class:`~fairlearn.metrics.MetricFrame` as
follows:

.. GENERATED FROM PYTHON SOURCE LINES 139-145

.. code-block:: default


    acc_frame = MetricFrame(
        skm.accuracy_score, y_test, y_pred, sensitive_features=A_test["sex"]
    )
    print("Minimum accuracy_score: ", acc_frame.group_min())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Minimum accuracy_score:  0.8098103202121151




.. GENERATED FROM PYTHON SOURCE LINES 146-173

We can create a function to perform this in a single call
using :func:`~fairlearn.metrics.make_derived_metric`.
This takes the following arguments (which must always be
supplied as keyword arguments):

- :code:`metric=`, the base metric function
- :code:`transform=`, the name of the aggregation
  transformation to perform. For this demonstration, we
  want this to be :code:`'group_min'`
- :code:`sample_param_names=`, a list of parameter names
  which should be treated as sample
  parameters. This is optional, and defaults to
  :code:`['sample_weight']` which is appropriate for many
  metrics in `scikit-learn`.

The result is a new function with the same signature as the
base metric, which accepts two extra arguments:

 - :code:`sensitive_features=` to specify the sensitive features
   which define the subgroups
 - :code:`method=` to adjust how the aggregation transformation
   operates. This corresponds to the same argument in
   :meth:`fairlearn.metrics.MetricFrame.difference` and
   :meth:`fairlearn.metrics.MetricFrame.ratio`

For the current case, we do not need the :code:`method=`
argument, since we are taking the minimum value.

.. GENERATED FROM PYTHON SOURCE LINES 173-178

.. code-block:: default


    my_acc = make_derived_metric(metric=skm.accuracy_score, transform="group_min")
    my_acc_min = my_acc(y_test, y_pred, sensitive_features=A_test["sex"])
    print("Minimum accuracy_score: ", my_acc_min)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Minimum accuracy_score:  0.8098103202121151




.. GENERATED FROM PYTHON SOURCE LINES 179-180

To show that the returned function also works with sample weights:

.. GENERATED FROM PYTHON SOURCE LINES 180-201

.. code-block:: default

    random_weights = np.random.rand(len(y_test))

    acc_frame_sw = MetricFrame(
        skm.accuracy_score,
        y_test,
        y_pred,
        sensitive_features=A_test["sex"],
        sample_params={"sample_weight": random_weights},
    )

    from_frame = acc_frame_sw.group_min()
    from_func = my_acc(
        y_test,
        y_pred,
        sensitive_features=A_test["sex"],
        sample_weight=random_weights,
    )

    print("From MetricFrame:", from_frame)
    print("From function   :", from_func)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    From MetricFrame: 0.8089392737735116
    From function   : 0.8089392737735116




.. GENERATED FROM PYTHON SOURCE LINES 202-207

The returned function can also handle parameters which are not sample
parameters. Consider :func:`sklearn.metrics.fbeta_score`, which
has a required :code:`beta=` argument (and suppose that this time
we are most interested in the maximum difference to the overall value).
First we evaluate this with a :class:`fairlearn.metrics.MetricFrame`:

.. GENERATED FROM PYTHON SOURCE LINES 207-222

.. code-block:: default


    fbeta_03 = functools.partial(skm.fbeta_score, beta=0.3)
    fbeta_03.__name__ = "fbeta_score__beta_0.3"

    beta_frame = MetricFrame(
        fbeta_03,
        y_test,
        y_pred,
        sensitive_features=A_test["sex"],
        sample_params={"sample_weight": random_weights},
    )
    beta_from_frame = beta_frame.difference(method="to_overall")

    print("From frame:", beta_from_frame)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    From frame: 0.01894136001209601




.. GENERATED FROM PYTHON SOURCE LINES 223-226

And next, we create a function to evaluate the same. Note that
we do not need to use :func:`functools.partial` to bind the
:code:`beta=` argument:

.. GENERATED FROM PYTHON SOURCE LINES 226-241

.. code-block:: default


    beta_func = make_derived_metric(metric=skm.fbeta_score, transform="difference")

    beta_from_func = beta_func(
        y_test,
        y_pred,
        sensitive_features=A_test["sex"],
        beta=0.3,
        sample_weight=random_weights,
        method="to_overall",
    )

    print("From function:", beta_from_func)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    From function: 0.01894136001209601




.. GENERATED FROM PYTHON SOURCE LINES 242-249

Pregenerated Metrics
====================

We provide a number of pregenerated metrics, to cover
common use cases. For example, we provide a
:code:`accuracy_score_group_min()` function to
find the minimum over the accuracy scores:

.. GENERATED FROM PYTHON SOURCE LINES 249-260

.. code-block:: default



    from_myacc = my_acc(y_test, y_pred, sensitive_features=A_test["race"])

    from_pregen = accuracy_score_group_min(
        y_test, y_pred, sensitive_features=A_test["race"]
    )

    print("From my function :", from_myacc)
    print("From pregenerated:", from_pregen)
    assert from_myacc == from_pregen




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    From my function : 0.8114035087719298
    From pregenerated: 0.8114035087719298





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  2.356 seconds)


.. _sphx_glr_download_auto_examples_plot_make_derived_metric.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_make_derived_metric.py <plot_make_derived_metric.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_make_derived_metric.ipynb <plot_make_derived_metric.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
