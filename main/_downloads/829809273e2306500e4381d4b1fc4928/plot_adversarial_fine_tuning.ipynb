{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[AdversarialFairness]{.title-ref} Fine Tuning {#adversarial_Example_2}\n=============================================\n\nAdversarial learning is inherently difficult because of various issues,\nsuch as mode collapse, divergence, and diminishing gradients. In\nparticular, mode collapse seems a real problem on this dataset: the\npredictor and adversary trap themselves in a local minimum by favoring\none class (mode). Problems with diverging parameters may also occur,\nwhich may be an indication of a bad choice of hyperparameters, such as a\nlearning rate that is too large. The problems that a user may encounter\nare of course case specific, but general good practices when training\nsuch models are: train slowly, ensuring the losses remain balanced, and\nkeep track of validation accuracies. Additionally, we found that single\nhidden layer neural networks work best for this use case.\n\nIn this example, we demonstrate some of these good practices. We start\nby defining our predictor neural network explicitly so that it is more\napparent. We will be using PyTorch, but the same can be achieved using\nTensorflow:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we cover a most basic application of adversarial mitigation. We\nstart by loading and preprocessing the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fairlearn.datasets import fetch_adult\n\nX, y = fetch_adult(return_X_y=True)\npos_label = y[0]\n\nz = X[\"sex\"]  # In this example, we consider 'sex' the sensitive feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As with other machine learning methods, it is wise to take a train-test\nsplit of the data in order to validate the model on unseen data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, y_test, Z_train, Z_test = train_test_split(\n    X, y, z, test_size=0.2, random_state=12345, stratify=y\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The UCI adult dataset cannot be fed into a neural network (yet), as we\nhave many columns that are not numerical in nature. To resolve this\nissue, we could for instance use one-hot encodings to preprocess\ncategorical columns. Additionally, let\\'s preprocess the numeric columns\nto a standardized range. For these tasks, we can use functionality from\nscikit-learn (:py`sklearn.preprocessing`{.interpreted-text role=\"mod\"}).\nWe also use an imputer to get rid of NaN\\'s:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sklearn\nfrom numpy import number\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nsklearn.set_config(enable_metadata_routing=True)\n\nct = make_column_transformer(\n    (\n        Pipeline(\n            [\n                (\"imputer\", SimpleImputer(strategy=\"mean\")),\n                (\"normalizer\", StandardScaler()),\n            ]\n        ),\n        make_column_selector(dtype_include=number),\n    ),\n    (\n        Pipeline(\n            [\n                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n                (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False)),\n            ]\n        ),\n        make_column_selector(dtype_include=\"category\"),\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define the PyTorch model to be used in the adversarial fairness\nclassifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\n\nclass PredictorModel(torch.nn.Module):\n    def __init__(self):\n        super(PredictorModel, self).__init__()\n        self.layers = torch.nn.Sequential(\n            # in_features is the number of features coming out of the above\n            # ColumnTransformer\n            torch.nn.Linear(in_features=104, out_features=200),\n            torch.nn.LeakyReLU(),\n            torch.nn.Linear(in_features=200, out_features=1),\n            torch.nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n\npredictor_model = PredictorModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also take a look at some validation metrics. Most importantly, we\nchose the demographic parity difference to check to what extent the\nconstraint (demographic parity in this case) is satisfied. We also look\nat the selection rate to observe whether our model is suffering from\nmode collapse, and we also calculate the accuracy on the validation set\nas well. We will pass this validation step to our model later:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from numpy import mean\n\nfrom fairlearn.metrics import demographic_parity_difference\n\n\ndef validate(pipeline, X, y, z, pos_label):\n    predictions = pipeline.predict(X)\n    dp_diff = demographic_parity_difference(\n        y == pos_label,\n        predictions == pos_label,\n        sensitive_features=z,\n    )\n    accuracy = mean(predictions == y.values)\n    selection_rate = mean(predictions == pos_label)\n    print(\n        \"DP diff: {:.4f}, accuracy: {:.4f}, selection_rate: {:.4f}\".format(\n            dp_diff, accuracy, selection_rate\n        )\n    )\n    return dp_diff, accuracy, selection_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We may define the optimizers however we like. In this case, we use the\nsuggestion from the paper to set the hyperparameters $\\alpha$ and\nlearning rate $\\eta$ to depend on the timestep such that $\\alpha \\eta\n\\rightarrow 0$ as the timestep grows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "schedulers = []\n\n\ndef optimizer_constructor(model):\n    global schedulers\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    schedulers.append(torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995))\n    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We make use of a callback function to both update the hyperparameters\nand to validate the model. We update these hyperparameters at every 10\nsteps, and we validate every 100 steps. Additionally, we can implement\nearly stopping easily by calling `return True` in a callback function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n\n\ndef callbacks(model, step, X, y, z, pos_label):\n    global schedulers\n    # Update hyperparameters\n    model.alpha = 0.3 * sqrt(step // 1)\n    for scheduler in schedulers:\n        scheduler.step()\n    # Validate (and early stopping) every 50 steps\n    if step % 50 == 0:\n        dp_diff, accuracy, selection_rate = validate(model, X, y, z, pos_label)\n        # Early stopping condition:\n        # Good accuracy + low dp_diff + no mode collapse\n        if dp_diff < 0.03 and accuracy > 0.8 and selection_rate > 0.01 and selection_rate < 0.99:\n            return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, the instance itself. Notice that we do not explicitly define loss\nfunctions, because adversarial fairness is able to infer the loss\nfunction on its own in this example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fairlearn.adversarial import AdversarialFairnessClassifier\n\nmitigator = AdversarialFairnessClassifier(\n    predictor_model=predictor_model,\n    adversary_model=[3, \"leaky_relu\"],\n    predictor_optimizer=optimizer_constructor,\n    adversary_optimizer=optimizer_constructor,\n    epochs=10,\n    batch_size=2**7,\n    shuffle=True,\n    callbacks=callbacks,\n    random_state=123,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now put the above model in a `Pipeline` with the transformation step.\nNote that we use `scikit-learn`\\'s metadata routing to pass the\nsensitive feature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(ct, mitigator.set_fit_request(sensitive_features=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we fit the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, Y_train, sensitive_features=Z_train)\n\nfrom sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we validate as before, and take a look at the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import MetricFrame, selection_rate\n\n# to see DP difference, accuracy, and selection_rate\nprint(validate(pipeline, X_test, y_test, z=Z_test, pos_label=pos_label))\npredictions = pipeline.predict(X_test)\nmf = MetricFrame(\n    metrics={\"accuracy\": accuracy_score, \"selection_rate\": selection_rate},\n    y_true=y_test == pos_label,\n    y_pred=predictions == pos_label,\n    sensitive_features=Z_test,\n)\nprint(mf.by_group)\n\n# Notice we achieve a much lower demographic parity\n# difference than in Exercise 1! This may come at the cost of some accuracy,\n# but such a tradeoff is to be expected as we are purposely mitigating\n# the unfairness that was present in the data.\n\nsklearn.set_config(enable_metadata_routing=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}