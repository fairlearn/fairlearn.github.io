
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_grid_search_census.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_grid_search_census.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_grid_search_census.py:


===========================
GridSearch with Census Data
===========================

.. GENERATED FROM PYTHON SOURCE LINES 11-27

This notebook shows how to use Fairlearn to generate predictors for the Census dataset.
This dataset is a classification problem - given a range of data about 32,000 individuals,
predict whether their annual income is above or below fifty thousand dollars per year.

For the purposes of this notebook, we shall treat this as a loan decision problem.
We will pretend that the label indicates whether or not each individual repaid a loan in
the past.
We will use the data to train a predictor to predict whether previously unseen individuals
will repay a loan or not.
The assumption is that the model predictions are used to decide whether an individual
should be offered a loan.

We will first train a fairness-unaware predictor and show that it leads to unfair
decisions under a specific notion of fairness called *demographic parity*.
We then mitigate unfairness by applying the :code:`GridSearch` algorithm from the
Fairlearn package.

.. GENERATED FROM PYTHON SOURCE LINES 29-34

Load and preprocess the data set
--------------------------------
We download the data set using `fetch_adult` function in `fairlearn.datasets`.
We start by importing the various modules we're going to use:


.. GENERATED FROM PYTHON SOURCE LINES 34-44

.. code-block:: default


    from sklearn.model_selection import train_test_split
    from fairlearn.reductions import GridSearch
    from fairlearn.reductions import DemographicParity, ErrorRate
    from fairlearn.metrics import MetricFrame, selection_rate, count
    from sklearn.preprocessing import LabelEncoder, StandardScaler
    from sklearn.linear_model import LogisticRegression
    from sklearn import metrics as skm
    import pandas as pd








.. GENERATED FROM PYTHON SOURCE LINES 45-46

We can now load and inspect the data by using the `fairlearn.datasets` module:

.. GENERATED FROM PYTHON SOURCE LINES 46-53

.. code-block:: default


    from sklearn.datasets import fetch_openml
    data = fetch_openml(data_id=1590, as_frame=True)
    X_raw = data.data
    Y = (data.target == '>50K') * 1
    X_raw






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>age</th>
          <th>workclass</th>
          <th>fnlwgt</th>
          <th>education</th>
          <th>education-num</th>
          <th>marital-status</th>
          <th>occupation</th>
          <th>relationship</th>
          <th>race</th>
          <th>sex</th>
          <th>capital-gain</th>
          <th>capital-loss</th>
          <th>hours-per-week</th>
          <th>native-country</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>25.0</td>
          <td>Private</td>
          <td>226802.0</td>
          <td>11th</td>
          <td>7.0</td>
          <td>Never-married</td>
          <td>Machine-op-inspct</td>
          <td>Own-child</td>
          <td>Black</td>
          <td>Male</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>40.0</td>
          <td>United-States</td>
        </tr>
        <tr>
          <th>1</th>
          <td>38.0</td>
          <td>Private</td>
          <td>89814.0</td>
          <td>HS-grad</td>
          <td>9.0</td>
          <td>Married-civ-spouse</td>
          <td>Farming-fishing</td>
          <td>Husband</td>
          <td>White</td>
          <td>Male</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>50.0</td>
          <td>United-States</td>
        </tr>
        <tr>
          <th>2</th>
          <td>28.0</td>
          <td>Local-gov</td>
          <td>336951.0</td>
          <td>Assoc-acdm</td>
          <td>12.0</td>
          <td>Married-civ-spouse</td>
          <td>Protective-serv</td>
          <td>Husband</td>
          <td>White</td>
          <td>Male</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>40.0</td>
          <td>United-States</td>
        </tr>
        <tr>
          <th>3</th>
          <td>44.0</td>
          <td>Private</td>
          <td>160323.0</td>
          <td>Some-college</td>
          <td>10.0</td>
          <td>Married-civ-spouse</td>
          <td>Machine-op-inspct</td>
          <td>Husband</td>
          <td>Black</td>
          <td>Male</td>
          <td>7688.0</td>
          <td>0.0</td>
          <td>40.0</td>
          <td>United-States</td>
        </tr>
        <tr>
          <th>4</th>
          <td>18.0</td>
          <td>NaN</td>
          <td>103497.0</td>
          <td>Some-college</td>
          <td>10.0</td>
          <td>Never-married</td>
          <td>NaN</td>
          <td>Own-child</td>
          <td>White</td>
          <td>Female</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>30.0</td>
          <td>United-States</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>48837</th>
          <td>27.0</td>
          <td>Private</td>
          <td>257302.0</td>
          <td>Assoc-acdm</td>
          <td>12.0</td>
          <td>Married-civ-spouse</td>
          <td>Tech-support</td>
          <td>Wife</td>
          <td>White</td>
          <td>Female</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>38.0</td>
          <td>United-States</td>
        </tr>
        <tr>
          <th>48838</th>
          <td>40.0</td>
          <td>Private</td>
          <td>154374.0</td>
          <td>HS-grad</td>
          <td>9.0</td>
          <td>Married-civ-spouse</td>
          <td>Machine-op-inspct</td>
          <td>Husband</td>
          <td>White</td>
          <td>Male</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>40.0</td>
          <td>United-States</td>
        </tr>
        <tr>
          <th>48839</th>
          <td>58.0</td>
          <td>Private</td>
          <td>151910.0</td>
          <td>HS-grad</td>
          <td>9.0</td>
          <td>Widowed</td>
          <td>Adm-clerical</td>
          <td>Unmarried</td>
          <td>White</td>
          <td>Female</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>40.0</td>
          <td>United-States</td>
        </tr>
        <tr>
          <th>48840</th>
          <td>22.0</td>
          <td>Private</td>
          <td>201490.0</td>
          <td>HS-grad</td>
          <td>9.0</td>
          <td>Never-married</td>
          <td>Adm-clerical</td>
          <td>Own-child</td>
          <td>White</td>
          <td>Male</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>20.0</td>
          <td>United-States</td>
        </tr>
        <tr>
          <th>48841</th>
          <td>52.0</td>
          <td>Self-emp-inc</td>
          <td>287927.0</td>
          <td>HS-grad</td>
          <td>9.0</td>
          <td>Married-civ-spouse</td>
          <td>Exec-managerial</td>
          <td>Wife</td>
          <td>White</td>
          <td>Female</td>
          <td>15024.0</td>
          <td>0.0</td>
          <td>40.0</td>
          <td>United-States</td>
        </tr>
      </tbody>
    </table>
    <p>48842 rows Ã— 14 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 54-60

We are going to treat the sex of each individual as a sensitive
feature (where 0 indicates female and 1 indicates male), and in
this particular case we are going separate this feature out and drop it
from the main data.
We then perform some standard data preprocessing steps to convert the
data into a format suitable for the ML algorithms

.. GENERATED FROM PYTHON SOURCE LINES 60-72

.. code-block:: default


    A = X_raw["sex"]
    X = X_raw.drop(labels=['sex'], axis=1)
    X = pd.get_dummies(X)

    sc = StandardScaler()
    X_scaled = sc.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

    le = LabelEncoder()
    Y = le.fit_transform(Y)








.. GENERATED FROM PYTHON SOURCE LINES 73-74

Finally, we split the data into training and test sets:

.. GENERATED FROM PYTHON SOURCE LINES 74-88

.. code-block:: default


    X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_scaled,
                                                                         Y,
                                                                         A,
                                                                         test_size=0.2,
                                                                         random_state=0,
                                                                         stratify=Y)

    # Work around indexing bug
    X_train = X_train.reset_index(drop=True)
    A_train = A_train.reset_index(drop=True)
    X_test = X_test.reset_index(drop=True)
    A_test = A_test.reset_index(drop=True)








.. GENERATED FROM PYTHON SOURCE LINES 89-96

Training a fairness-unaware predictor
-------------------------------------

To show the effect of Fairlearn we will first train a standard ML predictor
that does not incorporate fairness.
For speed of demonstration, we use the simple
:class:`sklearn.linear_model.LogisticRegression` class:

.. GENERATED FROM PYTHON SOURCE LINES 96-101

.. code-block:: default


    unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)

    unmitigated_predictor.fit(X_train, Y_train)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    LogisticRegression(solver='liblinear')



.. GENERATED FROM PYTHON SOURCE LINES 102-103

We can start to assess the predictor's fairness using the `MetricFrame`:

.. GENERATED FROM PYTHON SOURCE LINES 103-115

.. code-block:: default

    metric_frame = MetricFrame(metrics={"accuracy": skm.accuracy_score,
                                        "selection_rate": selection_rate,
                                        "count": count},
                               sensitive_features=A_test,
                               y_true=Y_test,
                               y_pred=unmitigated_predictor.predict(X_test))
    print(metric_frame.overall)
    print(metric_frame.by_group)
    metric_frame.by_group.plot.bar(
            subplots=True, layout=[3, 1], legend=False, figsize=[12, 8],
            title='Accuracy and selection rate by group')




.. image:: /auto_examples/images/sphx_glr_plot_grid_search_census_001.png
    :alt: Accuracy and selection rate by group, accuracy, selection_rate, count
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    accuracy          0.854335
    selection_rate    0.196233
    count                 9769
    dtype: object
            accuracy  ... count
    sex               ...      
    Female  0.930074  ...  3232
    Male    0.816888  ...  6537

    [2 rows x 3 columns]

    array([[<AxesSubplot:title={'center':'accuracy'}, xlabel='sex'>],
           [<AxesSubplot:title={'center':'selection_rate'}, xlabel='sex'>],
           [<AxesSubplot:title={'center':'count'}, xlabel='sex'>]],
          dtype=object)



.. GENERATED FROM PYTHON SOURCE LINES 116-127

Looking at the disparity in accuracy, we see that males have an error
about three times greater than the females.
More interesting is the disparity in opportunity - males are offered loans at
three times the rate of females.

Despite the fact that we removed the feature from the training data, our
predictor still discriminates based on sex.
This demonstrates that simply ignoring a sensitive feature when fitting a
predictor rarely eliminates unfairness.
There will generally be enough other features correlated with the removed
feature to lead to disparate impact.

.. GENERATED FROM PYTHON SOURCE LINES 129-145

Mitigation with GridSearch
--------------------------

The :class:`fairlearn.reductions.GridSearch` class implements a simplified version of the
exponentiated gradient reduction of `Agarwal et al. 2018 <https://arxiv.org/abs/1803.02453>`_.
The user supplies a standard ML estimator, which is treated as a blackbox.
`GridSearch` works by generating a sequence of relabellings and reweightings, and
trains a predictor for each.

For this example, we specify demographic parity (on the sensitive feature of sex) as
the fairness metric.
Demographic parity requires that individuals are offered the opportunity (are approved
for a loan in this example) independent of membership in the sensitive class (i.e., females
and males should be offered loans at the same rate).
We are using this metric for the sake of simplicity; in general, the appropriate fairness
metric will not be obvious.

.. GENERATED FROM PYTHON SOURCE LINES 145-150

.. code-block:: default


    sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),
                       constraints=DemographicParity(),
                       grid_size=71)








.. GENERATED FROM PYTHON SOURCE LINES 151-158

Our algorithms provide :code:`fit()` and :code:`predict()` methods, so they behave in a similar manner
to other ML packages in Python.
We do however have to specify two extra arguments to :code:`fit()` - the column of sensitive
feature labels, and also the number of predictors to generate in our sweep.

After :code:`fit()` completes, we extract the full set of predictors from the
:class:`fairlearn.reductions.GridSearch` object.

.. GENERATED FROM PYTHON SOURCE LINES 158-164

.. code-block:: default


    sweep.fit(X_train, Y_train,
              sensitive_features=A_train)

    predictors = sweep.predictors_








.. GENERATED FROM PYTHON SOURCE LINES 165-173

We could plot performance and fairness metrics of these predictors now.
However, the plot would be somewhat confusing due to the number of models.
In this case, we are going to remove the predictors which are dominated in the
error-disparity space by others from the sweep (note that the disparity will only be
calculated for the sensitive feature; other potentially sensitive features will
not be mitigated).
In general, one might not want to do this, since there may be other considerations
beyond the strict optimization of error and disparity (of the given sensitive feature).

.. GENERATED FROM PYTHON SOURCE LINES 173-194

.. code-block:: default


    errors, disparities = [], []
    for m in predictors:
        def classifier(X): return m.predict(X)

        error = ErrorRate()
        error.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train)
        disparity = DemographicParity()
        disparity.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train)

        errors.append(error.gamma(classifier)[0])
        disparities.append(disparity.gamma(classifier).max())

    all_results = pd.DataFrame({"predictor": predictors, "error": errors, "disparity": disparities})

    non_dominated = []
    for row in all_results.itertuples():
        errors_for_lower_or_eq_disparity = all_results["error"][all_results["disparity"] <= row.disparity]
        if row.error <= errors_for_lower_or_eq_disparity.min():
            non_dominated.append(row.predictor)








.. GENERATED FROM PYTHON SOURCE LINES 195-196

Finally, we can evaluate the dominant models along with the unmitigated model.

.. GENERATED FROM PYTHON SOURCE LINES 196-220

.. code-block:: default


    predictions = {"unmitigated": unmitigated_predictor.predict(X_test)}
    metric_frames = {"unmitigated": metric_frame}
    for i in range(len(non_dominated)):
        key = "dominant_model_{0}".format(i)
        predictions[key] = non_dominated[i].predict(X_test)

        metric_frames[key] = MetricFrame(metrics={"accuracy": skm.accuracy_score,
                                                  "selection_rate": selection_rate,
                                                  "count": count},
                                         sensitive_features=A_test,
                                         y_true=Y_test,
                                         y_pred=predictions[key])

    import matplotlib.pyplot as plt
    x = [metric_frame.overall['accuracy'] for metric_frame in metric_frames.values()]
    y = [metric_frame.difference()['selection_rate'] for metric_frame in metric_frames.values()]
    keys = list(metric_frames.keys())
    plt.scatter(x, y)
    for i in range(len(x)):
        plt.annotate(keys[i], (x[i] + 0.0003, y[i]))
    plt.xlabel("accuracy")
    plt.ylabel("selection rate difference")




.. image:: /auto_examples/images/sphx_glr_plot_grid_search_census_002.png
    :alt: plot grid search census
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Text(25.222222222222214, 0.5, 'selection rate difference')



.. GENERATED FROM PYTHON SOURCE LINES 221-232

We see a Pareto front forming - the set of predictors which represent optimal tradeoffs
between accuracy and disparity in predictions.
In the ideal case, we would have a predictor at (1,0) - perfectly accurate and without
any unfairness under demographic parity (with respect to the sensitive feature "sex").
The Pareto front represents the closest we can come to this ideal based on our data and
choice of estimator.
Note the range of the axes - the disparity axis covers more values than the accuracy,
so we can reduce disparity substantially for a small loss in accuracy.

In a real example, we would pick the model which represented the best trade-off
between accuracy and disparity given the relevant business constraints.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 5 minutes  22.078 seconds)


.. _sphx_glr_download_auto_examples_plot_grid_search_census.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_grid_search_census.py <plot_grid_search_census.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_grid_search_census.ipynb <plot_grid_search_census.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
