
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_new_metrics.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_new_metrics.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_new_metrics.py:


==============================
Metrics with Multiple Features
==============================

.. GENERATED FROM PYTHON SOURCE LINES 10-53

This notebook demonstrates the new API for metrics, which supports
multiple sensitive and conditional features. This example does not
contain a proper discussion of how fairness relates to the dataset
used, although it does highlight issues which users may want to
consider when analysing their datasets.

We are going to consider a lending scenario, supposing that we have
a model which predicts whether or not a particular customer will
repay a loan. This could be used as the basis of deciding whether
or not to offer that customer a loan. With traditional metrics,
we would assess the model using:

- The 'true' values from the test set
- The model predictions from the test set

Our fairness metrics compute group-based fairness statistics.
To use these, we also need categorical columns from the test
set. For this example, we will include:

- The sex of each individual (two unique values)
- The race of each individual (three unique values)
- The credit score band of each individual (three unique values)
- Whether the loan is considered 'large' or 'small'

An individual's sex and race should not affect a lending decision,
but it would be legitimate to consider an individual's credit score
and the relative size of the loan which they desired.

A real scenario will be more complicated, but this will serve to
illustrate the use of the new metrics.

Getting the Data
================

*This section may be skipped. It simply creates a dataset for
illustrative purposes*

We will use the well-known UCI 'Adult' dataset as the basis of this
demonstration. This is not for a lending scenario, but we will regard
it as one for the purposes of this example. We will use the existing
'race' and 'sex' columns (trimming the former to three unique values),
and manufacture credit score bands and loan sizes from other columns.
We start with some uncontroversial `import` statements:

.. GENERATED FROM PYTHON SOURCE LINES 53-69

.. code-block:: default


    import functools

    import numpy as np
    import sklearn.metrics as skm
    from sklearn.compose import ColumnTransformer
    from sklearn.compose import make_column_selector as selector
    from sklearn.impute import SimpleImputer
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import OneHotEncoder, StandardScaler

    from fairlearn.datasets import fetch_adult
    from fairlearn.metrics import MetricFrame, count, selection_rate








.. GENERATED FROM PYTHON SOURCE LINES 70-71

Next, we import the data:

.. GENERATED FROM PYTHON SOURCE LINES 71-76

.. code-block:: default


    data = fetch_adult()
    X_raw = data.data
    y = (data.target == ">50K") * 1








.. GENERATED FROM PYTHON SOURCE LINES 77-79

For purposes of clarity, we consolidate the 'race' column to have
three unique values:

.. GENERATED FROM PYTHON SOURCE LINES 79-92

.. code-block:: default



    def race_transform(input_str):
        """Reduce values to White, Black and Other."""
        result = "Other"
        if input_str == "White" or input_str == "Black":
            result = input_str
        return result


    X_raw["race"] = X_raw["race"].map(race_transform).fillna("Other").astype("category")
    print(np.unique(X_raw["race"]))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/circleci/tmp-fairlearn/examples/plot_new_metrics.py:89: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame.
    Try using .loc[row_indexer,col_indexer] = value instead

    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      X_raw["race"] = X_raw["race"].map(race_transform).fillna("Other").astype("category")
    ['Black' 'Other' 'White']




.. GENERATED FROM PYTHON SOURCE LINES 93-97

Now, we manufacture the columns for the credit score band and
requested loan size. These are wholly constructed, and not
part of the actual dataset in any way. They are simply for
illustrative purposes.

.. GENERATED FROM PYTHON SOURCE LINES 97-127

.. code-block:: default



    def marriage_transform(m_s_string):
        """Perform some simple manipulations."""
        result = "Low"
        if m_s_string.startswith("Married"):
            result = "Medium"
        elif m_s_string.startswith("Widowed"):
            result = "High"
        return result


    def occupation_transform(occ_string):
        """Perform some simple manipulations."""
        result = "Small"
        if occ_string.startswith("Machine"):
            result = "Large"
        return result


    col_credit = X_raw["marital-status"].map(marriage_transform).fillna("Low")
    col_credit.name = "Credit Score"
    col_loan_size = X_raw["occupation"].map(occupation_transform).fillna("Small")
    col_loan_size.name = "Loan Size"

    A = X_raw[["race", "sex"]]
    A["Credit Score"] = col_credit
    A["Loan Size"] = col_loan_size
    A





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/circleci/tmp-fairlearn/examples/plot_new_metrics.py:123: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame.
    Try using .loc[row_indexer,col_indexer] = value instead

    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      A["Credit Score"] = col_credit
    /home/circleci/tmp-fairlearn/examples/plot_new_metrics.py:124: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame.
    Try using .loc[row_indexer,col_indexer] = value instead

    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      A["Loan Size"] = col_loan_size


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>race</th>
          <th>sex</th>
          <th>Credit Score</th>
          <th>Loan Size</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>Black</td>
          <td>Male</td>
          <td>Low</td>
          <td>Large</td>
        </tr>
        <tr>
          <th>1</th>
          <td>White</td>
          <td>Male</td>
          <td>Medium</td>
          <td>Small</td>
        </tr>
        <tr>
          <th>2</th>
          <td>White</td>
          <td>Male</td>
          <td>Medium</td>
          <td>Small</td>
        </tr>
        <tr>
          <th>3</th>
          <td>Black</td>
          <td>Male</td>
          <td>Medium</td>
          <td>Large</td>
        </tr>
        <tr>
          <th>4</th>
          <td>White</td>
          <td>Female</td>
          <td>Low</td>
          <td>Small</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>48837</th>
          <td>White</td>
          <td>Female</td>
          <td>Medium</td>
          <td>Small</td>
        </tr>
        <tr>
          <th>48838</th>
          <td>White</td>
          <td>Male</td>
          <td>Medium</td>
          <td>Large</td>
        </tr>
        <tr>
          <th>48839</th>
          <td>White</td>
          <td>Female</td>
          <td>High</td>
          <td>Small</td>
        </tr>
        <tr>
          <th>48840</th>
          <td>White</td>
          <td>Male</td>
          <td>Low</td>
          <td>Small</td>
        </tr>
        <tr>
          <th>48841</th>
          <td>White</td>
          <td>Female</td>
          <td>Medium</td>
          <td>Small</td>
        </tr>
      </tbody>
    </table>
    <p>48842 rows Ã— 4 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 128-133

Now that we have imported our dataset and manufactured a few features, we
can perform some more conventional processing. To avoid the problem of
`data leakage <https://en.wikipedia.org/wiki/Leakage_(machine_learning)>`_,
we need to split the data into training and test sets before applying
any transforms or scaling:

.. GENERATED FROM PYTHON SOURCE LINES 133-150

.. code-block:: default



    (X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(
        X_raw, y, A, test_size=0.3, random_state=54321, stratify=y
    )

    # Ensure indices are aligned between X, y and A,
    # after all the slicing and splitting of DataFrames
    # and Series

    X_train = X_train.reset_index(drop=True)
    X_test = X_test.reset_index(drop=True)
    y_train = y_train.reset_index(drop=True)
    y_test = y_test.reset_index(drop=True)
    A_train = A_train.reset_index(drop=True)
    A_test = A_test.reset_index(drop=True)








.. GENERATED FROM PYTHON SOURCE LINES 151-160

Next, we build two :class:`~sklearn.pipeline.Pipeline` objects
to process the columns, one for numeric data, and the other
for categorical data. Both impute missing values; the difference
is whether the data are scaled (numeric columns) or
one-hot encoded (categorical columns). Imputation of missing
values should generally be done with care, since it could
potentially introduce biases. Of course, removing rows with
missing data could also cause trouble, if particular subgroups
have poorer data quality.

.. GENERATED FROM PYTHON SOURCE LINES 160-177

.. code-block:: default


    numeric_transformer = Pipeline(
        steps=[("impute", SimpleImputer()), ("scaler", StandardScaler())]
    )
    categorical_transformer = Pipeline(
        [
            ("impute", SimpleImputer(strategy="most_frequent")),
            ("ohe", OneHotEncoder(handle_unknown="ignore")),
        ]
    )
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, selector(dtype_exclude="category")),
            ("cat", categorical_transformer, selector(dtype_include="category")),
        ]
    )








.. GENERATED FROM PYTHON SOURCE LINES 178-180

With our preprocessor defined, we can now build a
new pipeline which includes an Estimator:

.. GENERATED FROM PYTHON SOURCE LINES 180-188

.. code-block:: default


    unmitigated_predictor = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("classifier", LogisticRegression(solver="liblinear", fit_intercept=True)),
        ]
    )








.. GENERATED FROM PYTHON SOURCE LINES 189-192

With the pipeline fully defined, we can first train it
with the training data, and then generate predictions
from the test data.

.. GENERATED FROM PYTHON SOURCE LINES 192-197

.. code-block:: default


    unmitigated_predictor.fit(X_train, y_train)
    y_pred = unmitigated_predictor.predict(X_test)









.. GENERATED FROM PYTHON SOURCE LINES 198-214

Analysing the Model with Metrics
================================

After our data manipulations and model training, we have the following
from our test set:

- A vector of true values called ``y_test``
- A vector of model predictions called ``y_pred``
- A DataFrame of categorical features relevant to fairness called ``A_test``

In a traditional model analysis, we would now look at some metrics
evaluated on the entire dataset. Suppose in this case, the relevant
metrics are :func:`fairlearn.metrics.selection_rate` and
:func:`sklearn.metrics.fbeta_score` (with
``beta=0.6``).
We can evaluate these metrics directly:

.. GENERATED FROM PYTHON SOURCE LINES 214-218

.. code-block:: default


    print("Selection Rate:", selection_rate(y_test, y_pred))
    print("fbeta:", skm.fbeta_score(y_test, y_pred, beta=0.6))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Selection Rate: 0.1947041561454992
    fbeta: 0.6827826864569057




.. GENERATED FROM PYTHON SOURCE LINES 219-225

We know that there are sensitive features in our data, and we want to
ensure that we're not harming individuals due to membership in any of
these groups. For this purpose, Fairlearn provides the
:class:`fairlearn.metrics.MetricFrame`
class. Let us construct an instance of this class, and then look at
its capabilities:

.. GENERATED FROM PYTHON SOURCE LINES 225-234

.. code-block:: default


    fbeta_06 = functools.partial(skm.fbeta_score, beta=0.6, zero_division=1)

    metric_fns = {"selection_rate": selection_rate, "fbeta_06": fbeta_06, "count": count}

    grouped_on_sex = MetricFrame(
        metrics=metric_fns, y_true=y_test, y_pred=y_pred, sensitive_features=A_test["sex"]
    )








.. GENERATED FROM PYTHON SOURCE LINES 235-256

The :class:`fairlearn.metrics.MetricFrame` object requires a
minimum of four arguments:

1. The underlying metric function(s) to be evaluated
2. The true values
3. The predicted values
4. The sensitive feature values

These are all passed as arguments to the constructor. If more than
one underlying metric is required (as in this case), then we must
provide them in a dictionary.

The underlying metrics must have a signature ``fn(y_true, y_pred)``,
so we have to use :func:`functools.partial` on ``fbeta_score()`` to
furnish ``beta=0.6`` (we will show how to pass in extra array
arguments such as sample weights shortly).

We will now take a closer look at the :class:`fairlearn.metrics.MetricFrame`
object. First, there is the ``overall`` property, which contains
the metrics evaluated on the entire dataset. We see that this contains the
same values calculated above:

.. GENERATED FROM PYTHON SOURCE LINES 256-261

.. code-block:: default


    assert grouped_on_sex.overall["selection_rate"] == selection_rate(y_test, y_pred)
    assert grouped_on_sex.overall["fbeta_06"] == skm.fbeta_score(y_test, y_pred, beta=0.6)
    print(grouped_on_sex.overall)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    selection_rate        0.194704
    fbeta_06              0.682783
    count             14653.000000
    dtype: float64




.. GENERATED FROM PYTHON SOURCE LINES 262-267

The other property in the :class:`fairlearn.metrics.MetricFrame` object
is ``by_group``. This contains the metrics evaluated on each subgroup defined
by the categories in the ``sensitive_features=`` argument. Note that
:func:`fairlearn.metrics.count` can be used to display the number of
data points in each subgroup. In this case, we have results for males and females:

.. GENERATED FROM PYTHON SOURCE LINES 267-270

.. code-block:: default


    grouped_on_sex.by_group






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>selection_rate</th>
          <th>fbeta_06</th>
          <th>count</th>
        </tr>
        <tr>
          <th>sex</th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>Female</th>
          <td>0.06883</td>
          <td>0.634014</td>
          <td>4838.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.25675</td>
          <td>0.689789</td>
          <td>9815.0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 271-276

We can immediately see a substantial disparity in the selection rate between
males and females.

We can also create another :class:`fairlearn.metrics.MetricFrame` object
using race as the sensitive feature:

.. GENERATED FROM PYTHON SOURCE LINES 276-281

.. code-block:: default


    grouped_on_race = MetricFrame(
        metrics=metric_fns, y_true=y_test, y_pred=y_pred, sensitive_features=A_test["race"]
    )








.. GENERATED FROM PYTHON SOURCE LINES 282-283

The ``overall`` property is unchanged:

.. GENERATED FROM PYTHON SOURCE LINES 283-285

.. code-block:: default

    assert (grouped_on_sex.overall == grouped_on_race.overall).all()








.. GENERATED FROM PYTHON SOURCE LINES 286-288

The ``by_group`` property now contains the metrics evaluated based on the 'race'
column:

.. GENERATED FROM PYTHON SOURCE LINES 288-290

.. code-block:: default

    grouped_on_race.by_group






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>selection_rate</th>
          <th>fbeta_06</th>
          <th>count</th>
        </tr>
        <tr>
          <th>race</th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>Black</th>
          <td>0.068198</td>
          <td>0.592125</td>
          <td>1437.0</td>
        </tr>
        <tr>
          <th>Other</th>
          <td>0.167630</td>
          <td>0.693717</td>
          <td>692.0</td>
        </tr>
        <tr>
          <th>White</th>
          <td>0.210715</td>
          <td>0.686081</td>
          <td>12524.0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 291-293

We see that there is also a significant disparity in selection rates when
grouping by race.

.. GENERATED FROM PYTHON SOURCE LINES 295-312

Sample weights and other arrays
-------------------------------

We noted above that the underlying metric functions passed to the
:class:`fairlearn.metrics.MetricFrame` constructor need to be of
the form ``fn(y_true, y_pred)`` - we do not support scalar arguments
such as ``pos_label=`` or ``beta=`` in the constructor. Such
arguments should be bound into a new function using
:func:`functools.partial`, and the result passed in. However, we do
support arguments which have one entry for each sample, with an array
of sample weights being the most common example. These are divided
into subgroups along with ``y_true`` and ``y_pred``, and passed along
to the underlying metric.

To use these arguments, we pass in a dictionary as the ``sample_params=``
argument of the constructor. Let us generate some random weights, and
pass these along:

.. GENERATED FROM PYTHON SOURCE LINES 312-329

.. code-block:: default


    random_weights = np.random.rand(len(y_test))

    example_sample_params = {
        "selection_rate": {"sample_weight": random_weights},
        "fbeta_06": {"sample_weight": random_weights},
    }


    grouped_with_weights = MetricFrame(
        metrics=metric_fns,
        y_true=y_test,
        y_pred=y_pred,
        sensitive_features=A_test["sex"],
        sample_params=example_sample_params,
    )








.. GENERATED FROM PYTHON SOURCE LINES 330-331

We can inspect the overall values, and check they are as expected:

.. GENERATED FROM PYTHON SOURCE LINES 331-339

.. code-block:: default

    assert grouped_with_weights.overall["selection_rate"] == selection_rate(
        y_test, y_pred, sample_weight=random_weights
    )
    assert grouped_with_weights.overall["fbeta_06"] == skm.fbeta_score(
        y_test, y_pred, beta=0.6, sample_weight=random_weights
    )
    print(grouped_with_weights.overall)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    selection_rate        0.197356
    fbeta_06              0.677371
    count             14653.000000
    dtype: float64




.. GENERATED FROM PYTHON SOURCE LINES 340-341

We can also see the effect on the metric being evaluated on the subgroups:

.. GENERATED FROM PYTHON SOURCE LINES 341-343

.. code-block:: default

    grouped_with_weights.by_group






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>selection_rate</th>
          <th>fbeta_06</th>
          <th>count</th>
        </tr>
        <tr>
          <th>sex</th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>Female</th>
          <td>0.071326</td>
          <td>0.63530</td>
          <td>4838.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.258733</td>
          <td>0.68351</td>
          <td>9815.0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 344-361

Quantifying Disparities
=======================

We now know that our model is selecting individuals who are female far less
often than individuals who are male. There is a similar effect when
examining the results by race, with blacks being selected far less often than
whites (and those classified as 'other'). However, there are many cases where
presenting all these numbers at once will not be useful (for example, a high
level dashboard which is monitoring model performance). Fairlearn provides
several means of aggregating metrics across the subgroups, so that disparities
can be readily quantified.

The simplest of these aggregations is ``group_min()``, which reports the
minimum value seen for a subgroup for each underlying metric (we also provide
``group_max()``). This is
useful if there is a mandate that "no subgroup should have an ``fbeta_score()``
of less than 0.6." We can evaluate the minimum values easily:

.. GENERATED FROM PYTHON SOURCE LINES 361-363

.. code-block:: default

    grouped_on_race.group_min()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    selection_rate    0.068198
    fbeta_06          0.592125
    count                692.0
    dtype: object



.. GENERATED FROM PYTHON SOURCE LINES 364-368

As noted above, the selection rates varies greatly by race and by sex.
This can be quantified in terms of a difference between the subgroup with
the highest value of the metric, and the subgroup with the lowest value.
For this, we provide the method ``difference(method='between_groups)``:

.. GENERATED FROM PYTHON SOURCE LINES 368-370

.. code-block:: default

    grouped_on_race.difference(method="between_groups")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    selection_rate        0.142518
    fbeta_06              0.101591
    count             11832.000000
    dtype: float64



.. GENERATED FROM PYTHON SOURCE LINES 371-374

We can also evaluate the difference relative to the corresponding overall
value of the metric. In this case we take the absolute value, so that the
result is always positive:

.. GENERATED FROM PYTHON SOURCE LINES 374-376

.. code-block:: default

    grouped_on_race.difference(method="to_overall")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    selection_rate        0.126507
    fbeta_06              0.090657
    count             13961.000000
    dtype: float64



.. GENERATED FROM PYTHON SOURCE LINES 377-380

There are situations where knowing the ratios of the metrics evaluated on
the subgroups is more useful. For this we have the ``ratio()`` method.
We can take the ratios between the minimum and maximum values of each metric:

.. GENERATED FROM PYTHON SOURCE LINES 380-382

.. code-block:: default

    grouped_on_race.ratio(method="between_groups")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    selection_rate    0.323648
    fbeta_06          0.853555
    count             0.055254
    dtype: float64



.. GENERATED FROM PYTHON SOURCE LINES 383-386

We can also compute the ratios relative to the overall value for each
metric. Analogous to the differences, the ratios are always in the range
:math:`[0,1]`:

.. GENERATED FROM PYTHON SOURCE LINES 386-388

.. code-block:: default

    grouped_on_race.ratio(method="to_overall")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    selection_rate    0.350263
    fbeta_06          0.867223
    count             0.047226
    dtype: float64



.. GENERATED FROM PYTHON SOURCE LINES 389-403

Intersections of Features
=========================

So far we have only considered a single sensitive feature at a time,
and we have already found some serious issues in our example data.
However, sometimes serious issues can be hiding in intersections of
features. For example, the
`Gender Shades project <https://www.media.mit.edu/projects/gender-shades/overview/>`_
found that facial recognition algorithms performed worse for blacks
than whites, and also worse for women than men (despite overall high
accuracy score). Moreover, performance on black females was *terrible*.
We can examine the intersections of sensitive features by passing
multiple columns to the :class:`fairlearn.metrics.MetricFrame`
constructor:

.. GENERATED FROM PYTHON SOURCE LINES 403-411

.. code-block:: default


    grouped_on_race_and_sex = MetricFrame(
        metrics=metric_fns,
        y_true=y_test,
        y_pred=y_pred,
        sensitive_features=A_test[["race", "sex"]],
    )








.. GENERATED FROM PYTHON SOURCE LINES 412-414

The overall values are unchanged, but the ``by_group`` table now
shows the intersections between subgroups:

.. GENERATED FROM PYTHON SOURCE LINES 414-417

.. code-block:: default

    assert (grouped_on_race_and_sex.overall == grouped_on_race.overall).all()
    grouped_on_race_and_sex.by_group






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th></th>
          <th>selection_rate</th>
          <th>fbeta_06</th>
          <th>count</th>
        </tr>
        <tr>
          <th>race</th>
          <th>sex</th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="2" valign="top">Black</th>
          <th>Female</th>
          <td>0.032258</td>
          <td>0.630316</td>
          <td>713.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.103591</td>
          <td>0.580624</td>
          <td>724.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">Other</th>
          <th>Female</th>
          <td>0.070866</td>
          <td>0.503704</td>
          <td>254.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.223744</td>
          <td>0.728972</td>
          <td>438.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">White</th>
          <th>Female</th>
          <td>0.075433</td>
          <td>0.642076</td>
          <td>3871.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.271235</td>
          <td>0.692069</td>
          <td>8653.0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 418-423

The aggregations are still performed across all subgroups for each metric,
so each continues to reduce to a single value. If we look at the
``group_min()``, we see that we violate the mandate we specified for the
``fbeta_score()`` suggested above (for females with a race of 'Other' in
fact):

.. GENERATED FROM PYTHON SOURCE LINES 423-425

.. code-block:: default

    grouped_on_race_and_sex.group_min()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    selection_rate    0.032258
    fbeta_06          0.503704
    count                254.0
    dtype: object



.. GENERATED FROM PYTHON SOURCE LINES 426-429

Looking at the ``ratio()`` method, we see that the disparity is worse
(specifically between white males and black females, if we check in
the ``by_group`` table):

.. GENERATED FROM PYTHON SOURCE LINES 429-431

.. code-block:: default

    grouped_on_race_and_sex.ratio(method="between_groups")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    selection_rate    0.118930
    fbeta_06          0.690978
    count             0.029354
    dtype: float64



.. GENERATED FROM PYTHON SOURCE LINES 432-446

Control Features
================

There is a further way we can slice up our data. We have (*completely
made up*) features for the individuals' credit scores (in three bands)
and also the size of the loan requested (large or small). In our loan
scenario, it is acceptable that individuals with high credit scores
are selected more often than individuals with low credit scores.
However, within each credit score band, we do not want a disparity
between (say) black females and white males. To example these cases,
we have the concept of *control features*.

Control features are introduced by the ``control_features=``
argument to the :class:`fairlearn.metrics.MetricFrame` object:

.. GENERATED FROM PYTHON SOURCE LINES 446-454

.. code-block:: default

    cond_credit_score = MetricFrame(
        metrics=metric_fns,
        y_true=y_test,
        y_pred=y_pred,
        sensitive_features=A_test[["race", "sex"]],
        control_features=A_test["Credit Score"],
    )








.. GENERATED FROM PYTHON SOURCE LINES 455-458

This has an immediate effect on the ``overall`` property. Instead
of having one value for each metric, we now have a value for each
unique value of the control feature:

.. GENERATED FROM PYTHON SOURCE LINES 458-460

.. code-block:: default

    cond_credit_score.overall






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>selection_rate</th>
          <th>fbeta_06</th>
          <th>count</th>
        </tr>
        <tr>
          <th>Credit Score</th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>High</th>
          <td>0.036170</td>
          <td>0.664928</td>
          <td>470.0</td>
        </tr>
        <tr>
          <th>Low</th>
          <td>0.022924</td>
          <td>0.549994</td>
          <td>7285.0</td>
        </tr>
        <tr>
          <th>Medium</th>
          <td>0.386924</td>
          <td>0.695034</td>
          <td>6898.0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 461-462

The ``by_group`` property is similarly expanded:

.. GENERATED FROM PYTHON SOURCE LINES 462-464

.. code-block:: default

    cond_credit_score.by_group






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th></th>
          <th></th>
          <th>selection_rate</th>
          <th>fbeta_06</th>
          <th>count</th>
        </tr>
        <tr>
          <th>Credit Score</th>
          <th>race</th>
          <th>sex</th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="6" valign="top">High</th>
          <th rowspan="2" valign="top">Black</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>54.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.066667</td>
          <td>1.000000</td>
          <td>15.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">Other</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>21.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>4.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">White</th>
          <th>Female</th>
          <td>0.019608</td>
          <td>0.529595</td>
          <td>306.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.142857</td>
          <td>0.759305</td>
          <td>70.0</td>
        </tr>
        <tr>
          <th rowspan="6" valign="top">Low</th>
          <th rowspan="2" valign="top">Black</th>
          <th>Female</th>
          <td>0.007030</td>
          <td>0.626728</td>
          <td>569.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.020513</td>
          <td>0.563536</td>
          <td>390.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">Other</th>
          <th>Female</th>
          <td>0.012048</td>
          <td>0.519084</td>
          <td>166.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.037267</td>
          <td>0.693878</td>
          <td>161.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">White</th>
          <th>Female</th>
          <td>0.015084</td>
          <td>0.525773</td>
          <td>2917.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.033420</td>
          <td>0.550250</td>
          <td>3082.0</td>
        </tr>
        <tr>
          <th rowspan="6" valign="top">Medium</th>
          <th rowspan="2" valign="top">Black</th>
          <th>Female</th>
          <td>0.211111</td>
          <td>0.639653</td>
          <td>90.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.206897</td>
          <td>0.577576</td>
          <td>319.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">Other</th>
          <th>Female</th>
          <td>0.238806</td>
          <td>0.500000</td>
          <td>67.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.336996</td>
          <td>0.732057</td>
          <td>273.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">White</th>
          <th>Female</th>
          <td>0.373457</td>
          <td>0.680881</td>
          <td>648.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.406108</td>
          <td>0.700837</td>
          <td>5501.0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 465-467

The aggregates are also evaluated once for each group identified
by the control feature:

.. GENERATED FROM PYTHON SOURCE LINES 467-469

.. code-block:: default

    cond_credit_score.group_min()






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>selection_rate</th>
          <th>fbeta_06</th>
          <th>count</th>
        </tr>
        <tr>
          <th>Credit Score</th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>High</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>4.0</td>
        </tr>
        <tr>
          <th>Low</th>
          <td>0.007030</td>
          <td>0.519084</td>
          <td>161.0</td>
        </tr>
        <tr>
          <th>Medium</th>
          <td>0.206897</td>
          <td>0.500000</td>
          <td>67.0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 470-471

And:

.. GENERATED FROM PYTHON SOURCE LINES 471-473

.. code-block:: default

    cond_credit_score.ratio(method="between_groups")






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>selection_rate</th>
          <th>fbeta_06</th>
          <th>count</th>
        </tr>
        <tr>
          <th>Credit Score</th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>High</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.013072</td>
        </tr>
        <tr>
          <th>Low</th>
          <td>0.188635</td>
          <td>0.748092</td>
          <td>0.052239</td>
        </tr>
        <tr>
          <th>Medium</th>
          <td>0.509462</td>
          <td>0.683007</td>
          <td>0.012180</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 474-479

In our data, we see that we have a dearth of positive results
for high income non-whites, which significantly affects the
aggregates.

We can continue adding more control features:

.. GENERATED FROM PYTHON SOURCE LINES 479-487

.. code-block:: default

    cond_both = MetricFrame(
        metrics=metric_fns,
        y_true=y_test,
        y_pred=y_pred,
        sensitive_features=A_test[["race", "sex"]],
        control_features=A_test[["Loan Size", "Credit Score"]],
    )








.. GENERATED FROM PYTHON SOURCE LINES 488-489

The ``overall`` property now splits into more values:

.. GENERATED FROM PYTHON SOURCE LINES 489-491

.. code-block:: default

    cond_both.overall






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th></th>
          <th>selection_rate</th>
          <th>fbeta_06</th>
          <th>count</th>
        </tr>
        <tr>
          <th>Loan Size</th>
          <th>Credit Score</th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="3" valign="top">Large</th>
          <th>High</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>23.0</td>
        </tr>
        <tr>
          <th>Low</th>
          <td>0.004348</td>
          <td>0.601770</td>
          <td>460.0</td>
        </tr>
        <tr>
          <th>Medium</th>
          <td>0.071429</td>
          <td>0.388325</td>
          <td>434.0</td>
        </tr>
        <tr>
          <th rowspan="3" valign="top">Small</th>
          <th>High</th>
          <td>0.038031</td>
          <td>0.664928</td>
          <td>447.0</td>
        </tr>
        <tr>
          <th>Low</th>
          <td>0.024176</td>
          <td>0.549299</td>
          <td>6825.0</td>
        </tr>
        <tr>
          <th>Medium</th>
          <td>0.408106</td>
          <td>0.700288</td>
          <td>6464.0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 492-494

As does the ``by_groups`` property, where ``NaN`` values
indicate that there were no samples in the cell:

.. GENERATED FROM PYTHON SOURCE LINES 494-496

.. code-block:: default

    cond_both.by_group






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th>selection_rate</th>
          <th>fbeta_06</th>
          <th>count</th>
        </tr>
        <tr>
          <th>Loan Size</th>
          <th>Credit Score</th>
          <th>race</th>
          <th>sex</th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="18" valign="top">Large</th>
          <th rowspan="6" valign="top">High</th>
          <th rowspan="2" valign="top">Black</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>5.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">Other</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>3.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">White</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>13.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th rowspan="6" valign="top">Low</th>
          <th rowspan="2" valign="top">Black</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>52.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.030303</td>
          <td>1.000000</td>
          <td>33.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">Other</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>3.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>14.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">White</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>133.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.004444</td>
          <td>0.557377</td>
          <td>225.0</td>
        </tr>
        <tr>
          <th rowspan="6" valign="top">Medium</th>
          <th rowspan="2" valign="top">Black</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>7.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.026316</td>
          <td>0.295652</td>
          <td>38.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">Other</th>
          <th>Female</th>
          <td>0.111111</td>
          <td>0.000000</td>
          <td>9.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>19.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">White</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>28.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.087087</td>
          <td>0.420976</td>
          <td>333.0</td>
        </tr>
        <tr>
          <th rowspan="18" valign="top">Small</th>
          <th rowspan="6" valign="top">High</th>
          <th rowspan="2" valign="top">Black</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>49.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.071429</td>
          <td>1.000000</td>
          <td>14.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">Other</th>
          <th>Female</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>18.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>4.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">White</th>
          <th>Female</th>
          <td>0.020478</td>
          <td>0.529595</td>
          <td>293.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.144928</td>
          <td>0.759305</td>
          <td>69.0</td>
        </tr>
        <tr>
          <th rowspan="6" valign="top">Low</th>
          <th rowspan="2" valign="top">Black</th>
          <th>Female</th>
          <td>0.007737</td>
          <td>0.626728</td>
          <td>517.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.019608</td>
          <td>0.518293</td>
          <td>357.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">Other</th>
          <th>Female</th>
          <td>0.012270</td>
          <td>0.519084</td>
          <td>163.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.040816</td>
          <td>0.715789</td>
          <td>147.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">White</th>
          <th>Female</th>
          <td>0.015805</td>
          <td>0.527656</td>
          <td>2784.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.035702</td>
          <td>0.550162</td>
          <td>2857.0</td>
        </tr>
        <tr>
          <th rowspan="6" valign="top">Medium</th>
          <th rowspan="2" valign="top">Black</th>
          <th>Female</th>
          <td>0.228916</td>
          <td>0.648094</td>
          <td>83.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.231317</td>
          <td>0.590371</td>
          <td>281.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">Other</th>
          <th>Female</th>
          <td>0.258621</td>
          <td>0.524085</td>
          <td>58.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.362205</td>
          <td>0.740024</td>
          <td>254.0</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">White</th>
          <th>Female</th>
          <td>0.390323</td>
          <td>0.682328</td>
          <td>620.0</td>
        </tr>
        <tr>
          <th>Male</th>
          <td>0.426664</td>
          <td>0.705861</td>
          <td>5168.0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 497-499

The aggregates behave similarly. By this point, we are having significant issues
with under-populated intersections. Consider:

.. GENERATED FROM PYTHON SOURCE LINES 499-516

.. code-block:: default



    def member_counts(y_true, y_pred):
        assert len(y_true) == len(y_pred)
        return len(y_true)


    counts = MetricFrame(
        metrics=member_counts,
        y_true=y_test,
        y_pred=y_pred,
        sensitive_features=A_test[["race", "sex"]],
        control_features=A_test[["Loan Size", "Credit Score"]],
    )

    counts.by_group





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Loan Size  Credit Score  race   sex   
    Large      High          Black  Female       5.0
                                    Male         1.0
                             Other  Female       3.0
                                    Male         NaN
                             White  Female      13.0
                                    Male         1.0
               Low           Black  Female      52.0
                                    Male        33.0
                             Other  Female       3.0
                                    Male        14.0
                             White  Female     133.0
                                    Male       225.0
               Medium        Black  Female       7.0
                                    Male        38.0
                             Other  Female       9.0
                                    Male        19.0
                             White  Female      28.0
                                    Male       333.0
    Small      High          Black  Female      49.0
                                    Male        14.0
                             Other  Female      18.0
                                    Male         4.0
                             White  Female     293.0
                                    Male        69.0
               Low           Black  Female     517.0
                                    Male       357.0
                             Other  Female     163.0
                                    Male       147.0
                             White  Female    2784.0
                                    Male      2857.0
               Medium        Black  Female      83.0
                                    Male       281.0
                             Other  Female      58.0
                                    Male       254.0
                             White  Female     620.0
                                    Male      5168.0
    Name: member_counts, dtype: float64



.. GENERATED FROM PYTHON SOURCE LINES 517-519

Recall that ``NaN`` indicates that there were no individuals
in a cell - ``member_counts()`` will not even have been called.

.. GENERATED FROM PYTHON SOURCE LINES 521-530

Exporting from MetricFrame
==========================

Sometimes, we need to extract our data for use in other tools.
For this, we can use the :py:meth:`pandas.DataFrame.to_csv` method,
since the :py:meth:`~fairlearn.metrics.MetricFrame.by_group` property
will be a :class:`pandas.DataFrame` (or in a few cases, it will be
a :class:`pandas.Series`, but that has a similar
:py:meth:`~pandas.Series.to_csv` method):

.. GENERATED FROM PYTHON SOURCE LINES 530-534

.. code-block:: default


    csv_output = cond_credit_score.by_group.to_csv()
    print(csv_output)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Credit Score,race,sex,selection_rate,fbeta_06,count
    High,Black,Female,0.0,0.0,54.0
    High,Black,Male,0.06666666666666667,1.0,15.0
    High,Other,Female,0.0,1.0,21.0
    High,Other,Male,0.0,1.0,4.0
    High,White,Female,0.0196078431372549,0.5295950155763239,306.0
    High,White,Male,0.14285714285714285,0.7593052109181142,70.0
    Low,Black,Female,0.007029876977152899,0.6267281105990783,569.0
    Low,Black,Male,0.020512820512820513,0.56353591160221,390.0
    Low,Other,Female,0.012048192771084338,0.5190839694656488,166.0
    Low,Other,Male,0.037267080745341616,0.6938775510204082,161.0
    Low,White,Female,0.015083990401097017,0.5257731958762887,2917.0
    Low,White,Male,0.033419857235561325,0.5502497502497502,3082.0
    Medium,Black,Female,0.2111111111111111,0.6396526772793053,90.0
    Medium,Black,Male,0.20689655172413793,0.5775764439411097,319.0
    Medium,Other,Female,0.23880597014925373,0.5,67.0
    Medium,Other,Male,0.336996336996337,0.7320574162679426,273.0
    Medium,White,Female,0.3734567901234568,0.6808811402992107,648.0
    Medium,White,Male,0.40610798036720597,0.700837357443748,5501.0





.. GENERATED FROM PYTHON SOURCE LINES 535-542

The :py:meth:`pandas.DataFrame.to_csv` method has a large number of
arguments to control the exported CSV. For example, it can write
directly to a CSV file, rather than returning a string (as shown
above).

The :meth:`~fairlearn.metrics.MetricFrame.overall` property can
be handled similarly, in the cases that it is not a scalar.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  4.001 seconds)


.. _sphx_glr_download_auto_examples_plot_new_metrics.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_new_metrics.py <plot_new_metrics.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_new_metrics.ipynb <plot_new_metrics.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
