
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Intersectionality in Mental Health Care &#8212; Fairlearn 0.13.0.dev0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Vibur" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyterlite_sphinx.css?v=2c9f8f05" />
    <link rel="stylesheet" type="text/css" href="../_static/css/hide_links.css?v=81b1ad43" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=b01019a4" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=323ffc05"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=97f0b27d"></script>
    <script src="../_static/jupyterlite_sphinx.js?v=96e329c5"></script>
    <script>window.MathJax = {"tex": {"macros": {"E": "{\\mathbb{E}}", "P": "{\\mathbb{P}}", "given": "\\mathbin{\\vert}"}}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'auto_examples/plot_intersectional_bias';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://fairlearn.org/main/_static/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>
    <link rel="icon" href="../_static/fairlearn-favicon.ico"/>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Credit Loan Decisions" href="plot_credit_loan_decisions.html" />
    <link rel="prev" title="Metrics with Multiple Features" href="plot_new_metrics.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
     
  

<a class="navbar-brand logo" href="https://fairlearn.org">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/fairlearn_full_color_lightmode.svg" class="logo__image only-light" alt="Fairlearn 0.13.0.dev0 documentation - Home"/>
    <img src="../_static/fairlearn_full_color_darkmode.svg" class="logo__image only-dark pst-js-only" alt="Fairlearn 0.13.0.dev0 documentation - Home"/>
  
  
</a></div>
    
      <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../quickstart.html">
    Get Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide/index.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api_reference/index.html">
    API Docs
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Example Notebooks
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributor_guide/index.html">
    Contributor Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about/index.html">
    About Us
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/fairlearn/fairlearn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.linkedin.com/company/fairlearn/" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://stackoverflow.com/questions/tagged/fairlearn" title="StackOverflow" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-stack-overflow fa-lg" aria-hidden="true"></i>
            <span class="sr-only">StackOverflow</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/R22yCfgsRn" title="Discord" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../quickstart.html">
    Get Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide/index.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api_reference/index.html">
    API Docs
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Example Notebooks
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributor_guide/index.html">
    Contributor Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about/index.html">
    About Us
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/fairlearn/fairlearn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.linkedin.com/company/fairlearn/" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://stackoverflow.com/questions/tagged/fairlearn" title="StackOverflow" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-stack-overflow fa-lg" aria-hidden="true"></i>
            <span class="sr-only">StackOverflow</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/R22yCfgsRn" title="Discord" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="plot_quickstart_counts.html">Value counts</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_adversarial_basics.html">Basics &amp; Model Specification of <cite>AdversarialFairnessClassifier</cite></a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_correlationremover_before_after.html">CorrelationRemover visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_quickstart.html">MetricFrame visualizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_mitigation_pipeline.html">Passing pipelines to mitigation techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_metricframe_beyond_binary_classification.html">MetricFrame: Beyond Binary Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_make_derived_metric.html">Making Derived Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_grid_search_census.html">GridSearch with Census Data</a></li>

<li class="toctree-l1"><a class="reference internal" href="plot_adversarial_fine_tuning.html">Fine Tuning ad <code class="docutils literal notranslate"><span class="pre">AdversarialFairnessClassifier</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_plotting_metrics_with_error.html">Plotting Metrics with Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_new_metrics.html">Metrics with Multiple Features</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Intersectionality in Mental Health Care</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_credit_loan_decisions.html">Credit Loan Decisions</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Example Notebooks</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Intersectionality in Mental Health Care</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-plot-intersectional-bias-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code. or to run this example in your browser via JupyterLite</p>
</div>
<section class="sphx-glr-example-title" id="intersectionality-in-mental-health-care">
<span id="sphx-glr-auto-examples-plot-intersectional-bias-py"></span><h1>Intersectionality in Mental Health Care<a class="headerlink" href="#intersectionality-in-mental-health-care" title="Link to this heading">#</a></h1>
<p><em>This notebook was written by Yifan Wang, Marta Maslej, and Laura Sikstrom and is licenced
under a</em> <a class="reference external" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International
License</a>.</p>
<hr class="docutils" />
<p>There is an increasing interest in applying innovations in artificial intelligence to provide
more efficient, precise, or personalized patient care. Specifically, the ability of machine
learning (ML) to identify patterns in large and complex datasets holds tremendous promise for
solving some of the most complex and intractable problems in health care. However, there are
ongoing questions <a class="footnote-reference brackets" href="#footcite-ghassemi2022medicine" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> about a range of known <strong>gendered and
racialized biases</strong> - arising from diagnostic tools, clinical interactions, and health policies -
that get <em>baked</em> into these datasets. In nephrology, for example, algorithms developed to
estimate glomerular filtration rate assign higher values (which suggest better kidney function)
to Black individuals <a class="footnote-reference brackets" href="#footcite-vyas2020hidden" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, which could delay treatent for this patient
group while ultimately worsening outcomes. In turn, any ML algorithms that are trained on this
data could exhibit <strong>differences in predictive performance across certain groups</strong> - not by any
flaw of the algorithm itself, but because it is capturing societal biases that are encoded into
the data itself. Upon deployment, these tools can <em>amplify harms</em> for marginalized populations,
particularly those defined by intersections of features (e.g., gender, race, class).</p>
<p>In this tutorial, we will examine another instance of this: in psychiatric diagnosis data, Black
men are much more likely to be misdiagnosed with schizophrenia as compared to white men due to
factors such as diagnostic bias by clinicians. Through this case study, we demonstrate the
<strong>value of applying an interdisciplinary approach to analyzing intersectional biases</strong>, towards
ensuring that these innovative tools are implemented safely and ethically.</p>
<hr class="docutils" />
<p><strong>Learning objectives</strong>. This notebook has three main learning objectives. After this tutorial,
we hope that you will be able to:</p>
<ol class="arabic simple">
<li><p>Think critically about how populations can be defined and how this relates to the measurement,
identification, and interpretation of health inequities</p></li>
<li><p>Explain the advantages of an intersectional approach to research</p></li>
<li><p>Conduct an intersectional bias assessment of simulated psychiatric data, based on a finding
from the psychiatric literature</p></li>
</ol>
<hr class="docutils" />
<section id="what-is-a-fair-machine-learning-model">
<h2>What is a fair machine learning model?<a class="headerlink" href="#what-is-a-fair-machine-learning-model" title="Link to this heading">#</a></h2>
<p>Sikstrom et al (2022) identify 3 general pillars of fairness as it pertains to ML: Transparency, Inclusion, and
Impartiality <a class="footnote-reference brackets" href="#footcite-sikstrom2022conceptualising" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p>
<p><strong>Transparency</strong>: A range of methods designed to see, understand, and hold complex algorithmic
systems accountable in a timely fashion.</p>
<p><strong>Inclusion</strong>: The process of improving the ability, opportunity, and dignity of people,
disadvantaged on the basis of their identity, to access health services, receive compassionate
care, and achieve equitable treatment outcomes.</p>
<p><strong>Impartiality</strong>: Health care should be free from unfair bias and systemic discrimination.
Deploying a ML algorithm requires a sociotechnical understanding of how data is collected and
interpreted within algorithmic system in order to ensure its responsible implementation in a
clinical setting.</p>
<p>Although all aspects of fairness are equally important, this tutorial will focus on impartiality.
Specifically, we aim to examine fairness through the scope of intersectionality, which was
originally coined by Kimberle Crenshaw <a class="footnote-reference brackets" href="#footcite-crenshaw1991intersectionality" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>:</p>
<blockquote>
<div><p><strong>Intersectionality</strong> is a framework for understanding how different forms of inequality (e.g.,
gender and race) often operate together and exacerbate each other.</p>
</div></blockquote>
<p>While by no means exhaustive, this serves as a useful frame that can be coupled with other
fairness approaches to enable a more thoughtful discussion of fairness-related issues.</p>
</section>
<section id="who-and-what-is-a-population">
<h2>Who and what is a population?<a class="headerlink" href="#who-and-what-is-a-population" title="Link to this heading">#</a></h2>
<p>When we do a fairness assessment, we need to decide which groups of people to compare to identify
whether there is some kind of fairness-related harm occurring.</p>
<p>The key question in being able to do so is this: <strong>Who and what is a population?</strong> It may seem
like this question is trivial, and has a clear-cut meaning in no need of further clarification.
However, researchers like Nancy Krieger have pointed out that a clear notion of “population” is
rarely defined <a class="footnote-reference brackets" href="#footcite-krieger2012population" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>, despite its centrality to fields like ML
fairness.</p>
<p>As such, seeking a clearer answer to this question is central to ML because it enables us to
determine how and when can populations be <strong>meaningfully and appropriately compared</strong>, and allows
a recognition of when such comparisons may be meaningless, or even worse, misleading.</p>
<section id="a-reflective-activity">
<h3>A reflective activity<a class="headerlink" href="#a-reflective-activity" title="Link to this heading">#</a></h3>
<p><em>“Every man is in certain respects, like all other men, like some other men, like no other men.”</em>
– Murray and Klukholne (1953).</p>
<blockquote>
<div><p>Consider a group or community that you’ve been part of. This could be anything from group of
friends or colleagues, to the people you’re currently sitting next to.  Consider the following
questions:</p>
<ul class="simple">
<li><p><em>What is something that your whole group has in common?</em> For example, on a soccer team, this
might be the fact that everyone plays soccer.</p></li>
<li><p><em>What is something that some of your group has in common?</em> Going off the same soccer team
example, perhaps some of the team identifies as boys, while some of the team identifies as
girls.</p></li>
<li><p><em>What is something that makes each of you unique?</em> Perhaps everyone is a different height, or
everyone grew up in a different neighborhood.</p></li>
</ul>
</div></blockquote>
<p>First, notice the intersectional approach that we took with this activity - each individual is
not defined by features about themselves in isolation, but the intersection of many different
identities, which constitutes the experience of that individual.</p>
<p>Second, note that disparities can result from both the factors that make us the same, and the
factors that make us different.  We need to keep this idea in mind - when we’re comparing groups,
are these meaningful differences that we’re comparing, and how do we know? For example, a
potential similarity between a group of people is that “all of us wear glasses” - but <strong>does this
constitute a meaningful way to compare groups of people?</strong> Answering this question requires
context, and can help us identify biases in our judgement that can lead to fairness harms, and
think about possible solutions.</p>
<p>Let’s look at a real-life example to see how exactly this is important.</p>
<p>In 2011, a paper studying the epidemiology of cancer noted that “early onset ER negative tumors
also develop more frequently in Asian Indian and Pakistani women and in women from other parts of
Asia, although not as prevalent as it is in West Africa.” <a class="footnote-reference brackets" href="#footcite-wallace2011interactions" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a></p>
<p>At first glance, this seems like a reasonable comparison that helps establish the basis for
certain populations having a higher prevalence of cancer.</p>
<p>However, Krieger points out that the cancer incidence rates used to arrive at this conclusion are
based on:</p>
<ul class="simple">
<li><p>For Pakistan, the weighted average of observed rates within a single region</p></li>
<li><p>For India, a complex estimation involving several rates in different states</p></li>
<li><p>For West Africa, the weighted average for 16 countries:
- 10 of these countries have rates estimated based on neighboring countries
- 5 rely on extrapolation from a single city within that country
- Only one has a national cancer registry</p></li>
</ul>
<p>This added context makes it clear that these population comparisons are not so clear-cut, and
that perhaps there is more nuance we need to be mindful of than we first thought.</p>
</section>
<section id="defining-a-population">
<h3>Defining a population<a class="headerlink" href="#defining-a-population" title="Link to this heading">#</a></h3>
<p>How then, should we conceptualize populations to enable the nuanced understanding required when
we compare them? To give some background on this work, we will again draw on some of the work of
Nancy Krieger, an epidemiologist who has written extensively on the concept of populations.</p>
<section id="populations-as-statistical-entities">
<h4>Populations as statistical entities<a class="headerlink" href="#populations-as-statistical-entities" title="Link to this heading">#</a></h4>
<p>Much of Krieger’s work stands in contrast to the conventional definition of population, which is
limited to an understanding of populations as statistical objects, rather than of substantive
beings. This definition is as follows:</p>
<blockquote>
<div><p><strong>Statistical view on populations.</strong> Populations are (statistical) entities composed of
component parts defined by <strong>innate (or intrinsic)</strong> attributes’</p>
</div></blockquote>
<p>Implicit in this definition is a notion of causality: if populations differed in their means,
this indicated that there was either a difference in “essence, or in or external factors, that
caused this difference between populations. To this end, <strong>populations can be compared on the
basis of their means because they are caused by comparable differences.</strong></p>
<p>This idea that humans have <strong>innate and comparable</strong> attributes was largely derived from  Adolphe Quetelet’s
invention of the <strong>“Average Man,”</strong> <a class="footnote-reference brackets" href="#footcite-eknoyan2007adolphe" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> establishing the notion of a
population mean.</p>
<p>Originally a physicist, Quetelet borrowed the idea from astronomy, where the “true location” of a
single star was determined the observations done by multiple observatories. Applied to
populations, this meant that observing the characteristics of multiple/all the individuals within
a group allowed the establishment of a “true mean” like human height, or body weight (which is
how we got the Body Mass Index).</p>
<p>While the population mean of a star is a descriptor of its position in space, the population mean
of a human population depends on how that population is defined. For example, recall our
reflective activity: those similarities and differences constitute how one might define a group.
This raises some interesting questions and issues.</p>
</section>
<section id="the-issues">
<h4>The issues<a class="headerlink" href="#the-issues" title="Link to this heading">#</a></h4>
<blockquote>
<div><p><strong>“For a start, the location of the mean referred to the location of a singular real object,
whereas for a population, the location of a population mean depended on how the population was
defined.”</strong></p>
</div></blockquote>
<p>We could define a population as all those who are human, or perhaps all those who are of a
certain nationality, or many other possibilities. Thinking back about the reflective activity, the
elements that your group had in common and those that were different are all elements that
could help define a population.</p>
<p>Crucially, we need to be careful about how we define a population, because it can impact the
results we get from any analyses - perhaps it might skew them, or perhaps the lack of appropriate
comparison groups might render an analysis inappropriate. Let’s look an example of this from a
2016 BBC article <a class="footnote-reference brackets" href="#footcite-bbc2016dutch" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> which compares body heights:</p>
<a class="reference internal image-reference" href="../_images/imhc_bodyheight.png"><img alt="A visualization that shows the average height of different countries in 1914 and 2014. The Netherlands has the tallest average body height for men, Iran the biggest height gain for men, South Korea the biggest height gain for women, and Guatemala the smallest average body height for women." src="../_images/imhc_bodyheight.png" style="width: 600px;" /></a>
<p>On the basis of what population definitions are these comparisons being made? In this case,
comparisons are being made between genders, nationalities, and time frames.</p>
<p>But consider this - what exactly makes it meaningful to compare 1914 to 2014? How can we truly
interpret this data? We don’t have a frame of reference for how each population is defined. We
don’t know their ages, we don’t how the data was collected, and we don’t know why nationality was
considered an appropriate characteristic for comparison. These elements are <strong>critical in
establishing the context for whether something can be considered a meaningful comparison</strong>, and
in their absence, we are left questioning.</p>
<p>This article is hardly unique in this problem - body mass index (BMI) has become a ubiquitous way
of defining obesity, but comparing individuals on the basis of their BMI is problematic: the
scale was built by and for white populations, leading to an overestimation of health risks for
Black individuals <a class="footnote-reference brackets" href="#footcite-endocrine2009widely" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>, and an underestimation of health risks for
Asian individuals <a class="footnote-reference brackets" href="#footcite-racette2003obesity" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>. Even more interestingly, BMI was never meant to
be a way of measuring health at an individual level.
<a class="footnote-reference brackets" href="#footcite-karasu2016adolphe" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a></p>
<p>The point is this: <strong>social relations, not just individual traits, shape population distributions
of health.</strong></p>
<p>What this means is that we cannot exclude the context that social relations <em>between</em> populations
illuminate. This could include anything from gender dynamics, to class disparities, to differing
economic systems - all of which can have differing impacts on human health.</p>
<p>This point is especially important because of how <strong>heterogeneous humans are</strong>, across space and
time, both between groups and within groups. Someone who has written extensively about this in
relation to ML, disability, and design is Jutta Treviranus.</p>
<p>Treviranus points out that the further you get from a population mean, the more heterogeneous the
population gets. And as you move away from this “average mean” - not only do people get more
diverse from one another, but solutions that are designed with the population mean in mind are
increasingly ineffective. <a class="footnote-reference brackets" href="#footcite-treviranus2019inclusivedesign" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a></p>
<a class="reference internal image-reference" href="../_images/imhc_design.png"><img alt="A visualization that shows scattered dots. The further you move away from the middle, the more scattered the points are. Around the points in the center, a blue circle is drawn that points to 'design works'. Moving further from the center, a second yellow circle points to 'design is difficult to use'. Moving even further from the center, a third orange circle points to 'can't use design'." src="../_images/imhc_design.png" style="width: 400px;" /></a>
<p>Notice the parallel we are drawing throughout here - we must always be thinking about how we
compare people, and how we can make those comparisons meaningful.</p>
</section>
<section id="proposition-2">
<h4>Proposition 2<a class="headerlink" href="#proposition-2" title="Link to this heading">#</a></h4>
<p>Given these limitations, let’s consider an alternative understanding of a population that
Krieger<a class="footnote-reference brackets" href="#footcite-krieger2012population" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> proposes.</p>
<blockquote>
<div><p>Populations are dynamic beings constituted by intrinsic relationships both among their members
and with other populations that together produce their existence and make causal inference
possible.</p>
</div></blockquote>
<p>What Krieger points out here, is that to make conclusions about differences between populations,
we need to understand <strong>how these populations came to be, and how we define them.</strong></p>
<p>Krieger’s view emphasizes that <strong>identity is not a fixed attribute (like a star), but rather a
fluid social process</strong>. A great example that illustrates the degree to which identity can change
can be seen in the racial identity of Puerto Ricans in the aftermath of Hurricane Maria.
<a class="footnote-reference brackets" href="#footcite-godreau2021nonsovereign" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a> Following the hurricane, there was a 72% drop in the amount
of Puerto Ricans identifying as white. Did all the white people move away? Nope - they just no
longer thought of themselves as white. The significant shift can be largely attributed to Puerto
Ricans feeling as though they had been neglected by the US government in their inadequate
response to the hurricane, and as such, redefined themselves as Latinx. That’s an entire
population collectively deciding to change their racial identity, and  gives an example of just
how dynamic - and unpredictable - identity can be.</p>
<p>This definition of populations also recognizes that this social process is <strong>linked to a history
of racism and sexism</strong>. For instance, people’s skin color was taken as being intrinsically
related to specific characteristics - a stereotype which has promoted all sorts of inequalities
in today’s society.</p>
<p>A notion of populations that emphasizes their dynamic nature is helpful because it helps
understand the relationships between these different groups, like gender, class, and religion -
all social constructions that have meaning for us. It acknowledges that <strong>we’re all working
within these systems through various different lenses, and within different dynamics of power and
privilege</strong>. The graphic below is a great representation of that - of how varied all of our
different experiences can be:</p>
<blockquote>
<div><p><em>Intersectionality is a lens through which you can see where power comes and collides, where it
locks and intersects. It is the acknowledgement that everyone has their own unique experiences
of discrimination and privilege.</em> –Kimberle Crenshaw</p>
</div></blockquote>
<a class="reference internal image-reference" href="../_images/imhc_intersectionality.jpg"><img alt="In the graphic above, Sylvia Duckworth uses a Spirograph to illustrate the multitude of ways that social identities might intersect. The Spirograph is split into 12 overlapping circles, each numbered, connected to a specific social identity, and assigned a unique colour. To illustrate the intersections of the different social identities, where each circle intersects, a new shade of the original colour is visible (as would happen when mixing paint colours together). At a glance the graphic shows all colours of the rainbow in different shades. The 12 social identities listed are: race, ethnicity, gender identity, class, language, religion, ability, sexuality, mental health, age, education, and body size. A quote from Kimberlé Crenshaw appears beneath the spirograph that reads “Intersectionality is a lens through which you can see where power comes and collides, where it locks and intersects. It is the acknowledgement that everyone has their own unique experiences of discrimination and privilege.” &quot;Intersectionality&quot; by Sylvia Duckworth, licensed under a `CC-BY-NC-ND`&lt;https://creativecommons.org/licenses/by-nc-nd/2.0/&gt; license." src="../_images/imhc_intersectionality.jpg" style="width: 600px;" /></a>
</section>
</section>
<section id="critical-race-theory">
<h3>Critical race theory<a class="headerlink" href="#critical-race-theory" title="Link to this heading">#</a></h3>
<section id="ain-t-i-a-woman">
<h4>Ain’t I A Woman?<a class="headerlink" href="#ain-t-i-a-woman" title="Link to this heading">#</a></h4>
<p>Note that although this intersectional approach has only recently been applied to ML, it is not a
novel concept. Rather, it’s a well-established idea that stretches back nearly 200 years to the
Black feminist and the suffragette movement, when white women were fighting for the right to
vote, but Black women were left out of this call for reform. There were a range of Black
activists and scholars who responded to this - wondering “What about me? Don’t I matter?”</p>
<p>For example, let us consider a quote from Sojourner Truth - an enslaved person at the time.
<a class="footnote-reference brackets" href="#footcite-npssojournertruth" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a></p>
<blockquote>
<div><p>That man over there says that women need to be helped into carriages, and lifted over ditches,
and to have the best place everywhere. Nobody ever helps me into carriages, or over
mud-puddles, or gives me any best place! And ain’t I a woman? Look at me! Look at my arm! I
have ploughed and planted, and gathered into barns, and no man could head me! And ain’t I a
woman? I could work as much and eat as much as a man - when I could get it - and bear the lash
as well! And ain’t I a woman? I have borne thirteen children, and seen most all sold off to
slavery, and when I cried out with my mother’s grief, none but Jesus heard me! And ain’t I a
woman?</p>
</div></blockquote>
<p>What she’s saying is “I am a woman - yet all these social factors, like slavery, have somehow
disenfranchised me from this feminist movement that’s going on.” In asking “Aint I A Woman?”,
Sojourner Truth is really asking the question that’s been central to our discussion - what makes
a population? Defining a population a specific way, like ignoring the realities of Black women,
can cause a lot of harm.</p>
</section>
<section id="id16">
<h4>Critical race theory<a class="headerlink" href="#id16" title="Link to this heading">#</a></h4>
<blockquote>
<div><p><strong>Critical race theory</strong> is an iterative methodology that draws on the collective wisdom of
activists and scholars to study and transform the relationship between race, racism, and power.
– Ford and Airhihenbuwa<a class="footnote-reference brackets" href="#footcite-ford2010critical" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a></p>
</div></blockquote>
<p>Intersectionality is one tool within the critical race theory toolkit, among many others, and
there has already been some great work <a class="footnote-reference brackets" href="#footcite-hanna2020towards" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></a> done on how critical race
theory (CRT) may be applied to algorithmic fairness, such as the need for disaggregated analysis
that operates on a descriptive level in order to interrogate the most salient aspects of race for
an algorithmic system. A central element of CRT, especially as it relates to ML systems, echoes a
theme we have already discussed extensively: <strong>how we define a group of people matters.</strong> The
power to define a group of people matters, and we need to question our assumptions about what
makes people have something in common, and how that might affect our ability to compare
populations.</p>
</section>
</section>
<section id="the-practical-applications-to-machine-learning">
<h3>The practical applications to machine learning<a class="headerlink" href="#the-practical-applications-to-machine-learning" title="Link to this heading">#</a></h3>
<p>In practice, the focus on these intersectional comparisons within the ML field has been on
protected groups <a class="footnote-reference brackets" href="#footcite-justicecanadianhuman" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a>, such as:</p>
<ul class="simple">
<li><p>Race, national or ethnic origin, color</p></li>
<li><p>Religion</p></li>
<li><p>Age</p></li>
<li><p>Sex</p></li>
<li><p>Sexual orientation</p></li>
<li><p>Gender identity or expression</p></li>
<li><p>Family status</p></li>
<li><p>Marital status</p></li>
<li><p>Disability, genetic characteristics</p></li>
</ul>
<p>We have legal obligations to ensure that any medical interventions do not discriminate against
individuals on the basis of these attributes, so it makes sense that most of the ML fairness
assessment have taken place on race, sex or gender.</p>
<p>However, it’s important to note that protected groups vary substantially between countries, and
even across specific applications within a country. Within the United States, for example, the
Fair Housing Act recognizes disability and gender identity as protected classes while the Equal
Credit Opportunity Act does not.</p>
<p>Furthermore, the social determinants of health that are known to impact health inequalities, such
as class, language, citizenship status (e.g., undocumented), and geography (e.g., rural vs urban)
are not protected groups. How these issues are understood and measured in the literature are
quite varied. This means that we understand <em>some</em> ways that ML can lead to fairness harms, but
there are <strong>many fairness harms happening that we know almost nothing about.</strong></p>
</section>
</section>
<section id="case-study-mental-health-care">
<h2>Case Study: Mental Health Care<a class="headerlink" href="#case-study-mental-health-care" title="Link to this heading">#</a></h2>
<p>We now turn to a case study on a hypothetical scenario, where we train a machine learning model
to predict a patient’s diagnosis in a mental health care setting.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The scenario considered in this case study is <strong>overly simplified</strong> and based on a
<strong>simulated data set</strong>. But we hope it gives you a sense of how the application of ML can
exacerbate biases in training data, and how we can evaluate models for bias from an
intersectional perspective.</p>
</div>
<section id="scenario">
<h3>Scenario<a class="headerlink" href="#scenario" title="Link to this heading">#</a></h3>
<p>To highlight the way that fairness harms has been evaluated in health care, we will first examine
a study by Obermeyer <em>et al.</em><a class="footnote-reference brackets" href="#footcite-obermeyer2019dissecting" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a>.</p>
<section id="algorithmic-fairness-in-health-care">
<h4>Algorithmic Fairness in Health Care<a class="headerlink" href="#algorithmic-fairness-in-health-care" title="Link to this heading">#</a></h4>
<p>Obermeyer <em>et al.</em><a class="footnote-reference brackets" href="#footcite-obermeyer2019dissecting" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a> examined a risk assessment algorithm for health insurance
across approximately <strong>50000 primary care patients</strong>. This algorithm was used to identify
individuals who required further follow-up for complex care.</p>
<p>The authors found that this algorithm was <strong>underestimating disease severity in Black clients.</strong>
Why might this be the case?</p>
<p>They proposed several issues:</p>
<p>First, the algorithm used <strong>total health expenditures as a proxy for disease severity.</strong> This is
problematic because health expenditures might vary based on socioeconomic status, and poverty
levels in Black populations tend to be higher. As such, even if someone is very sick, they may be
unwilling or unable to spend money on health care. There may also be an issue of trust involved,
but in short, the authors didn’t truly understand what was going on.</p>
<p>One of the things they point out, however, is that the bias which they encountered - termed
“Labelling Bias” - is pernicious, because labels are measured with inequities built into them. In
other words, when the labels on a dataset carry structural inequities, those biases are
unknowingly built in (we will examine this further in the applied component of this tutorial).</p>
<p>The authors also note that although the developers of the algorithm assumed that Black and white
people had some meaningful difference, they <strong>didn’t distinguish between the two groups</strong> based
on income or gender - this likely led to an underestimation of how poorly this model might be
estimating risk for certain groups of people.</p>
<p>A similar issue is presented by Buolamwini and Gebru in the gender shades project, which analyzed
fairness issues in facial detection software and found that the AI system led to larger errors
for dark-skinned women than for other groups. <a class="footnote-reference brackets" href="#footcite-buolamwini2018gender" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a></p>
<p>With the problems that Obermeyer and colleagues investigated related to using proxies as labels
and keeping intersectionality in mind, let’s move forward with an applied problem of our own.</p>
</section>
<section id="background-schizophrenia">
<h4>Background: Schizophrenia<a class="headerlink" href="#background-schizophrenia" title="Link to this heading">#</a></h4>
<p>Our case scenario for this practice component of the tutorial is based on the finding that
<strong>Black patients (and men in particular) are diagnosed with schizophrenia at a higher rate than
other demographic groups</strong> (for example, white men). <a class="footnote-reference brackets" href="#footcite-olbert2018meta" id="id23" role="doc-noteref"><span class="fn-bracket">[</span>20<span class="fn-bracket">]</span></a></p>
<blockquote>
<div><p><strong>Schizophrenia is a severe, chronic, and debilitating illness</strong> characterized by various
symptoms, which are broadly divided into positive and negative symptoms. Positive symptoms
include the symptom types that generally come to mind when thinking about schizophrenia, like
delusions, hallucinations, disorganized speech and thought, but negative symptoms are also
common. These include a lack of emotion, motivation, or interest.</p>
</div></blockquote>
<p>It’s unclear why Black patients are more likely to be diagnosed with this disorder, but it’s
likely that different factors play a role, such as genetics or being more likely to reside in
stressful environments (which can be a risk factor for developing schizophrenia). However, there
is also some pretty compelling evidence <a class="footnote-reference brackets" href="#footcite-gara2019naturalistic" id="id24" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a> that this effect is
<strong>due to diagnostic bias</strong>, or clinicians misdiagnosing black patients with schizophrenia when they
have another illness, like an affective disorder or depression instead. Clinicians may be
underemphasizing symptoms of depression in black patients and overemphasizing psychotic symptoms,
leading to misdiagnosis (and higher rates of schizophrenia). This tendency may be <strong>particularly
pronounced for Black men.</strong></p>
<p>Why is this important?</p>
<p><strong>Misdiagnosis can have negative downstream effects</strong> <a class="footnote-reference brackets" href="#footcite-gara2012influence" id="id25" role="doc-noteref"><span class="fn-bracket">[</span>22<span class="fn-bracket">]</span></a>, leading to
inequities
in care. Schizophrenia is a more serious and stigmatized illness than affective disorder, it has
a poorer prognosis, and involves treatments with greater side effects. Misdiagnosis can delay
getting the right treatment, increase patient frustration and distrust, and worsen illness, all
of which may be disproportionately affecting one subgroup of the population defined by at least
two intersecting features that we know of (gender and race).</p>
</section>
<section id="the-hypothetical-scenario">
<h4>The hypothetical scenario<a class="headerlink" href="#the-hypothetical-scenario" title="Link to this heading">#</a></h4>
<p>Based on this finding, we’ve developed a hypothetical scenario. You should note that this
scenario (and the simulated data we’re using) is <strong>overly simplified</strong>. But we hope it gives you
a sense of how the application of ML can exacerbate biases in training data, and how we can
evaluate models for bias from an intersectional perspective.</p>
<p>Imagine we have some <strong>electronic health record data on 10,000 patients</strong> who have been diagnosed
at <em>Health System A</em> with either affective disorder (denoted with 0) or schizophrenia (denoted
with 1) over the past 10 years. This data contains information on their sex, race, some
psychosocial information, and information from clinical assessments on their symptoms. Hospital
executives have had a classifier built that will take this information collected about a patient
at intake, and <strong>assign a diagnosis of either affective disorder (0) or schizophrenia (1)</strong>. They
train a binary classifier on the data to assign new incoming patients a diagnosis, to triage them
into the appropriate clinic for treatment.</p>
<p>Now, let’s say this hospital is run by some executives who would really like to cut costs. They
don’t want to do any further assessment after classification, and they are <strong>planning to
administer treatments to patients following this automated triage.</strong></p>
<p>But first, hospital executives must provide must provide some evidence to stakeholders that this
classifier works well in
diagnosing patients, so they ask the data science team to collect
a test sample of 1000 patients, on which they must evaluate the
model. The executives argue that the classifier works very well, based on some impressive
sensitivity and
specificity values. The stakeholders are not fully convinced (thinking that the executives may be
a little too eager to get this model deployed), so they hire us (an independent consulting firm)
to evaluate the model.</p>
<p>Note that this is one overly simplistic component of our case scenario. Typically, much more
consideration and evaluation would occur before ML is deployed in a real-life setting, especially
in healthcare. When it comes to complicated diagnoses (like schizophrenia), any use of ML is also
likely to complement human or clinical judgment.</p>
</section>
</section>
<section id="model-development">
<h3>Model Development<a class="headerlink" href="#model-development" title="Link to this heading">#</a></h3>
<p>Let’s build our first machine learning model. First, we need to import the required libraries and
data set.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Import relevant libraries</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml" title="sklearn.datasets.fetch_openml" class="sphx-glr-backref-module-sklearn-datasets sphx-glr-backref-type-py-function"><span class="n">fetch_openml</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-class"><span class="n">LogisticRegression</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler" title="sklearn.preprocessing.MinMaxScaler" class="sphx-glr-backref-module-sklearn-preprocessing sphx-glr-backref-type-py-class"><span class="n">MinMaxScaler</span></a><span class="p">,</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder" title="sklearn.preprocessing.OneHotEncoder" class="sphx-glr-backref-module-sklearn-preprocessing sphx-glr-backref-type-py-class"><span class="n">OneHotEncoder</span></a>

<span class="kn">from</span><span class="w"> </span><span class="nn">fairlearn.metrics</span><span class="w"> </span><span class="kn">import</span> <a href="../api_reference/generated/fairlearn.metrics.MetricFrame.html#fairlearn.metrics.MetricFrame" title="fairlearn.metrics.MetricFrame" class="sphx-glr-backref-module-fairlearn-metrics sphx-glr-backref-type-py-class"><span class="n">MetricFrame</span></a><span class="p">,</span> <a href="../api_reference/generated/fairlearn.metrics.false_positive_rate.html#fairlearn.metrics.false_positive_rate" title="fairlearn.metrics.false_positive_rate" class="sphx-glr-backref-module-fairlearn-metrics sphx-glr-backref-type-py-function"><span class="n">false_positive_rate</span></a>

<span class="c1"># Read in dataset</span>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch" title="sklearn.utils.Bunch" class="sphx-glr-backref-module-sklearn-utils sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_openml</span></a> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml" title="sklearn.datasets.fetch_openml" class="sphx-glr-backref-module-sklearn-datasets sphx-glr-backref-type-py-function"><span class="n">fetch_openml</span></a><span class="p">(</span><span class="n">data_id</span><span class="o">=</span><span class="mi">45040</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_openml</span><span class="o">.</span><span class="n">data</span></a>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="p">[</span><span class="s2">&quot;Diagnosis&quot;</span><span class="p">]</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_openml</span><span class="o">.</span><span class="n">target</span></a>

<span class="c1"># Partition the data into train and test sets</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">data</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span><span class="o">.</span><span class="n">dataset</span></a> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">]</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">data</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span><span class="o">.</span><span class="n">dataset</span></a> <span class="o">==</span> <span class="s2">&quot;test&quot;</span><span class="p">]</span>

<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop" title="pandas.DataFrame.drop" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">train</span><span class="o">.</span><span class="n">drop</span></a><span class="p">(</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop" title="pandas.DataFrame.drop" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">test</span><span class="o">.</span><span class="n">drop</span></a><span class="p">(</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># This function formats the data for stacked bar graphs</span>
<span class="k">def</span><span class="w"> </span><span class="nf">grouppivot</span><span class="p">(</span><span class="n">labelgroup</span><span class="p">,</span> <span class="n">yvalue</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">SZonly</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># Select only columns with a SZ diagnosis</span>
    <span class="k">if</span> <span class="n">SZonly</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dataset</span><span class="o">.</span><span class="n">Diagnosis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Group by label group, and normalize by y value within those groups</span>
    <span class="n">grouped</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">dataset</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="n">labelgroup</span><span class="p">])[</span><span class="n">yvalue</span><span class="p">]</span>
        <span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s2">&quot;percentage&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">pivot</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html#pandas.pivot_table" title="pandas.pivot_table" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-function"><span class="n">pd</span><span class="o">.</span><span class="n">pivot_table</span></a><span class="p">(</span>
        <span class="n">grouped</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">labelgroup</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">yvalue</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s2">&quot;percentage&quot;</span><span class="p">,</span> <span class="n">aggfunc</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">pivot</span>
</pre></div>
</div>
<section id="exploring-the-data">
<h4>Exploring the data<a class="headerlink" href="#exploring-the-data" title="Link to this heading">#</a></h4>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html#pandas.DataFrame.head" title="pandas.DataFrame.head" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">train</span><span class="o">.</span><span class="n">head</span></a><span class="p">()</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sex</th>
      <th>Race</th>
      <th>Housing</th>
      <th>Delay</th>
      <th>Anhedonia</th>
      <th>Dep_Mood</th>
      <th>Sleep</th>
      <th>Tired</th>
      <th>Appetite</th>
      <th>Rumination</th>
      <th>Concentration</th>
      <th>Psychomotor</th>
      <th>Delusion</th>
      <th>Suspicious</th>
      <th>Withdrawal</th>
      <th>Passive</th>
      <th>Tension</th>
      <th>Unusual_Thought</th>
      <th>Diagnosis</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Male</td>
      <td>Asian</td>
      <td>Stable</td>
      <td>No</td>
      <td>6.859778</td>
      <td>8.596433</td>
      <td>5.835555</td>
      <td>4.249037</td>
      <td>4.120215</td>
      <td>4.663982</td>
      <td>5.112734</td>
      <td>2.055298</td>
      <td>4.967700</td>
      <td>0.339114</td>
      <td>4.168202</td>
      <td>6.281626</td>
      <td>8.124157</td>
      <td>1.869677</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Male</td>
      <td>Hispanic</td>
      <td>Stable</td>
      <td>No</td>
      <td>9.833686</td>
      <td>8.148029</td>
      <td>4.497432</td>
      <td>6.308813</td>
      <td>6.270848</td>
      <td>5.398494</td>
      <td>5.716011</td>
      <td>6.071951</td>
      <td>2.477303</td>
      <td>4.355527</td>
      <td>2.559291</td>
      <td>4.240824</td>
      <td>4.005548</td>
      <td>1.069391</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Female</td>
      <td>Black</td>
      <td>Unstable</td>
      <td>No</td>
      <td>5.663383</td>
      <td>4.076933</td>
      <td>6.460087</td>
      <td>6.918369</td>
      <td>3.322668</td>
      <td>5.549999</td>
      <td>5.797641</td>
      <td>5.523801</td>
      <td>3.343273</td>
      <td>0.725848</td>
      <td>4.975947</td>
      <td>5.987116</td>
      <td>7.064453</td>
      <td>4.383587</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Female</td>
      <td>Black</td>
      <td>Stable</td>
      <td>Yes</td>
      <td>7.265911</td>
      <td>6.812793</td>
      <td>6.334380</td>
      <td>5.264936</td>
      <td>3.403835</td>
      <td>7.229377</td>
      <td>5.897029</td>
      <td>4.190843</td>
      <td>2.195623</td>
      <td>1.535735</td>
      <td>4.337831</td>
      <td>5.536262</td>
      <td>7.429212</td>
      <td>0.887562</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Male</td>
      <td>White</td>
      <td>Stable</td>
      <td>Yes</td>
      <td>7.476635</td>
      <td>6.901016</td>
      <td>8.782125</td>
      <td>5.199603</td>
      <td>5.085312</td>
      <td>2.251041</td>
      <td>10.019488</td>
      <td>3.782775</td>
      <td>3.251150</td>
      <td>5.779466</td>
      <td>4.654906</td>
      <td>4.517311</td>
      <td>6.460591</td>
      <td>2.613142</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<br />
<br /><p>By examining the first few cases, we can broadly get a sense of what the data looks like:</p>
<ul class="simple">
<li><p>Diagnosis is binary, with 1 corresponding to schizophrenia, and 0 corresponding to affective
disorder.</p></li>
<li><p>We have a binary sex variable, along with a race variable with Black, Asian, White, and
Hispanic as possible values. While including these in a model seems problematic, we will
explore the problems that arise when they are removed, while also using these features to
conduct an intersectional bias assessment.</p></li>
<li><p>Finally, we have a range of psychosocial and clinical variables that will help the model to
make a prediction</p></li>
</ul>
<p>You’ll notice that this dataset is very clean, with no missing or unexpected values. If we used
real-world data from hospital records, it would be much messier, including:</p>
<ul class="simple">
<li><p>many missing values, and likely not missing at random (e.g., distress, impairment, or
language barriers preventing a patient from being able to answer questions, no resources, or
staff available to help, unwillingness of patients to disclose sensitive information, such as
disability or sexual orientation)</p></li>
<li><p>many unexpected values, potentially due to human or system logging errors (e.g., a numeric
responses for a categorical variable, a negative value for a variable that must be positive,
such as a wait time)</p></li>
<li><p>variables that are much more complex (e.g., race or ethnicity coded in a multitude of ways,
<a class="footnote-reference brackets" href="#footcite-maslej2022race" id="id26" role="doc-noteref"><span class="fn-bracket">[</span>23<span class="fn-bracket">]</span></a> with many individuals having mixed racial backgrounds,
psychosocial
variables never cleanly separate into <em>stable</em> or <em>unstable</em> housing or <em>yes</em> or <em>no</em> for
delay (in reality, these constructs are rarely even captured, and they may be inferred, e.g.,
based on location or income)</p></li>
<li><p>subsets of older data not digitally captured</p></li>
</ul>
<p>Real world data for this type of task would include many more variables and require months of
processing and linking, but we are going to use this <strong>simulated</strong> dataset in order to convey the
important ideas.</p>
<p>Keeping this caveat in mind, let’s plot some graphs to better visualize the data.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Format graphs</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">diagnosis</span></a> <span class="o">=</span> <span class="n">grouppivot</span><span class="p">(</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="s2">&quot;Diagnosis&quot;</span><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sex</span></a> <span class="o">=</span> <span class="n">grouppivot</span><span class="p">(</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="s2">&quot;Sex&quot;</span><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">race</span></a> <span class="o">=</span> <span class="n">grouppivot</span><span class="p">(</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="s2">&quot;Race&quot;</span><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="p">)</span>

<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.figure.Figure.html#matplotlib.figure.Figure" title="matplotlib.figure.Figure" class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">fig</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">diagnosis</span></a><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">stacked</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Diagnosis across train and test sets&quot;</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sex</span></a><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">stacked</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Sex across train and test sets&quot;</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">race</span></a><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">stacked</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Race across train and test sets&quot;</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.figure.Figure.show.html#matplotlib.figure.Figure.show" title="matplotlib.figure.Figure.show" class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-method"><span class="n">fig</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_intersectional_bias_001.png" srcset="../_images/sphx_glr_plot_intersectional_bias_001.png" alt="Diagnosis across train and test sets, Sex across train and test sets, Race across train and test sets" class = "sphx-glr-single-img"/><p>These plots show that the train and test sets have similar proportions of data across
diagnosis, sex, and race, so the way our data is partitioned seems fine.</p>
<p>We observe a substantially higher proportion of white and Black individuals compared to Asian and
Hispanic individuals. If this were real-world data, we might hypothesize about why this would be
the case. In what ways could these trends be related to systemic factors, like the
underrepresentation of some groups in data collection? However, this is a dataset simulated for
our case scenario, and these trends may not appear in real-world health systems.</p>
<p>Let’s move forward with building our predictive model.</p>
</section>
<section id="preprocessing">
<h4>Preprocessing<a class="headerlink" href="#preprocessing" title="Link to this heading">#</a></h4>
<p>First, we distinguish our outcome or label (diagnosis) from the training features.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split the data into x (features) and y (diagnosis)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop" title="pandas.DataFrame.drop" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">train</span><span class="o">.</span><span class="n">drop</span></a><span class="p">(</span><span class="s2">&quot;Diagnosis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainy</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train</span><span class="o">.</span><span class="n">Diagnosis</span></a>

<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testx</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop" title="pandas.DataFrame.drop" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">test</span><span class="o">.</span><span class="n">drop</span></a><span class="p">(</span><span class="s2">&quot;Diagnosis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testy</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span><span class="o">.</span><span class="n">Diagnosis</span></a>
</pre></div>
</div>
<p>Next, we do some minimal preprocessing to one hot encode the categorical variables, bearing in
mind that preprocessing real-world hospital record data is typically much more laborious.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform one hot encoding</span>
<span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Sex&quot;</span><span class="p">,</span> <span class="s2">&quot;Race&quot;</span><span class="p">,</span> <span class="s2">&quot;Housing&quot;</span><span class="p">,</span> <span class="s2">&quot;Delay&quot;</span><span class="p">]</span>  <span class="c1"># Categorial variables</span>


<span class="c1"># Define a function for one hot encoding</span>
<span class="k">def</span><span class="w"> </span><span class="nf">onehot</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">):</span>
    <span class="n">ordinalencoder</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder" title="sklearn.preprocessing.OneHotEncoder" class="sphx-glr-backref-module-sklearn-preprocessing sphx-glr-backref-type-py-class"><span class="n">OneHotEncoder</span></a><span class="p">()</span>
    <span class="n">onehot</span> <span class="o">=</span> <span class="n">ordinalencoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="p">[</span><span class="n">categories</span><span class="p">])</span>

    <span class="n">columns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ordinalencoder</span><span class="o">.</span><span class="n">categories_</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">values</span><span class="p">:</span>
            <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">categories</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">j</span><span class="p">))</span>

    <span class="k">return</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class"><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span></a><span class="p">(</span><span class="n">onehot</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>


<span class="c1"># Apply transformation to data</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html#pandas.DataFrame.reset_index" title="pandas.DataFrame.reset_index" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">trainx</span><span class="o">.</span><span class="n">reset_index</span></a><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">onehot</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx</span></a><span class="p">))</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testx</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html#pandas.DataFrame.reset_index" title="pandas.DataFrame.reset_index" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">testx</span><span class="o">.</span><span class="n">reset_index</span></a><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">onehot</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testx</span></a><span class="p">))</span>

<span class="c1"># Drop the original categories</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop" title="pandas.DataFrame.drop" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">trainx</span><span class="o">.</span><span class="n">drop</span></a><span class="p">(</span><span class="n">categories</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testx</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop" title="pandas.DataFrame.drop" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">testx</span><span class="o">.</span><span class="n">drop</span></a><span class="p">(</span><span class="n">categories</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training">
<h4>Training<a class="headerlink" href="#training" title="Link to this heading">#</a></h4>
<p>For this model, we’ll use a simple logistic regression model with elastic net for regularization
in sklearn across 1000 max iterations. You could still do the same bias assessment we’ll be
carrying out here with other models, because the approach we will use is a <strong>post-hoc approach</strong>,
meaning it only requires the model predictions and not access to the model itself.</p>
<p>This is in contrast to some model-specific fairness approaches that require access to the model
internals.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining a logistic regression model</span>

<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-class"><span class="n">LogisticRegression</span></a><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;elasticnet&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;saga&quot;</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>We train the model and apply it to generate predictions on our test set.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training the model with all available features</span>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit" title="sklearn.linear_model.LogisticRegression.fit" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">fit</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx</span></a><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainy</span></a><span class="p">)</span>

<span class="c1"># generate 10000 predictions for 10000 train individuals</span>
<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train_predictions</span></a> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict" title="sklearn.linear_model.LogisticRegression.predict" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">predict</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training accuracy: &quot;</span><span class="p">,</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">skm</span><span class="o">.</span><span class="n">accuracy_score</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainy</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train_predictions</span></a><span class="p">))</span>  <span class="c1"># Training accuracy</span>

<span class="c1"># generate 1000 predictions for 1000 test individuals</span>
<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict" title="sklearn.linear_model.LogisticRegression.predict" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">predict</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testx</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy: &quot;</span><span class="p">,</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">skm</span><span class="o">.</span><span class="n">accuracy_score</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testy</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">))</span>  <span class="c1"># Test accuracy</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Training accuracy:  0.9306
Test accuracy:  0.938
</pre></div>
</div>
<p>We notice that the train and test accuracy are all fairly good. We can visualize the
performance of the model further by looking at a confusion matrix.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">confusionmatrix</span><span class="p">(</span><span class="n">truelabels</span><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">):</span>
    <span class="n">confusion_matrix</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">skm</span><span class="o">.</span><span class="n">confusion_matrix</span></a><span class="p">(</span><span class="n">truelabels</span><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">)</span>
    <span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;Sensitivity: &quot;</span><span class="p">,</span>
        <span class="n">tp</span> <span class="o">/</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fn</span><span class="p">),</span>
        <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Specificity: &quot;</span><span class="p">,</span>
        <span class="n">tn</span> <span class="o">/</span> <span class="p">(</span><span class="n">tn</span> <span class="o">+</span> <span class="n">fp</span><span class="p">),</span>
        <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">PPV: &quot;</span><span class="p">,</span>
        <span class="n">tp</span> <span class="o">/</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fp</span><span class="p">),</span>
        <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">NPV: &quot;</span><span class="p">,</span>
        <span class="n">tn</span> <span class="o">/</span> <span class="p">(</span><span class="n">tn</span> <span class="o">+</span> <span class="n">fn</span><span class="p">),</span>
    <span class="p">)</span>

    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay" title="sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay" class="sphx-glr-backref-module-sklearn-metrics-_plot-confusion_matrix sphx-glr-backref-type-py-class"><span class="n">skm</span><span class="o">.</span><span class="n">ConfusionMatrixDisplay</span></a><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>


<span class="n">confusionmatrix</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testy</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_intersectional_bias_002.png" srcset="../_images/sphx_glr_plot_intersectional_bias_002.png" alt="plot intersectional bias" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Sensitivity:  0.9404761904761905
Specificity:  0.9344660194174758
PPV:  0.9534482758620689
NPV:  0.9166666666666666
</pre></div>
</div>
<p>Again, we notice very solid performance. The model is correctly identifying positive cases
(Sensitivity), while rejecting negative ones (Specificity). The hospital executives probably took
a look at the model, and believed that it was ready for deployment.</p>
<p>This performance is another overly-simplistic component of our case scenario. In real-world
settings, and for psychiatric outcomes in particular, ML rarely achieves performance this good,
partly because training features are never as clearly related to outcomes as complex as diagnoses
of Schizophrenia or Affective Disorder. These conditions are often heterogeneous, meaning that
individuals with the same disorder can have very different symptom profiles. Symptoms of
schizophrenia (such as a lack of emotion and motivation, cognitive impairment, and even
psychosis) can overlap with symptoms of depression. Another reason ML often falls short in
predicting psychiatric outcomes is when these outcomes tend to be rare (such as in the case of
suicide <a class="footnote-reference brackets" href="#footcite-belsher2019prediction" id="id27" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a>). Although our simulated data is fairly balanced with
respect to
the two diagnoses, in reality, Schizophrenia is much less common than Affective Disorder. If
collected naturally based on different patients seen at a health system, our classes would be
very imbalanced.</p>
</section>
</section>
<section id="fairness-assessment">
<h3>Fairness Assessment<a class="headerlink" href="#fairness-assessment" title="Link to this heading">#</a></h3>
<p>Many ML algorithms deployed in the past have failed to properly
address fairness. Perhaps the most infamous example of this is the COMPAS algorithm
<a class="footnote-reference brackets" href="#footcite-angwin2016machine" id="id28" role="doc-noteref"><span class="fn-bracket">[</span>25<span class="fn-bracket">]</span></a> for prediction criminal recidivism, where Black defendants were
found to have higher risk scores, a higher false positive rate, and a lower false negative rate
compared to white defendants. In other applications of ML, the lack of fairness concerns proves
pervasive. An audit of three facial recognition software from IBM and Microsoft found the error
rate for darker-skinned females to be 34% higher than for lighter-skinned males
<a class="footnote-reference brackets" href="#footcite-buolamwini2018gender" id="id29" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a>. When translating into a gendered language, Google Translate
skews towards masculine translations for words like “strong” or “doctor,” while skewing feminine
for words like “beautiful” or “nurse.” <a class="footnote-reference brackets" href="#footcite-kuczmarski2018reducing" id="id30" role="doc-noteref"><span class="fn-bracket">[</span>26<span class="fn-bracket">]</span></a></p>
<p>These are clear problems which we aim to avoid by performing a proper fairness assessment. Let’s
do that now.</p>
<section id="quantifying-fairness">
<h4>Quantifying fairness<a class="headerlink" href="#quantifying-fairness" title="Link to this heading">#</a></h4>
<p>Many fairness metrics are calculated from basic performance metrics, like sensitivity,
specificity, TPR, FPR, etc. There are many performance metrics available, so we need to select
one that is most relevant to our task:
decision trees like
<a class="reference external" href="http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/">this</a>
one provided by Aequitas provide an easy way to understand the use cases
for different metrics.</p>
<p>As an example,
one common (and arguably most successful to date) application of ML in healthcare is in the
diagnosis of tumours from medical images (like chest x-rays) as being cancerous or non-cancerous.</p>
<p><strong>In this task, which performance metrics are most relevant?</strong></p>
<p>In this case, <strong>false negative predictions</strong> would probably be most relevant, because when
classifying tumours as being cancerous or non-cancerous, we would prefer to mistake a benign
tumour as being cancerous, as compared to the other way around. This is called a false positive
(or misdiagnosis), and it’s better than a false negative (or underdiagnosis) in this specific
circumstance.</p>
<p>Why? Because a false negative would result in a false perception that the patient does not have
cancer, which could delay potentially life-saving treatment.</p>
<p>There is a study suggesting that, when diagnosing the nature of tumours from medical images,
false negative rates are higher for disadvantaged or underserved groups defined by intersecting
features (e.g., low-income, Black women). <a class="footnote-reference brackets" href="#footcite-seyyedkalantari2021underdiagnosis" id="id31" role="doc-noteref"><span class="fn-bracket">[</span>27<span class="fn-bracket">]</span></a> This is an excellent,
real-world example of how the application of ML in healthcare can <em>amplify harms</em>, if deployed
without concern for fairness.</p>
<p><em>Which performance metric is most important in our case?</em></p>
<p>In our case, the opposite would be true - a false positive or misdiagnosis is more harmful for
our hypothetical scenario, because it would lead to unnecessary treatment for schizophrenia being
administered, as well as other negative impacts of a schizophrenia diagnosis, such as stigma or a
poorer perceived prognosis. While we want every patient to be appropriately diagnosed, clinicians
generally agree that it is better to be misdiagnosed with an affective disorder as compared to
schizophrenia. Indeed, when diagnosing patients, clinicians tend to rule out any factors that
could lead to psychosis or symptoms of schizophrenia, such as trauma, substance use, and
affective disorder, which in some cases, can have psychotic features.</p>
<p>So in our example, we’d like to evaluate whether rates of misdiagnosis or <strong>false positives</strong> are
the same across patient groups. False positive rates are calculated by dividing false positive
predictions by all negative predictions (<span class="math notranslate nohighlight">\(FPR = \frac{FP}{FP + TN}\)</span>)</p>
</section>
<section id="fairness-metrics">
<h4>Fairness metrics<a class="headerlink" href="#fairness-metrics" title="Link to this heading">#</a></h4>
<p>When evaluating ML models for fairness, we typically examine the ratio of a given performance
metric between two groups of interest, and whether it is greater or less than true parity (1).
The numerator in this equation is the metric for the group we are interested in evaluating (on
top), and the denominator is the same metric for the reference group (bottom).</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\text{Relative Metric} = \frac{\text{Metric }_{\text{group of interest}}}{\text{Metric
}_{\text{reference group}}}\)</span></p>
</div></blockquote>
<p>If this ratio is greater than 1, then the metric is higher in the group of interest vs reference.</p>
<p>If this ratio is less than 1, then the metric is lower in the group of interest vs reference.</p>
<p>Fairness metrics themselves can be broadly divided into 3 categories
<a class="footnote-reference brackets" href="#footcite-barocas2019fairness" id="id32" role="doc-noteref"><span class="fn-bracket">[</span>28<span class="fn-bracket">]</span></a>:</p>
<ul class="simple">
<li><p>Independence: Outcomes should be evenly distributed between subgroups</p></li>
<li><p>Separation: Errors should be evenly distributed between subgroups</p></li>
<li><p>Sufficiency: Given a prediction, an individual has an equal likelihood of belonging to any
subgroup</p></li>
</ul>
<p>In our case, we calculate the relative false positive rate via the category of separation, by
comparing false positive or misdiagnosis rates between our group of interest (Black men) and our
reference group (white men).</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\text{Relative FPR} = \frac{\text{FPR}_{\text{Black men}}}{\text{FPR}_{\text{white
men}}}\)</span></p>
</div></blockquote>
<p>Note that there are some subjective decisions we are making in our fairness assessment. First, we
have chosen white men as our reference group, but other groups might be justified here. For
instance, if we are interested in racialization related to Black groups specifically, we might
choose another racialized group as our reference (e.g., Hispanic men). If we are interested in
the impact of gender, we may choose Black women as our reference group.</p>
<p>Another decision is related to the metric – we could have also considered some other performance
metrics. Selection rate, for example, refers to the proportion of positive predictions between
the two groups:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\text{Selection rate}= \frac{TP + FP}{n}\)</span></p>
</div></blockquote>
<p>Because we are primarily interested in misdiagnosis of schizophrenia and not the total number of
schizophrenia diagnoses (which indeed may be higher in certain demographic groups), false
positive rate is likely a better fit for our task.</p>
<p>Critically, there are no readily-agreed upon definitions and understandings of fairness. There
are over 70 definitions of fairness, many of which conflict with each other, making it
<strong>impossible to simultaneously satisfy all possible metrics for fairness.</strong></p>
</section>
<section id="racial-bias">
<h4>Racial bias<a class="headerlink" href="#racial-bias" title="Link to this heading">#</a></h4>
<p>In order to perform a fairness assessment, there are a few key things we need to do:</p>
<p><strong>1) Which population might be unfairly affected by this model?</strong></p>
<p>Based on the research, we have defined this group to be Black men. In other words, we are
interested in a particular intersection of race and sex. In fairness terms, we define race and
sex to be <strong>sensitive variables</strong> - features that we want to ensure our model isn’t being
discriminatory against.</p>
<p>We’ll start with an assessment purely on the basis of Race as a sensitive variable (How might all
Black individuals be affected by an unfair model?) and then add in sex (How might Black men be
affected by unfair model?) in order to demonstrate the value of an intersectional approach.</p>
<p><strong>2) What is fairness in this context?</strong></p>
<p>We’ve also determined a fairness metric, which quantifies the exact nature of the discrimination
which groups may face and that we seek to minimize. As we mentioned, various fairness metrics may
be relevant, but we are primarily concerned about misdiagnosing individuals with affective
disorder as having schizophrenia. We examine false positive rates (i.e., false diagnoses of
schizophrenia), with the help of Fairlearn.</p>
<p><strong>3) How do we compare populations?</strong></p>
<p>Finally, the performance of a ML model cannot be analyzed in isolation, but rather in comparison
between different demographic subgroups. In order to do this, a reference group is required.</p>
<p>As we explained, we will use white individuals as a reference group, but this assessment could be
conducted with any reference group, or compared to an overall score across all groups. It is
important to note that both approaches can be problematic: there are concerns that using white
groups as reference groups centers their perspectives or experiences and represents other groups
as <em>other</em> or outliers, or that using overall averages may mask disparities (especially
intersectional ones) and ignore important historical and social context. Keeping this in mind,
we should carefully consider which group we use as a reference and the implications of this choice.</p>
<p>Here, we apply Fairlearn to compare performance in the test set among the different racial
groups, with a focus on evaluating false positive rate ratio.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">truelabels</span><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">):</span>
    <span class="c1"># Define race to be the sensitive variable</span>
    <span class="n">sensitive</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span><span class="o">.</span><span class="n">Race</span></a>

    <span class="c1"># Define a MetricFrame using a FPR of the defined sensitive features, using the true labels and</span>
    <span class="c1"># predictions</span>
    <span class="n">fmetrics</span> <span class="o">=</span> <a href="../api_reference/generated/fairlearn.metrics.MetricFrame.html#fairlearn.metrics.MetricFrame" title="fairlearn.metrics.MetricFrame" class="sphx-glr-backref-module-fairlearn-metrics sphx-glr-backref-type-py-class"><span class="n">MetricFrame</span></a><span class="p">(</span>
        <span class="n">metrics</span><span class="o">=</span><a href="../api_reference/generated/fairlearn.metrics.false_positive_rate.html#fairlearn.metrics.false_positive_rate" title="fairlearn.metrics.false_positive_rate" class="sphx-glr-backref-module-fairlearn-metrics sphx-glr-backref-type-py-function"><span class="n">false_positive_rate</span></a><span class="p">,</span>
        <span class="n">y_true</span><span class="o">=</span><span class="n">truelabels</span><span class="p">,</span>
        <span class="n">y_pred</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">,</span>
        <span class="n">sensitive_features</span><span class="o">=</span><span class="n">sensitive</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Compute the Relative FPR relative to white individuals.</span>
    <span class="n">results</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class"><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span></a><span class="p">(</span>
        <span class="p">[</span><span class="n">fmetrics</span><span class="o">.</span><span class="n">by_group</span><span class="p">,</span> <span class="n">fmetrics</span><span class="o">.</span><span class="n">by_group</span> <span class="o">/</span> <span class="n">fmetrics</span><span class="o">.</span><span class="n">by_group</span><span class="o">.</span><span class="n">White</span><span class="p">],</span>
        <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;FPR&quot;</span><span class="p">,</span> <span class="s2">&quot;Relative FPR&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>


<span class="n">f</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testy</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Race</th>
      <th>Asian</th>
      <th>Black</th>
      <th>Hispanic</th>
      <th>White</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>FPR</th>
      <td>0.033898</td>
      <td>0.126214</td>
      <td>0.057971</td>
      <td>0.044199</td>
    </tr>
    <tr>
      <th>Relative FPR</th>
      <td>0.766949</td>
      <td>2.855583</td>
      <td>1.311594</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<br />
<br /><p>In the table above, the first row shows the metric (false positive rates), while the second
row shows the ratio of the metric between specified group and reference. These values are all
expressed as percentages.</p>
<p>Some observations to note:</p>
<ul class="simple">
<li><p>The FPR are fairly low across the board, which are due to our use of simulated data (as we
explained, false positive rates in real-world data are likely to be much higher)</p></li>
<li><p>However, <strong>disparities in performance are emerging</strong>: Black patients have a relative FPR of
2.9, which means they are being misdiagnosed with schizophrenia when they have affective
disorder at a rate that is 2.9x higher than those who are white. This is quite concerning.</p></li>
<li><p>We see that Hispanic individuals are also poorly affected by this model, with a relative FPR of
1.3 (but you should note that this is not an effect that has been explicitly noted in the
literature and is likely an artifact of our simulated data)</p></li>
<li><p>The white population has a relative FPR of 1 - this makes since they are our reference
population, and other other group is being compared against the FPR for whites.</p></li>
</ul>
</section>
<section id="intersectional-bias">
<h4>Intersectional bias<a class="headerlink" href="#intersectional-bias" title="Link to this heading">#</a></h4>
<p>However, we suspect this bias might only extend to identities defined by the intersecting
features of sex and race (i.e., Black men <a class="footnote-reference brackets" href="#footcite-gara2019naturalistic" id="id33" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a>).</p>
<p>Let’s repeat this assessment then.</p>
<p>To do this with Fairlearn, we define sensitive features as the intersection of race and sex.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">intersectionalf</span><span class="p">(</span><span class="n">truelabels</span><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">):</span>
    <span class="c1"># Sensitive features are now the intersection of race and sex</span>
    <span class="n">sensitive</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class"><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span></a><span class="p">(</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.stack.html#numpy.stack" title="numpy.stack" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">stack</span></a><span class="p">([</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span><span class="o">.</span><span class="n">Race</span></a><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span><span class="o">.</span><span class="n">Sex</span></a><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Race&quot;</span><span class="p">,</span> <span class="s2">&quot;Sex&quot;</span><span class="p">])</span>

    <span class="n">fmetrics</span> <span class="o">=</span> <a href="../api_reference/generated/fairlearn.metrics.MetricFrame.html#fairlearn.metrics.MetricFrame" title="fairlearn.metrics.MetricFrame" class="sphx-glr-backref-module-fairlearn-metrics sphx-glr-backref-type-py-class"><span class="n">MetricFrame</span></a><span class="p">(</span>
        <span class="n">metrics</span><span class="o">=</span><a href="../api_reference/generated/fairlearn.metrics.false_positive_rate.html#fairlearn.metrics.false_positive_rate" title="fairlearn.metrics.false_positive_rate" class="sphx-glr-backref-module-fairlearn-metrics sphx-glr-backref-type-py-function"><span class="n">false_positive_rate</span></a><span class="p">,</span>
        <span class="n">y_true</span><span class="o">=</span><span class="n">truelabels</span><span class="p">,</span>
        <span class="n">y_pred</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">,</span>
        <span class="n">sensitive_features</span><span class="o">=</span><span class="n">sensitive</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class"><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span></a><span class="p">(</span>
        <span class="p">[</span><span class="n">fmetrics</span><span class="o">.</span><span class="n">by_group</span><span class="p">,</span> <span class="n">fmetrics</span><span class="o">.</span><span class="n">by_group</span> <span class="o">/</span> <span class="n">fmetrics</span><span class="o">.</span><span class="n">by_group</span><span class="o">.</span><span class="n">White</span><span class="o">.</span><span class="n">Male</span><span class="p">],</span>
        <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;FPR&quot;</span><span class="p">,</span> <span class="s2">&quot;Relative FPR&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>


<span class="n">intersectionalf</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testy</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>Race</th>
      <th colspan="2" halign="left">Asian</th>
      <th colspan="2" halign="left">Black</th>
      <th colspan="2" halign="left">Hispanic</th>
      <th colspan="2" halign="left">White</th>
    </tr>
    <tr>
      <th>Sex</th>
      <th>Female</th>
      <th>Male</th>
      <th>Female</th>
      <th>Male</th>
      <th>Female</th>
      <th>Male</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>FPR</th>
      <td>0.0</td>
      <td>0.076923</td>
      <td>0.076923</td>
      <td>0.28</td>
      <td>0.108108</td>
      <td>0.0</td>
      <td>0.037383</td>
      <td>0.054054</td>
    </tr>
    <tr>
      <th>Relative FPR</th>
      <td>0.0</td>
      <td>1.423077</td>
      <td>1.423077</td>
      <td>5.18</td>
      <td>2.000000</td>
      <td>0.0</td>
      <td>0.691589</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<br />
<br /><p>What do we notice?</p>
<ul class="simple">
<li><p>Both the FPR and Relative FPR of Black men is much higher than that of Black women and men
combined. Black men are now misdiagnosed at a rate that is almost 5.18x that of White men. This
suggests that Black men are specifically being unfairly treated by this model.</p></li>
<li><p>Black women still have a higher rate of misdiagnosis than white men, but we see now that the
Relative FPR for this group is actually lower than the Black population as a whole. This
provides further support for the notion that Black men are an intersectional group that is
being unfairly harmed.</p></li>
<li><p>Crucially, this insight is something we would have completely missed out on without looking at
this problem through an intersectional lens</p></li>
</ul>
<p>We can take a look at the data to get an idea about what might be going on. First, we examine the
frequency of diagnoses, stratified by intersectional groups.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train</span></a><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="s2">&quot;Sex&quot;</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;Race&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">,</span> <span class="s2">&quot;Diagnosis&quot;</span><span class="p">,</span> <span class="n">discrete</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_intersectional_bias_003.png" srcset="../_images/sphx_glr_plot_intersectional_bias_003.png" alt="Sex = Male | Race = Asian, Sex = Male | Race = Hispanic, Sex = Male | Race = Black, Sex = Male | Race = White, Sex = Female | Race = Asian, Sex = Female | Race = Hispanic, Sex = Female | Race = Black, Sex = Female | Race = White" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.FacetGrid object at 0x7efe721ecfe0&gt;
</pre></div>
</div>
<p>We see that the frequency of the two diagnoses (of affective disorder or 0 and schizophrenia
or 1) are fairly similar among all intersectional subgroups, except Black men, who have a much
higher rate of schizophrenia diagnosis.</p>
</section>
</section>
<section id="fairness-through-unawareness">
<h3>Fairness through Unawareness?<a class="headerlink" href="#fairness-through-unawareness" title="Link to this heading">#</a></h3>
<p>In our data, Black men are less likely to be diagnosed with affective disorder, and more likely
to be diagnosed with schizophrenia than other groups (and almost two times as likely as white
men). Perhaps the model is picking up on this trend, which is contributing to bias. If we remove
race, the classifier will no longer have access to this information during training.</p>
<p>This is an approach that is commonly termed <strong>fairness through unawareness.</strong> Specifically, it
refers to the notion that a model that does not have access to a given feature when making
predictions cannot be unfair with respect to that feature.</p>
<p>Define and drop race-related variables</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">race_cat</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Race_Asian&quot;</span><span class="p">,</span> <span class="s2">&quot;Race_Black&quot;</span><span class="p">,</span> <span class="s2">&quot;Race_Hispanic&quot;</span><span class="p">,</span> <span class="s2">&quot;Race_White&quot;</span><span class="p">]</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx_norace</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop" title="pandas.DataFrame.drop" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">trainx</span><span class="o">.</span><span class="n">drop</span></a><span class="p">(</span><span class="n">race_cat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testx_norace</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop" title="pandas.DataFrame.drop" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">testx</span><span class="o">.</span><span class="n">drop</span></a><span class="p">(</span><span class="n">race_cat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Now that we’ve dropped the variables, we’ll train a second model that is identical to the
first one, except it no longer uses Race as a feature.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define and train a second model</span>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model2</span></a> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-class"><span class="n">LogisticRegression</span></a><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;elasticnet&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;saga&quot;</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model2</span></a> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit" title="sklearn.linear_model.LogisticRegression.fit" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-method"><span class="n">model2</span><span class="o">.</span><span class="n">fit</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx_norace</span></a><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainy</span></a><span class="p">)</span>

<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train_predictions</span></a> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict" title="sklearn.linear_model.LogisticRegression.predict" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-method"><span class="n">model2</span><span class="o">.</span><span class="n">predict</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx_norace</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training accuracy: &quot;</span><span class="p">,</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">skm</span><span class="o">.</span><span class="n">accuracy_score</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainy</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train_predictions</span></a><span class="p">))</span>  <span class="c1"># Training accuracy</span>

<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict" title="sklearn.linear_model.LogisticRegression.predict" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-method"><span class="n">model2</span><span class="o">.</span><span class="n">predict</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testx_norace</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy: &quot;</span><span class="p">,</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">skm</span><span class="o">.</span><span class="n">accuracy_score</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testy</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">))</span>  <span class="c1"># Test accuracy</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Training accuracy:  0.9297
Test accuracy:  0.937
</pre></div>
</div>
<p>We’ll note that the accuracy is still solid. It has decreased slightly, which makes sense
given that our model has access to fewer features with which to maximize predictive accuracy.</p>
<p>That said, our most important objective is to analyze whether fairness has increased as a result
of our change, so let’s perform another bias assessment.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testy</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Race</th>
      <th>Asian</th>
      <th>Black</th>
      <th>Hispanic</th>
      <th>White</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>FPR</th>
      <td>0.067797</td>
      <td>0.126214</td>
      <td>0.057971</td>
      <td>0.033149</td>
    </tr>
    <tr>
      <th>Relative FPR</th>
      <td>2.045198</td>
      <td>3.807443</td>
      <td>1.748792</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<br />
<br /><p>Oh yikes - by removing race from our model, the FPR for the Black population has increased to
3.8x that of our reference group.</p>
<p>Does intersectional fairness show a similar trend?</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">intersectionalf</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">testy</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">predictions</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>Race</th>
      <th colspan="2" halign="left">Asian</th>
      <th colspan="2" halign="left">Black</th>
      <th colspan="2" halign="left">Hispanic</th>
      <th colspan="2" halign="left">White</th>
    </tr>
    <tr>
      <th>Sex</th>
      <th>Female</th>
      <th>Male</th>
      <th>Female</th>
      <th>Male</th>
      <th>Female</th>
      <th>Male</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>FPR</th>
      <td>0.030303</td>
      <td>0.115385</td>
      <td>0.076923</td>
      <td>0.280000</td>
      <td>0.108108</td>
      <td>0.0</td>
      <td>0.028037</td>
      <td>0.040541</td>
    </tr>
    <tr>
      <th>Relative FPR</th>
      <td>0.747475</td>
      <td>2.846154</td>
      <td>1.897436</td>
      <td>6.906667</td>
      <td>2.666667</td>
      <td>0.0</td>
      <td>0.691589</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<br />
<br /><p>If we analyze this problem through an intersectional lens, we notice that the relative FPR has
increased even further, and Black men are now being misdiagnosed at a rate that is 7x higher than
white men.</p>
<p><strong>After we have observed this finding, is it a good idea to implement our model? Probably not.</strong></p>
<p>After all, the potential harm related to an unfair model like this one does not stop at the
fairness assessment - it can manifest in impactful and pervasive ways in the systems where it is
potentially deployed. A Black man misdiagnosed with schizophrenia with the help of a ML model may
become more distrustful towards healthcare in the future. At any future visits, this distrust
might manifest in certain types of behaviours (e.g., an increased sense of tension) that could be
interpreted as further evidence of schizophrenia, ultimately contributing to further
misdiagnosis. This type of feedback is extremely detrimental, as <strong>algorithms reinforce and
propagate the unfairness</strong> encoded within the data that is representative of society’s own
discriminatory practices.</p>
<p>This begs the question though - if unfair predictions made by an algorithm are capable of causing
harm, should we attempt to remove this bias from our model? After all, there are methods that
exist that allow us to transform our data or modify our training algorithm in a way that would
make our model fairer with respect to the fairness metrics we are using.</p>
<p>In answering this, it’s important to remember that the bias we are seeing in our model are a
reflection of systemic biases that exist in real life. While the real life biases that lead to
patient misdiagnosis are certainly problematic, detecting these biases in our model isn’t
necessarily a bad thing. In this case scenario, where our main goal is actually to better
understand the model, the presence and quantification of these biases are actually very helpful
because they enable us to understand the systemic biases that have been encoded into our data.
<strong>In other words, our model can be insightful *because* it captures these harmful real-world
biases.</strong></p>
<p>To this end, attempting to remove the biases from our dataset would be detrimental to our aim.
Instead, we should dig a little bit deeper to reflect and analyze some of the systemic factors
that could be underlying our findings.</p>
<section id="why-is-our-model-still-biased">
<h4>Why is our model still biased?<a class="headerlink" href="#why-is-our-model-still-biased" title="Link to this heading">#</a></h4>
<p>Why has removing race information from our training data not fixed our problem?</p>
<p>Recall that there is much evidence to support <strong>diagnostic bias</strong>, or the tendency for clinicians
to under-emphasize depressive symptoms and over-emphasize psychotic symptoms when assessing black
men. But one interesting 2004 study <a class="footnote-reference brackets" href="#footcite-arnold2004ethnicity" id="id34" role="doc-noteref"><span class="fn-bracket">[</span>29<span class="fn-bracket">]</span></a> shows that this effect is not
necessarily due to the appearance of race. In this study, the researchers examined whether
blinding clinicians to a patient’s race would remove this diagnostic bias. Black and white
patients presenting to the hospital with psychosis were evaluated with structured rating scales,
and this evaluation was transcribed. Cues indicating the patient’s race were removed from the
transcription. Regardless of whether clinicians were blinded or unblinded to the patient’s race,
they still rated the black men as having increased psychotic symptoms.</p>
<p><strong>So what is going on here?</strong></p>
<p>It’s likely that race is <strong>associated with other factors that are relevant to misdiagnosis</strong>. The
diagnostic bias (or a tendency to overemphasize symptoms of schizophrenia and under-emphasize
depressive symptoms) may be related to socio-environmental factors, for example, Black men with
depression facing more barriers to receiving mental healthcare, which results in more severe
illness when finally assessed. Growing up in low-income or stressful environments, or early
exposure to malnourishment and trauma, can also lead to more severe impairment in daily and
cognitive functioning. Black patients may additionally face racialization and poor treatment in
health settings, leading them to exhibit paranoia, tension, or distrust at assessment (especially
if being assessed by a white clinician). The diagnostic instruments we have are also likely
culturally biased, having been developed on mostly white populations, making it difficult to pick
up on symptoms of depression in Black patients or men, in particular. (This article
Herbst<a class="footnote-reference brackets" href="#footcite-herbst2022schizophrenia" id="id35" role="doc-noteref"><span class="fn-bracket">[</span>30<span class="fn-bracket">]</span></a> presents a nice overview of these various concerns)</p>
<p>Unfortunately then, it’s not as simple as removing race from the equation, because of these
<strong>persisting systemic biases which are related to misdiagnosis.</strong> And these biases are reflected
in the training data, since the socio-environmental and clinical factors relevant for
misdiagnosis are also associated with race. The model is picking up on these associations,
despite not having access to the race of each patient.</p>
<p>Now that we have evidence for intersectional bias in our model, we could explore some of the
training features that might underlie this bias. In fact, there is a very recent study by
Banerjee <em>et al.</em><a class="footnote-reference brackets" href="#footcite-banerjee2021readiang" id="id36" role="doc-noteref"><span class="fn-bracket">[</span>31<span class="fn-bracket">]</span></a> which takes this point a bit further. In this study,
researchers found that deep learning models trained on medical images, like chest X-rays,
performed well at predicting the patient’s race, despite not having access to any racial
information. The researchers examined some reasonable explanations for how this was even
possible, such as minor anatomical differences between racial groups, cumulative effects of
racialization or stress, and even image quality between health systems, and none were supported.
So there must be some way that models pick up on race that not even us humans can understand.</p>
</section>
<section id="feature-evaluation">
<h4>Feature evaluation<a class="headerlink" href="#feature-evaluation" title="Link to this heading">#</a></h4>
<p>In sum, the tendency for Black men to be misdiagnosed with schizophrenia is not simply a result
of clinician or interpersonal bias, but likely reflects systemic factors (e.g., barriers to care
leading to severe illness at assessment, expression of emotional and cognitive symptoms of
depression, experiences of racialization leading to greater paranoia or distrust).</p>
<p>These factors may be reflected in other features in the simulated data, which are related to
race, and contribute to bias.</p>
<p>We can explore relations among the features in different ways, but one option is to see how
various features are related to schizophrenia in the training set, and then explore these
features in groups with affective disorder in the test set.</p>
<p>The choice of which features to consider is subjective and can be based on existing research or
empirical observation (or both). In our case, we’ll use the first few features identified as
being important for prediction in our ML model.</p>
<p>Gets the weights associated with each feature, and scales them from 0-100</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">weights</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class"><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span></a><span class="p">(</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span><span class="o">.</span><span class="n">coef_</span></a><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html#pandas.Index" title="pandas.core.indexes.base.Index" class="sphx-glr-backref-module-pandas-core-indexes-base sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx</span><span class="o">.</span><span class="n">columns</span></a><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Weight&quot;</span><span class="p">])</span>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler" title="sklearn.preprocessing.MinMaxScaler" class="sphx-glr-backref-module-sklearn-preprocessing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">scaler</span></a> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler" title="sklearn.preprocessing.MinMaxScaler" class="sphx-glr-backref-module-sklearn-preprocessing sphx-glr-backref-type-py-class"><span class="n">MinMaxScaler</span></a><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">scaled_weights</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class"><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span></a><span class="p">(</span>
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.fit_transform" title="sklearn.preprocessing.MinMaxScaler.fit_transform" class="sphx-glr-backref-module-sklearn-preprocessing sphx-glr-backref-type-py-method"><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span></a><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">weights</span></a><span class="p">)),</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html#pandas.Index" title="pandas.core.indexes.base.Index" class="sphx-glr-backref-module-pandas-core-indexes-base sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">trainx</span><span class="o">.</span><span class="n">columns</span></a><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html#pandas.Index" title="pandas.core.indexes.base.Index" class="sphx-glr-backref-module-pandas-core-indexes-base sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">weights</span><span class="o">.</span><span class="n">columns</span></a>
<span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html#pandas.DataFrame.sort_values" title="pandas.DataFrame.sort_values" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">scaled_weights</span><span class="o">.</span><span class="n">sort_values</span></a><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Weight&quot;</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Rumination</th>
      <td>100.000000</td>
    </tr>
    <tr>
      <th>Housing_Unstable</th>
      <td>90.864409</td>
    </tr>
    <tr>
      <th>Tension</th>
      <td>87.330861</td>
    </tr>
    <tr>
      <th>Race_White</th>
      <td>82.566259</td>
    </tr>
    <tr>
      <th>Delusion</th>
      <td>55.671885</td>
    </tr>
    <tr>
      <th>Unusual_Thought</th>
      <td>53.261827</td>
    </tr>
    <tr>
      <th>Suspicious</th>
      <td>45.994127</td>
    </tr>
    <tr>
      <th>Concentration</th>
      <td>45.473434</td>
    </tr>
    <tr>
      <th>Anhedonia</th>
      <td>41.976544</td>
    </tr>
    <tr>
      <th>Tired</th>
      <td>41.872997</td>
    </tr>
    <tr>
      <th>Delay_Yes</th>
      <td>41.833748</td>
    </tr>
    <tr>
      <th>Sex_Female</th>
      <td>38.435744</td>
    </tr>
    <tr>
      <th>Psychomotor</th>
      <td>29.893116</td>
    </tr>
    <tr>
      <th>Dep_Mood</th>
      <td>26.117464</td>
    </tr>
    <tr>
      <th>Withdrawal</th>
      <td>18.919398</td>
    </tr>
    <tr>
      <th>Race_Black</th>
      <td>10.112947</td>
    </tr>
    <tr>
      <th>Race_Asian</th>
      <td>5.350208</td>
    </tr>
    <tr>
      <th>Sleep</th>
      <td>5.186498</td>
    </tr>
    <tr>
      <th>Housing_Stable</th>
      <td>2.541632</td>
    </tr>
    <tr>
      <th>Passive</th>
      <td>0.532210</td>
    </tr>
    <tr>
      <th>Appetite</th>
      <td>0.057536</td>
    </tr>
    <tr>
      <th>Sex_Male</th>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Race_Hispanic</th>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Delay_No</th>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<br />
<br /><p>When we examine features that are most important for prediction in our first model (i.e., the
model that included race), it indeed shows that although race is among the important features,
there are also other features related to clinical presentation that are contributing to
predicting the diagnosis or outcome.</p>
<p>As mentioned, we will probe into potential factors underlying misdiagnosis of individuals with
affective disorder by examining how these features are related to diagnosis in the training set.
Then, we will compare individuals with affective disorder on these features in the test set,
stratified by intersectional group.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reloading the data for analysis</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">data</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span><span class="o">.</span><span class="n">dataset</span></a> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">data</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span><span class="o">.</span><span class="n">dataset</span></a> <span class="o">==</span> <span class="s2">&quot;test&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train</span></a><span class="p">[</span><span class="s2">&quot;Diagnosis&quot;</span><span class="p">]</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train</span></a><span class="p">[</span><span class="s2">&quot;Diagnosis&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Diagnosis&quot;</span><span class="p">]</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Diagnosis&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Create new intersect column for plotting</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;intersect&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">test</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Sex&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Male&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Race&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;White&quot;</span><span class="p">),</span> <span class="s2">&quot;intersect&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WhiteM&quot;</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">test</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Sex&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Male&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Race&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Black&quot;</span><span class="p">),</span> <span class="s2">&quot;intersect&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;BlackM&quot;</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">test</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Sex&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Male&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Race&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">),</span> <span class="s2">&quot;intersect&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;HispanicM&quot;</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">test</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Sex&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Male&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Race&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Asian&quot;</span><span class="p">),</span> <span class="s2">&quot;intersect&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;AsianM&quot;</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">test</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Sex&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Female&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Race&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;White&quot;</span><span class="p">),</span> <span class="s2">&quot;intersect&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WhiteF&quot;</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">test</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Sex&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Female&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Race&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Black&quot;</span><span class="p">),</span> <span class="s2">&quot;intersect&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;BlackF&quot;</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">test</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Sex&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Female&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Race&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">),</span> <span class="s2">&quot;intersect&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;HispanicF&quot;</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">test</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Sex&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Female&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span></a><span class="p">[</span><span class="s2">&quot;Race&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Asian&quot;</span><span class="p">),</span> <span class="s2">&quot;intersect&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;AsianF&quot;</span>

<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.figure.Figure.html#matplotlib.figure.Figure" title="matplotlib.figure.Figure" class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">fig</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Diagnosis&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Rumination&quot;</span><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="o">=</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train</span></a><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Rumination vs Diagnosis (train)&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;intersect&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Rumination&quot;</span><span class="p">,</span>
    <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="o">=</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">test</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span><span class="o">.</span><span class="n">Diagnosis</span></a> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">ax</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">order</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;BlackF&quot;</span><span class="p">,</span>
        <span class="s2">&quot;WhiteF&quot;</span><span class="p">,</span>
        <span class="s2">&quot;HispanicF&quot;</span><span class="p">,</span>
        <span class="s2">&quot;AsianF&quot;</span><span class="p">,</span>
        <span class="s2">&quot;BlackM&quot;</span><span class="p">,</span>
        <span class="s2">&quot;WhiteM&quot;</span><span class="p">,</span>
        <span class="s2">&quot;HispanicM&quot;</span><span class="p">,</span>
        <span class="s2">&quot;AsianM&quot;</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">)</span>
<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Rumination vs Intersect across groups with AD (test)&quot;</span><span class="p">)</span>

<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_intersectional_bias_004.png" srcset="../_images/sphx_glr_plot_intersectional_bias_004.png" alt="Rumination vs Diagnosis (train), Rumination vs Intersect across groups with AD (test)" class = "sphx-glr-single-img"/><p>The first graph (left) shows average rumination scores stratified by diagnosis. Rumination is a
cognitive feature of depression, referring to repetitive, persistent thinking about the
depressive episode. As you can see, rumination is more common in affective disorder than
schizophrenia, which is consistent with clinical trends.</p>
<p>The second graph (right) shows rumination among individuals with affective disorder in the test
set. You can see that rumination is lower among men than women, but also that <strong>Black men as
compared to white men are less likely to report rumination</strong>, which are the two groups we’re
comparing. As such, this could be a potential factor contributing to misdiagnosis, leading to
higher false positive rates in Black men. Of course, other groups may have lower rumination
scores relative to white men as well (e.g., Hispanic men), but the model is picking up on trends
related to a variety of features, so we can take a look at one more.</p>
<p>Another important feature is tension, which is a symptom of schizophrenia. We can carry out a
similar exploration to examine how tension is reported by individuals with affective disorder
defined by intersecting features of sex and race, as compared to schizophrenia.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.figure.Figure.html#matplotlib.figure.Figure" title="matplotlib.figure.Figure" class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">fig</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Diagnosis&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Tension&quot;</span><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="o">=</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">train</span></a><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Tension vs Diagnosis (train)&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;intersect&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Tension&quot;</span><span class="p">,</span>
    <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="o">=</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" title="pandas.DataFrame.loc" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-property"><span class="n">test</span><span class="o">.</span><span class="n">loc</span></a><span class="p">[</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">test</span><span class="o">.</span><span class="n">Diagnosis</span></a> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">ax</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">order</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;BlackF&quot;</span><span class="p">,</span>
        <span class="s2">&quot;WhiteF&quot;</span><span class="p">,</span>
        <span class="s2">&quot;HispanicF&quot;</span><span class="p">,</span>
        <span class="s2">&quot;AsianF&quot;</span><span class="p">,</span>
        <span class="s2">&quot;BlackM&quot;</span><span class="p">,</span>
        <span class="s2">&quot;WhiteM&quot;</span><span class="p">,</span>
        <span class="s2">&quot;HispanicM&quot;</span><span class="p">,</span>
        <span class="s2">&quot;AsianM&quot;</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">)</span>
<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">axs</span></a><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Tension vs Intersect across groups with AD (test)&quot;</span><span class="p">)</span>

<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_intersectional_bias_005.png" srcset="../_images/sphx_glr_plot_intersectional_bias_005.png" alt="Tension vs Diagnosis (train), Tension vs Intersect across groups with AD (test)" class = "sphx-glr-single-img"/><p>In the training set, people with schizophrenia are more likely to report tension at their
clinical assessment (left graph). Again, looking at how Black men with affective disorder compare
to White men on this particular feature in the test set, they are more likely to report tension
at the clinical interview. But it’s interesting to see that other groups also report high tension
relative to our reference group (white men).</p>
<p>So it’s not the only factor that’s potentially contributing to misdiagnosis, but it could be one
of the factors that explain the high false positive rates in Black men with affective disorder
(as well as in Asian men or Hispanic women, for example).</p>
<p>The goal of this tutorial was to show one potential way to probe into factors potentially
underlying biased performance, but there are other ways, many of which are not limited to
quantitative means. For example, ethnographic techniques can provide a more comprehensive and
meaningful understanding of the social, systemic, and political factors that contribute to
inequities, and they can be particularly powerful at elucidating the contextual factors leading
to training data bias.</p>
</section>
</section>
</section>
<section id="conclusions">
<h2>Conclusions<a class="headerlink" href="#conclusions" title="Link to this heading">#</a></h2>
<section id="reporting-to-our-stakeholders">
<h3>Reporting to our stakeholders<a class="headerlink" href="#reporting-to-our-stakeholders" title="Link to this heading">#</a></h3>
<p>Getting back to our hypothetical scenario, what do we report back to our stakeholders following
our intersectional bias assessment?</p>
<p>We conclude that although the model shows excellent performance overall, it may underserve
certain demographic groups, like Black men, and it should not be deployed without further
assessment by clinicians and further research into the factors contributing to bias. The hospital
should consider some targeted interventions (e.g., further assessment for Black men and other
intersectional groups, especially those reporting or displaying less rumination or more tension
at clinical assessments).</p>
<p>Overall, the model should not be deployed without further assessment by clinicians or
intervention.</p>
</section>
<section id="some-final-points">
<h3>Some final points<a class="headerlink" href="#some-final-points" title="Link to this heading">#</a></h3>
<p>Researchers have developed algorithmic methods to mitigate the fairness-related harms that may
result from ML models (by adjusting model parameters or modifying training data), <strong>but this does
not do anything to address the systemic factors</strong> contributing to bias. For example, if we fix
our model to reduce false positive predictions in Black men, will this increase their access to
care or treatment? Will it help clinicians better differentiate between symptoms of depression or
schizophrenia in these groups? As we have demonstrated in our case scenario, the problem is more
nuanced and the solution is much more complex, <strong>requiring collaboration between researchers,
clinicians, and public health or policy administrators</strong>. We need more research into these issues
and interventions that can address them. For example, some evidence suggests that forcing
clinicians to carry out more consistent and structured assessments can reduce diagnostic bias
(though not completely).
Put differently, machine learning systems are of a <strong>sociotechnical nature</strong> (see also our
<a class="reference internal" href="../user_guide/fairness_in_machine_learning.html#concept-glossary"><span class="std std-ref">user guide</span></a> and Selbst <em>et al.</em><a class="footnote-reference brackets" href="#footcite-selbst2019fairness" id="id37" role="doc-noteref"><span class="fn-bracket">[</span>32<span class="fn-bracket">]</span></a>).</p>
<p>The point is <strong>not</strong> that we shouldn’t be using ML to automate or inform clinical tasks (this
will likely happen whether we like it or not). Rather, ML can potentially help us <strong>better
understand the potential health inequities present within a health system</strong> (many of which we
might not catch because our own biases can prevent us from seeing and thinking about the
inequities). This underscores the potential of ML to identify contributing features that warrant
more research and to improve current clinical practices.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id38">
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footcite-ghassemi2022medicine" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Marzyeh Ghassemi and Elaine Okanyene Nsoesie. In medicine, how do we machine learn anything real? <em>Patterns</em>, 3(1):100392, January 2022. URL: <a class="reference external" href="https://doi.org/10.1016/j.patter.2021.100392">https://doi.org/10.1016/j.patter.2021.100392</a>, <a class="reference external" href="https://doi.org/10.1016/j.patter.2021.100392">doi:10.1016/j.patter.2021.100392</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-vyas2020hidden" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Darshali A. Vyas, Leo G. Eisenstein, and David S. Jones. Hidden in plain sight — reconsidering the use of race correction in clinical algorithms. <em>New England Journal of Medicine</em>, 383(9):874–882, August 2020. URL: <a class="reference external" href="https://doi.org/10.1056/nejmms2004740">https://doi.org/10.1056/nejmms2004740</a>, <a class="reference external" href="https://doi.org/10.1056/nejmms2004740">doi:10.1056/nejmms2004740</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-sikstrom2022conceptualising" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Laura Sikstrom, Marta M Maslej, Katrina Hui, Zoe Findlay, Daniel Z Buchman, and Sean L Hill. Conceptualising fairness: three pillars for medical algorithms and health equity. <em>BMJ Health Care Inform</em>, 29(1):e100459, January 2022. URL: <a class="reference external" href="https://doi.org/10.1136/bmjhci-2021-100459">https://doi.org/10.1136/bmjhci-2021-100459</a>, <a class="reference external" href="https://doi.org/10.1136/bmjhci-2021-100459">doi:10.1136/bmjhci-2021-100459</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-crenshaw1991intersectionality" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>Kimberlé Crenshaw. Mapping the margins: intersectionality, identity politics, and violence against women of color. <em>Stanford Law Review</em>, 43(6):1241–1299, 1991. URL: <a class="reference external" href="https://www.jstor.org/stable/1229039">https://www.jstor.org/stable/1229039</a>, <a class="reference external" href="https://arxiv.org/abs/https://www.jstor.org/stable/1229039">arXiv:https://www.jstor.org/stable/1229039</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.2307/1229039">doi:https://doi.org/10.2307/1229039</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-krieger2012population" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id13">2</a>)</span>
<p>Nancy Krieger. Who and what is a \textquotedblleft population\textquotedblright ? historical debates, current controversies, and implications for understanding \textquotedblleft population health\textquotedblright  and rectifying health inequities. <em>Milbank Quarterly</em>, 90(4):634–681, December 2012. URL: <a class="reference external" href="https://doi.org/10.1111/j.1468-0009.2012.00678.x">https://doi.org/10.1111/j.1468-0009.2012.00678.x</a>, <a class="reference external" href="https://doi.org/10.1111/j.1468-0009.2012.00678.x">doi:10.1111/j.1468-0009.2012.00678.x</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-wallace2011interactions" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">6</a><span class="fn-bracket">]</span></span>
<p>T. A. Wallace, D. N. Martin, and S. Ambs. Interactions among genes, tumor biology and the environment in cancer health disparities: examining the evidence on a national and global scale. <em>Carcinogenesis</em>, 32(8):1107–1121, April 2011. URL: <a class="reference external" href="https://doi.org/10.1093/carcin/bgr066">https://doi.org/10.1093/carcin/bgr066</a>, <a class="reference external" href="https://doi.org/10.1093/carcin/bgr066">doi:10.1093/carcin/bgr066</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-eknoyan2007adolphe" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">7</a><span class="fn-bracket">]</span></span>
<p>G. Eknoyan. Adolphe quetelet (1796 1874) the average man and indices of obesity. <em>Nephrology Dialysis Transplantation</em>, 23(1):47–51, August 2007. URL: <a class="reference external" href="https://doi.org/10.1093/ndt/gfm517">https://doi.org/10.1093/ndt/gfm517</a>, <a class="reference external" href="https://doi.org/10.1093/ndt/gfm517">doi:10.1093/ndt/gfm517</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-bbc2016dutch" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">8</a><span class="fn-bracket">]</span></span>
<p>BBC News. Dutch men revealed as world’s tallest — bbc.com. <a class="reference external" href="https://www.bbc.com/news/science-environment-36888541">https://www.bbc.com/news/science-environment-36888541</a>, 2016. [Accessed 19-Jul-2023].</p>
</aside>
<aside class="footnote brackets" id="footcite-endocrine2009widely" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">9</a><span class="fn-bracket">]</span></span>
<p>The Endocrine Society. Widely used body fat measurements overestimate fatness in african-americans, study finds. <a class="reference external" href="www.sciencedaily.com/releases/2009/06/090611142407.htm">www.sciencedaily.com/releases/2009/06/090611142407.htm</a>, 2009. [Accessed 19-Jul-2023].</p>
</aside>
<aside class="footnote brackets" id="footcite-racette2003obesity" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">10</a><span class="fn-bracket">]</span></span>
<p>Susan B Racette, Susan S Deusinger, and Robert H Deusinger. Obesity: overview of prevalence, etiology, and treatment. <em>Physical Therapy</em>, 83(3):276–288, March 2003. URL: <a class="reference external" href="https://doi.org/10.1093/ptj/83.3.276">https://doi.org/10.1093/ptj/83.3.276</a>, <a class="reference external" href="https://doi.org/10.1093/ptj/83.3.276">doi:10.1093/ptj/83.3.276</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-karasu2016adolphe" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">11</a><span class="fn-bracket">]</span></span>
<p>Sylvia R. Karasu. Adolphe Quetelet and the Evolution of Body Mass Index (BMI). <a class="reference external" href="https://www.psychologytoday.com/us/blog/the-gravity-weight/201603/adolphe-quetelet-and-the-evolution-body-mass-index-bmi">https://www.psychologytoday.com/us/blog/the-gravity-weight/201603/adolphe-quetelet-and-the-evolution-body-mass-index-bmi</a>, 2016. [Accessed 19-Jul-2023].</p>
</aside>
<aside class="footnote brackets" id="footcite-treviranus2019inclusivedesign" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">12</a><span class="fn-bracket">]</span></span>
<p>Jutta Treviranus. Inclusive Design: The Bell Curve, the Starburst and the Virtuous Tornado - Inclusive Design Research Centre. <a class="reference external" href="https://idrc.ocadu.ca/ideas/inclusive-design-the-bell-curve-the-starburst-and-the-virtuous-tornado/">https://idrc.ocadu.ca/ideas/inclusive-design-the-bell-curve-the-starburst-and-the-virtuous-tornado/</a>, 2019. [Accessed 19-Jul-2023].</p>
</aside>
<aside class="footnote brackets" id="footcite-godreau2021nonsovereign" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">13</a><span class="fn-bracket">]</span></span>
<p>Isar Godreau and Yarimar Bonilla. Nonsovereign racecraft: how colonialism, debt, and disaster are transforming puerto rican racial subjectivities. <em>American Anthropologist</em>, 123(3):509–525, June 2021. URL: <a class="reference external" href="https://doi.org/10.1111/aman.13601">https://doi.org/10.1111/aman.13601</a>, <a class="reference external" href="https://doi.org/10.1111/aman.13601">doi:10.1111/aman.13601</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-npssojournertruth" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">14</a><span class="fn-bracket">]</span></span>
<p>US National Park Service. Sojourner Truth: Ain’t I A Woman? <a class="reference external" href="https://www.nps.gov/wori/learn/historyculture/sojourner-truth.htm">https://www.nps.gov/wori/learn/historyculture/sojourner-truth.htm</a>. [Accessed 19-Jul-2023].</p>
</aside>
<aside class="footnote brackets" id="footcite-ford2010critical" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">15</a><span class="fn-bracket">]</span></span>
<p>Chandra L. Ford and Collins O. Airhihenbuwa. Critical race theory, race equity, and public health: toward antiracism praxis. <em>American Journal of Public Health</em>, 100(S1):S30–S35, April 2010. URL: <a class="reference external" href="https://doi.org/10.2105/ajph.2009.171058">https://doi.org/10.2105/ajph.2009.171058</a>, <a class="reference external" href="https://doi.org/10.2105/ajph.2009.171058">doi:10.2105/ajph.2009.171058</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-hanna2020towards" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">16</a><span class="fn-bracket">]</span></span>
<p>Alex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical race methodology in algorithmic fairness. In <em>Proceedings of the 2020 conference on fairness, accountability, and transparency</em>, 501–512. 2020.</p>
</aside>
<aside class="footnote brackets" id="footcite-justicecanadianhuman" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">17</a><span class="fn-bracket">]</span></span>
<p>Canadian Human Rights Act — laws-lois.justice.gc.ca. <a class="reference external" href="https://laws-lois.justice.gc.ca/eng/acts/h-6/fulltext.html">https://laws-lois.justice.gc.ca/eng/acts/h-6/fulltext.html</a>. [Accessed 19-Jul-2023].</p>
</aside>
<aside class="footnote brackets" id="footcite-obermeyer2019dissecting" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id20">1</a>,<a role="doc-backlink" href="#id21">2</a>)</span>
<p>Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias in an algorithm used to manage the health of populations. <em>Science</em>, 366(6464):447–453, 2019. URL: <a class="reference external" href="https://www.science.org/doi/abs/10.1126/science.aax2342">https://www.science.org/doi/abs/10.1126/science.aax2342</a>, <a class="reference external" href="https://arxiv.org/abs/https://www.science.org/doi/pdf/10.1126/science.aax2342">arXiv:https://www.science.org/doi/pdf/10.1126/science.aax2342</a>, <a class="reference external" href="https://doi.org/10.1126/science.aax2342">doi:10.1126/science.aax2342</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-buolamwini2018gender" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id22">1</a>,<a role="doc-backlink" href="#id29">2</a>)</span>
<p>Joy Buolamwini and Timnit Gebru. Gender shades: intersectional accuracy disparities in commercial gender classification. In <em>Conference on fairness, accountability and transparency</em>, 77–91. PMLR, 2018. URL: <a class="reference external" href="http://gendershades.org/index.html">http://gendershades.org/index.html</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-olbert2018meta" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id23">20</a><span class="fn-bracket">]</span></span>
<p>Charles M. Olbert, Arundati Nagendra, and Benjamin Buck. Meta-analysis of black vs. white racial disparity in schizophrenia diagnosis in the united states: do structured assessments attenuate racial disparities? <em>Journal of Abnormal Psychology</em>, 127(1):104–115, January 2018. URL: <a class="reference external" href="https://doi.org/10.1037/abn0000309">https://doi.org/10.1037/abn0000309</a>, <a class="reference external" href="https://doi.org/10.1037/abn0000309">doi:10.1037/abn0000309</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-gara2019naturalistic" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id24">1</a>,<a role="doc-backlink" href="#id33">2</a>)</span>
<p>Michael A. Gara, Shula Minsky, Steven M Silverstein, Theresa Miskimen, and Stephen M. Strakowski. A naturalistic study of racial disparities in diagnoses at an outpatient behavioral health clinic. <em>Psychiatric Services</em>, 70(2):130–134, February 2019. URL: <a class="reference external" href="https://doi.org/10.1176/appi.ps.201800223">https://doi.org/10.1176/appi.ps.201800223</a>, <a class="reference external" href="https://doi.org/10.1176/appi.ps.201800223">doi:10.1176/appi.ps.201800223</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-gara2012influence" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id25">22</a><span class="fn-bracket">]</span></span>
<p>Michael A. Gara, William A. Vega, Stephan Arndt, Michael Escamilla, David E. Fleck, William B. Lawson, Ira Lesser, Harold W. Neighbors, Daniel R. Wilson, Lesley M. Arnold, and Stephen M. Strakowski. Influence of patient race and ethnicity on clinical assessment in patients with affective disorders. <em>Archives of General Psychiatry</em>, June 2012. URL: <a class="reference external" href="https://doi.org/10.1001/archgenpsychiatry.2011.2040">https://doi.org/10.1001/archgenpsychiatry.2011.2040</a>, <a class="reference external" href="https://doi.org/10.1001/archgenpsychiatry.2011.2040">doi:10.1001/archgenpsychiatry.2011.2040</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-maslej2022race" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">23</a><span class="fn-bracket">]</span></span>
<p>Marta M. Maslej, Nelson Shen, Iman Kassam, Terri Rodak, and Laura Sikstrom. Race and racialization in mental health research and implications for developing and evaluating machine learning models: a rapid review. In <em>MEDINFO 2021: One World, One Health – Global Partnership for Digital Innovation</em>. IOS Press, June 2022. URL: <a class="reference external" href="https://doi.org/10.3233/shti220281">https://doi.org/10.3233/shti220281</a>, <a class="reference external" href="https://doi.org/10.3233/shti220281">doi:10.3233/shti220281</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-belsher2019prediction" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">24</a><span class="fn-bracket">]</span></span>
<p>Bradley E. Belsher, Derek J. Smolenski, Larry D. Pruitt, Nigel E. Bush, Erin H. Beech, Don E. Workman, Rebecca L. Morgan, Daniel P. Evatt, Jennifer Tucker, and Nancy A. Skopp. Prediction models for suicide attempts and deaths. <em>JAMA Psychiatry</em>, 76(6):642, June 2019. URL: <a class="reference external" href="https://doi.org/10.1001/jamapsychiatry.2019.0174">https://doi.org/10.1001/jamapsychiatry.2019.0174</a>, <a class="reference external" href="https://doi.org/10.1001/jamapsychiatry.2019.0174">doi:10.1001/jamapsychiatry.2019.0174</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-angwin2016machine" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id28">25</a><span class="fn-bracket">]</span></span>
<p>Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. 2016.</p>
</aside>
<aside class="footnote brackets" id="footcite-kuczmarski2018reducing" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id30">26</a><span class="fn-bracket">]</span></span>
<p>James Kuczmarski. Reducing gender bias in Google Translate. <a class="reference external" href="https://blog.google/products/translate/reducing-gender-bias-google-translate/">https://blog.google/products/translate/reducing-gender-bias-google-translate/</a>, 2018. [Accessed 31-07-2023].</p>
</aside>
<aside class="footnote brackets" id="footcite-seyyedkalantari2021underdiagnosis" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id31">27</a><span class="fn-bracket">]</span></span>
<p>Laleh Seyyed-Kalantari, Haoran Zhang, Matthew B. A. McDermott, Irene Y. Chen, and Marzyeh Ghassemi. Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations. <em>Nature Medicine</em>, 27(12):2176–2182, December 2021. URL: <a class="reference external" href="https://doi.org/10.1038/s41591-021-01595-0">https://doi.org/10.1038/s41591-021-01595-0</a>, <a class="reference external" href="https://doi.org/10.1038/s41591-021-01595-0">doi:10.1038/s41591-021-01595-0</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-barocas2019fairness" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id32">28</a><span class="fn-bracket">]</span></span>
<p>Solon Barocas, Moritz Hardt, and Arvind Narayanan. <em>Fairness and Machine Learning</em>. fairmlbook.org, 2019. URL: <a class="reference external" href="http://www.fairmlbook.org/">http://www.fairmlbook.org/</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-arnold2004ethnicity" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id34">29</a><span class="fn-bracket">]</span></span>
<p>Lesley M. Arnold, Paul E. Keck, Jacqueline Collins, Rodgers Wilson, David E. Fleck, Kimberly B. Corey, Jennifer Amicone, Victor R. Adebimpe, and Stephen M. Strakowski. Ethnicity and first-rank symptoms in patients with psychosis. <em>Schizophrenia Research</em>, 67(2-3):207–212, April 2004. URL: <a class="reference external" href="https://doi.org/10.1016/s0920-9964(02)00497-8">https://doi.org/10.1016/s0920-9964(02)00497-8</a>, <a class="reference external" href="https://doi.org/10.1016/s0920-9964(02)00497-8">doi:10.1016/s0920-9964(02)00497-8</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-herbst2022schizophrenia" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id35">30</a><span class="fn-bracket">]</span></span>
<p>Diane Herbst. Schizophrenia in black people: racial disparities explained. <a class="reference external" href="https://www.psycom.net/schizophrenia-racial-disparities-black-people">https://www.psycom.net/schizophrenia-racial-disparities-black-people</a>, 2022. [Accessed 31-07-2023].</p>
</aside>
<aside class="footnote brackets" id="footcite-banerjee2021readiang" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id36">31</a><span class="fn-bracket">]</span></span>
<p>Imon Banerjee, Ananth Reddy Bhimireddy, John L. Burns, Leo Anthony Celi, Li-Ching Chen, Ramon Correa, Natalie Dullerud, Marzyeh Ghassemi, Shih-Cheng Huang, Po-Chih Kuo, Matthew P Lungren, Lyle Palmer, Brandon J Price, Saptarshi Purkayastha, Ayis Pyrros, Luke Oakden-Rayner, Chima Okechukwu, Laleh Seyyed-Kalantari, Hari Trivedi, Ryan Wang, Zachary Zaiman, Haoran Zhang, and Judy W Gichoya. Reading race: ai recognises patient’s racial identity in medical images. 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2107.10356">https://arxiv.org/abs/2107.10356</a>, <a class="reference external" href="https://doi.org/10.48550/ARXIV.2107.10356">doi:10.48550/ARXIV.2107.10356</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-selbst2019fairness" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id37">32</a><span class="fn-bracket">]</span></span>
<p>Andrew D. Selbst, danah boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. Fairness and abstraction in sociotechnical systems. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, FAT* ‘19, 59–68. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://dl.acm.org/doi/10.1145/3287560.3287598">https://dl.acm.org/doi/10.1145/3287560.3287598</a>.</p>
</aside>
</aside>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 5.618 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-intersectional-bias-py">
<div class="lite-badge docutils container">
<a class="reference external image-reference" href="../lite/lab/index.html?path=auto_examples/plot_intersectional_bias.ipynb"><img alt="Launch JupyterLite" src="../_images/jupyterlite_badge_logo.svg" width="150px" /></a>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/891efb6a487df9dbcfb32bdec29fe6d7/plot_intersectional_bias.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_intersectional_bias.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/79d9f1e0baae66a0acbe37ff1d8b5f59/plot_intersectional_bias.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_intersectional_bias.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/c93123d7b0dc7321d4bd65a21a359ae7/plot_intersectional_bias.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_intersectional_bias.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


                </article>
              
              
              
              
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-fair-machine-learning-model">What is a fair machine learning model?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#who-and-what-is-a-population">Who and what is a population?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-reflective-activity">A reflective activity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-a-population">Defining a population</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#populations-as-statistical-entities">Populations as statistical entities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-issues">The issues</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proposition-2">Proposition 2</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#critical-race-theory">Critical race theory</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ain-t-i-a-woman">Ain’t I A Woman?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Critical race theory</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-practical-applications-to-machine-learning">The practical applications to machine learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#case-study-mental-health-care">Case Study: Mental Health Care</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario">Scenario</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithmic-fairness-in-health-care">Algorithmic Fairness in Health Care</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#background-schizophrenia">Background: Schizophrenia</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hypothetical-scenario">The hypothetical scenario</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-development">Model Development</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-the-data">Exploring the data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing">Preprocessing</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fairness-assessment">Fairness Assessment</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#quantifying-fairness">Quantifying fairness</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fairness-metrics">Fairness metrics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#racial-bias">Racial bias</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#intersectional-bias">Intersectional bias</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fairness-through-unawareness">Fairness through Unawareness?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-our-model-still-biased">Why is our model still biased?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-evaluation">Feature evaluation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">Conclusions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reporting-to-our-stakeholders">Reporting to our stakeholders</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-final-points">Some final points</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item"><div class="edit-example-link">
    
        <a href="https://github.com/fairlearn/fairlearn/edit/main/examples/plot_intersectional_bias.py">
            Edit this example on GitHub
        </a>
        <a href="https://github.com/fairlearn/fairlearn/issues/new?labels=documentation&template=doc_improvement.md&title=DOC%20Issue%20in%20Example%20Notebook%20plot_intersectional_bias.py">
            Report an issue on GitHub
        </a>
    
</div></div>

  <div class="sidebar-secondary-item">


  <div class="sphx-glr-sidebar-component">
    
      
        <div class="sphx-glr-sidebar-item sphx-glr-download-python-sidebar" title="plot_intersectional_bias.py">
          <a download href="../_downloads/79d9f1e0baae66a0acbe37ff1d8b5f59/plot_intersectional_bias.py">
            <i class="fa-solid fa-download"></i>
            Download source code
          </a>
        </div>
      
    
      
        <div class="sphx-glr-sidebar-item sphx-glr-download-jupyter-sidebar" title="plot_intersectional_bias.ipynb">
          <a download href="../_downloads/891efb6a487df9dbcfb32bdec29fe6d7/plot_intersectional_bias.ipynb">
            <i class="fa-solid fa-download"></i>
            Download Jupyter notebook
          </a>
        </div>
      
    
      
        <div class="sphx-glr-sidebar-item sphx-glr-download-zip-sidebar" title="plot_intersectional_bias.zip">
          <a download href="../_downloads/c93123d7b0dc7321d4bd65a21a359ae7/plot_intersectional_bias.zip">
            <i class="fa-solid fa-download"></i>
            Download zipped
          </a>
        </div>
      
    
  </div>
</div>

  <div class="sidebar-secondary-item">


  <div class="sphx-glr-sidebar-component">
    
      
        <div class="sphx-glr-sidebar-item lite-badge-sidebar">
          <a href="../lite/lab/index.html?path=auto_examples/plot_intersectional_bias.ipynb">
            <img src="../_images/jupyterlite_badge_logo.svg" alt="Launch JupyterLite">
          </a>
        </div>
      
    
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018 - 2025, Fairlearn contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>