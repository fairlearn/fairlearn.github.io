{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metrics with Multiple Features\n==============================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates the new API for metrics, which supports\nmultiple sensitive and conditional features. This example does not\ncontain a proper discussion of how fairness relates to the dataset used,\nalthough it does highlight issues which users may want to consider when\nanalysing their datasets.\n\nWe are going to consider a lending scenario, supposing that we have a\nmodel which predicts whether or not a particular customer will repay a\nloan. This could be used as the basis of deciding whether or not to\noffer that customer a loan. With traditional metrics, we would assess\nthe model using:\n\n-   The 'true' values from the test set\n-   The model predictions from the test set\n\nOur fairness metrics compute group-based fairness statistics. To use\nthese, we also need categorical columns from the test set. For this\nexample, we will include:\n\n-   The sex of each individual (two unique values)\n-   The race of each individual (three unique values)\n-   The credit score band of each individual (three unique values)\n-   Whether the loan is considered 'large' or 'small'\n\nAn individual's sex and race should not affect a lending decision, but\nit would be legitimate to consider an individual's credit score and the\nrelative size of the loan which they desired.\n\nA real scenario will be more complicated, but this will serve to\nillustrate the use of the new metrics.\n\nGetting the Data\n================\n\n*This section may be skipped. It simply creates a dataset for\nillustrative purposes*\n\nWe will use the well-known UCI 'Adult' dataset as the basis of this\ndemonstration. This is not for a lending scenario, but we will regard it\nas one for the purposes of this example. We will use the existing 'race'\nand 'sex' columns (trimming the former to three unique values), and\nmanufacture credit score bands and loan sizes from other columns. We\nstart with some uncontroversial import statements:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import MetricFrame\nfrom fairlearn.metrics import selection_rate\nimport functools\nimport sklearn.metrics as skm\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we import the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = fetch_openml(data_id=1590, as_frame=True)\nX_raw = data.data\nY = (data.target == '>50K') * 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For purposes of clarity, we consolidate the 'race' column to have three\nunique values:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def race_transform(input_str):\n    \"\"\"Reduce values to White, Black and Other.\"\"\"\n    result = 'Other'\n    if input_str == 'White' or input_str == 'Black':\n        result = input_str\n    return result\n\n\nX_raw['race'] = X_raw['race'].map(race_transform).fillna('Other')\nprint(np.unique(X_raw['race']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we manufacture the columns for the credit score band and requested\nloan size. These are wholly constructed, and not part of the actual\ndataset in any way. They are simply for illustrative purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def marriage_transform(m_s_string):\n    \"\"\"Perform some simple manipulations.\"\"\"\n    result = 'Low'\n    if m_s_string.startswith(\"Married\"):\n        result = 'Medium'\n    elif m_s_string.startswith(\"Widowed\"):\n        result = 'High'\n    return result\n\n\ndef occupation_transform(occ_string):\n    \"\"\"Perform some simple manipulations.\"\"\"\n    result = 'Small'\n    if occ_string.startswith(\"Machine\"):\n        result = 'Large'\n    return result\n\n\ncol_credit = X_raw['marital-status'].map(marriage_transform).fillna('Low')\ncol_credit.name = \"Credit Score\"\ncol_loan_size = X_raw['occupation'].map(occupation_transform).fillna('Small')\ncol_loan_size.name = \"Loan Size\"\n\nA = X_raw[['race', 'sex']]\nA['Credit Score'] = col_credit\nA['Loan Size'] = col_loan_size\nA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the data imported, we perform some standard processing, and a\ntest/train split:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\nY = le.fit_transform(Y)\n\nle = LabelEncoder()\n\nsc = StandardScaler()\nX_dummies = pd.get_dummies(X_raw)\nX_scaled = sc.fit_transform(X_dummies)\nX_scaled = pd.DataFrame(X_scaled, columns=X_dummies.columns)\n\nX_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_scaled, Y, A,\n                                                                     test_size=0.3,\n                                                                     random_state=12345,\n                                                                     stratify=Y)\n\n# Ensure indices are aligned\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\nA_train = A_train.reset_index(drop=True)\nA_test = A_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we train a simple model on the data, and generate some\npredictions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\nunmitigated_predictor.fit(X_train, Y_train)\n\nY_pred = unmitigated_predictor.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analysing the Model with Metrics\n================================\n\nAfter our data manipulations and model training, we have the following\nfrom our test set:\n\n-   A vector of true values called `Y_test`\n-   A vector of model predictions called `Y_pred`\n-   A DataFrame of categorical features relevant to fairness called\n    `A_test`\n\nIn a traditional model analysis, we would now look at some metrics\nevaluated on the entire dataset. Suppose in this case, the relevant\nmetrics are fairlearn.metrics.selection\\_rate and\nsklearn.metrics.fbeta\\_score (with `beta=0.6`). We can evaluate these\nmetrics directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Selection Rate:\", selection_rate(Y_test, Y_pred))\nprint(\"fbeta:\", skm.fbeta_score(Y_test, Y_pred, beta=0.6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We know that there are sensitive features in our data, and we want to\nensure that we're not harming individuals due to membership in any of\nthese groups. For this purpose, Fairlearn provides the\nfairlearn.metrics.MetricFrame class. Let us construct an instance of\nthis class, and then look at its capabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fbeta_06 = functools.partial(skm.fbeta_score, beta=0.6)\n\nmetric_fns = {'selection_rate': selection_rate, 'fbeta_06': fbeta_06}\n\ngrouped_on_sex = MetricFrame(metric_fns,\n                             Y_test, Y_pred,\n                             sensitive_features=A_test['sex'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fairlearn.metrics.MetricFrame object requires a minimum of four\narguments:\n\n1.  The underlying metric function(s) to be evaluated\n2.  The true values\n3.  The predicted values\n4.  The sensitive feature values\n\nThese are all passed as arguments to the constructor. If more than one\nunderlying metric is required (as in this case), then we must provide\nthem in a dictionary.\n\nThe underlying metrics must have a signature `fn(y_true, y_pred)`, so we\nhave to use functools.partial on `fbeta_score()` to furnish `beta=0.6`\n(we will show how to pass in extra array arguments such as sample\nweights shortly).\n\nWe will now take a closer look at the fairlearn.metrics.MetricFrame\nobject. First, there is the `overall` property, which contains the\nmetrics evaluated on the entire dataset. We see that this contains the\nsame values calculated above:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert grouped_on_sex.overall['selection_rate'] == selection_rate(Y_test, Y_pred)\nassert grouped_on_sex.overall['fbeta_06'] == skm.fbeta_score(Y_test, Y_pred, beta=0.6)\nprint(grouped_on_sex.overall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The other property in the fairlearn.metrics.MetricFrame object is\n`by_group`. This contains the metrics evaluated on each subgroup defined\nby the categories in the `sensitive_features=` argument. In this case,\nwe have results for males and females:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_sex.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can immediately see a substantial disparity in the selection rate\nbetween males and females.\n\nWe can also create another fairlearn.metrics.MetricFrame object using\nrace as the sensitive feature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race = MetricFrame(metric_fns,\n                              Y_test, Y_pred,\n                              sensitive_features=A_test['race'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `overall` property is unchanged:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert (grouped_on_sex.overall == grouped_on_race.overall).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `by_group` property now contains the metrics evaluated based on the\n'race' column:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there is also a significant disparity in selection rates\nwhen grouping by race.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sample weights and other arrays\n===============================\n\nWe noted above that the underlying metric functions passed to the\nfairlearn.metrics.MetricFrame constructor need to be of the form\n`fn(y_true, y_pred)` - we do not support scalar arguments such as\n`pos_label=` or `beta=` in the constructor. Such arguments should be\nbound into a new function using functools.partial, and the result passed\nin. However, we do support arguments which have one entry for each\nsample, with an array of sample weights being the most common example.\nThese are divided into subgroups along with `y_true` and `y_pred`, and\npassed along to the underlying metric.\n\nTo use these arguments, we pass in a dictionary as the `sample_params=`\nargument of the constructor. Let us generate some random weights, and\npass these along:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_weights = np.random.rand(len(Y_test))\n\nexample_sample_params = {\n    'selection_rate': {'sample_weight': random_weights},\n    'fbeta_06': {'sample_weight': random_weights},\n}\n\n\ngrouped_with_weights = MetricFrame(metric_fns,\n                                   Y_test, Y_pred,\n                                   sensitive_features=A_test['sex'],\n                                   sample_params=example_sample_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can inspect the overall values, and check they are as expected:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert grouped_with_weights.overall['selection_rate'] == \\\n    selection_rate(Y_test, Y_pred, sample_weight=random_weights)\nassert grouped_with_weights.overall['fbeta_06'] == \\\n    skm.fbeta_score(Y_test, Y_pred, beta=0.6, sample_weight=random_weights)\nprint(grouped_with_weights.overall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see the effect on the metric being evaluated on the\nsubgroups:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_with_weights.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantifying Disparities\n=======================\n\nWe now know that our model is selecting individuals who are female far\nless often than individuals who are male. There is a similar effect when\nexamining the results by race, with blacks being selected far less often\nthan whites (and those classified as 'other'). However, there are many\ncases where presenting all these numbers at once will not be useful (for\nexample, a high level dashboard which is monitoring model performance).\nFairlearn provides several means of aggregating metrics across the\nsubgroups, so that disparities can be readily quantified.\n\nThe simplest of these aggregations is `group_min()`, which reports the\nminimum value seen for a subgroup for each underlying metric (we also\nprovide `group_max()`). This is useful if there is a mandate that \"no\nsubgroup should have an `fbeta_score()` of less than 0.6.\" We can\nevaluate the minimum values easily:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.group_min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As noted above, the selection rates varies greatly by race and by sex.\nThis can be quantified in terms of a difference between the subgroup\nwith the highest value of the metric, and the subgroup with the lowest\nvalue. For this, we provide the method\n`difference(method='between_groups)`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.difference(method='between_groups')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also evaluate the difference relative to the corresponding\noverall value of the metric. In this case we take the absolute value, so\nthat the result is always positive:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.difference(method='to_overall')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are situations where knowing the ratios of the metrics evaluated\non the subgroups is more useful. For this we have the `ratio()` method.\nWe can take the ratios between the minimum and maximum values of each\nmetric:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.ratio(method='between_groups')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also compute the ratios relative to the overall value for each\nmetric. Analogous to the differences, the ratios are always in the range\n$[0,1]$:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.ratio(method='to_overall')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intersections of Features\n=========================\n\nSo far we have only considered a single sensitive feature at a time, and\nwe have already found some serious issues in our example data. However,\nsometimes serious issues can be hiding in intersections of features. For\nexample, the [Gender Shades\nproject](https://www.media.mit.edu/projects/gender-shades/overview/)\nfound that facial recognition algorithms performed worse for blacks than\nwhites, and also worse for women than men (despite overall high accuracy\nscore). Moreover, performance on black females was *terrible*. We can\nexamine the intersections of sensitive features by passing multiple\ncolumns to the fairlearn.metrics.MetricFrame constructor:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race_and_sex = MetricFrame(metric_fns,\n                                      Y_test, Y_pred,\n                                      sensitive_features=A_test[['race', 'sex']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The overall values are unchanged, but the `by_group` table now shows the\nintersections between subgroups:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert (grouped_on_race_and_sex.overall == grouped_on_race.overall).all()\ngrouped_on_race_and_sex.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aggregations are still performed across all subgroups for each\nmetric, so each continues to reduce to a single value. If we look at the\n`group_min()`, we see that we violate the mandate we specified for the\n`fbeta_score()` suggested above (for females with a race of 'Other' in\nfact):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race_and_sex.group_min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the `ratio()` method, we see that the disparity is worse\n(specifically between white males and black females, if we check in the\n`by_group` table):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race_and_sex.ratio(method='between_groups')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Control Features\n================\n\nThere is a further way we can slice up our data. We have (*completely\nmade up*) features for the individuals' credit scores (in three bands)\nand also the size of the loan requested (large or small). In our loan\nscenario, it is acceptable that individuals with high credit scores are\nselected more often than individuals with low credit scores. However,\nwithin each credit score band, we do not want a disparity between (say)\nblack females and white males. To example these cases, we have the\nconcept of *control features*.\n\nControl features are introduced by the `control_features=` argument to\nthe fairlearn.metrics.MetricFrame object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_credit_score = MetricFrame(metric_fns,\n                                Y_test, Y_pred,\n                                sensitive_features=A_test[['race', 'sex']],\n                                control_features=A_test['Credit Score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This has an immediate effect on the `overall` property. Instead of\nhaving one value for each metric, we now have a value for each unique\nvalue of the control feature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_credit_score.overall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `by_group` property is similarly expanded:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_credit_score.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aggregates are also evaluated once for each group identified by the\ncontrol feature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_credit_score.group_min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_credit_score.ratio(method='between_groups')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In our data, we see that we have a dearth of positive results for high\nincome non-whites, which significantly affects the aggregates.\n\nWe can continue adding more control features:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_both = MetricFrame(metric_fns,\n                        Y_test, Y_pred,\n                        sensitive_features=A_test[['race', 'sex']],\n                        control_features=A_test[['Loan Size', 'Credit Score']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `overall` property now splits into more values:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_both.overall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As does the `by_groups` property, where `NaN` values indicate that there\nwere no samples in the cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_both.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aggregates behave similarly. By this point, we are having\nsignificant issues with under-populated intersections. Consider:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def member_counts(y_true, y_pred):\n    assert len(y_true) == len(y_pred)\n    return len(y_true)\n\n\ncounts = MetricFrame(member_counts,\n                     Y_test, Y_pred,\n                     sensitive_features=A_test[['race', 'sex']],\n                     control_features=A_test[['Loan Size', 'Credit Score']])\n\ncounts.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that `NaN` indicates that there were no individuals in a cell -\n`member_counts()` will not even have been called.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}