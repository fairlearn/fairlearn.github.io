{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Making Derived Metrics\n======================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates the use of the\nfairlearn.metrics.make\\_derived\\_metric function. Many higher-order\nmachine learning algorithms (such as hyperparameter tuners) make use of\nscalar metrics when deciding how to proceed. While the\nfairlearn.metrics.MetricFrame has the ability to produce such scalars\nthrough its aggregation functions, its API does not conform to that\nusually expected by these algorithms. The\nfairlearn.metrics.make\\_derived\\_metric function exists to bridge this\ngap.\n\nGetting the Data\n================\n\n*This section may be skipped. It simply creates a dataset for\nillustrative purposes*\n\nWe will use the well-known UCI 'Adult' dataset as the basis of this\ndemonstration. This is not for a lending scenario, but we will regard it\nas one for the purposes of this example. We will use the existing 'race'\nand 'sex' columns (trimming the former to three unique values), and\nmanufacture credit score bands and loan sizes from other columns. We\nstart with some uncontroversial import statements:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import MetricFrame, make_derived_metric\nfrom fairlearn.metrics import accuracy_score_group_min\nimport sklearn.metrics as skm\nimport functools\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we import the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = fetch_openml(data_id=1590, as_frame=True)\nX_raw = data.data\nY = (data.target == '>50K') * 1\nA = X_raw[['race', 'sex']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the data imported, we perform some standard processing, and a\ntest/train split:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\nY = le.fit_transform(Y)\n\nle = LabelEncoder()\n\nsc = StandardScaler()\nX_dummies = pd.get_dummies(X_raw)\nX_scaled = sc.fit_transform(X_dummies)\nX_scaled = pd.DataFrame(X_scaled, columns=X_dummies.columns)\n\nX_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_scaled, Y, A,\n                                                                     test_size=0.3,\n                                                                     random_state=12345,\n                                                                     stratify=Y)\n\n# Ensure indices are aligned\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\nA_train = A_train.reset_index(drop=True)\nA_test = A_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we train a simple model on the data, and generate some\npredictions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\nunmitigated_predictor.fit(X_train, Y_train)\n\nY_pred = unmitigated_predictor.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a derived metric\n=========================\n\nSuppose our key metric is the accuracy score, and we are most interested\nin ensuring that it exceeds some threshold for all subgroups We might\nuse the fairlearn.metrics.MetricFrame as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "acc_frame = MetricFrame(skm.accuracy_score,\n                        Y_test, Y_pred,\n                        sensitive_features=A_test['sex'])\nprint(\"Minimum accuracy_score: \", acc_frame.group_min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can create a function to perform this in a single call using\nfairlearn.metrics.make\\_derived\\_metric. This takes the following\narguments (which must always be supplied as keyword arguments):\n\n-   `metric=`{.sourceCode}, the base metric function\n-   `transform=`{.sourceCode}, the name of the aggregation\n    transformation to perform. For this demonstration, we want this to\n    be `'group_min'`{.sourceCode}\n-   `sample_param_names=`{.sourceCode}, a list of parameter names which\n    should be treated as sample parameters. This is optional, and\n    defaults to `['sample_weight']`{.sourceCode} which is appropriate\n    for many metrics in SciKit-Learn.\n\nThe result is a new function with the same signature as the base metric,\nwhich accepts two extra arguments:\n\n> -   `sensitive_features=`{.sourceCode} to specify the sensitive\n>     features which define the subgroups\n> -   `method=`{.sourceCode} to adjust how the aggregation\n>     transformation operates. This corresponds to the same argument in\n>     fairlearn.metrics.MetricFrame.difference and\n>     fairlearn.metrics.MetricFrame.ratio\n\nFor the current case, we do not need the `method=`{.sourceCode}\nargument, since we are taking the minimum value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "my_acc = make_derived_metric(metric=skm.accuracy_score,\n                             transform='group_min')\nmy_acc_min = my_acc(Y_test, Y_pred,\n                    sensitive_features=A_test['sex'])\nprint(\"Minimum accuracy_score: \", my_acc_min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To show that the returned function also works with sample weights:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_weights = np.random.rand(len(Y_test))\n\nacc_frame_sw = MetricFrame(skm.accuracy_score,\n                           Y_test, Y_pred,\n                           sensitive_features=A_test['sex'],\n                           sample_params={'sample_weight': random_weights})\n\nfrom_frame = acc_frame_sw.group_min()\nfrom_func = my_acc(Y_test, Y_pred,\n                   sensitive_features=A_test['sex'],\n                   sample_weight=random_weights)\n\nprint('From MetricFrame:', from_frame)\nprint('From function   :', from_func)\nassert from_frame == from_func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The returned function can also handle parameters which are not sample\nparameters. Consider sklearn.metrics.fbeta\\_score, which has a required\n`beta=`{.sourceCode} argument (and suppose that this time we are most\ninterested in the maximum difference to the overall value). First we\nevaluate this with a fairlearn.metrics.MetricFrame:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fbeta_03 = functools.partial(skm.fbeta_score, beta=0.3)\n\nbeta_frame = MetricFrame(fbeta_03,\n                         Y_test, Y_pred,\n                         sensitive_features=A_test['sex'],\n                         sample_params={'sample_weight': random_weights})\nbeta_from_frame = beta_frame.difference(method='to_overall')\n\nprint(\"From frame:\", beta_from_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And next, we create a function to evaluate the same. Note that we do not\nneed to use functools.partial to bind the `beta=`{.sourceCode} argument:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "beta_func = make_derived_metric(metric=skm.fbeta_score,\n                                transform='difference')\n\nbeta_from_func = beta_func(Y_test, Y_pred,\n                           sensitive_features=A_test['sex'],\n                           beta=0.3,\n                           sample_weight=random_weights,\n                           method='to_overall')\n\nprint(\"From function:\", beta_from_func)\nassert beta_from_func == beta_from_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pregenerated Metrics\n====================\n\nWe provide a number of pregenerated metrics, to cover common use cases.\nFor example, we provide a `accuracy_score_group_min()`{.sourceCode}\nfunction to find the minimum over the accuracy scores:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from_myacc = my_acc(Y_test, Y_pred,\n                    sensitive_features=A_test['race'])\n\nfrom_pregen = accuracy_score_group_min(Y_test, Y_pred,\n                                       sensitive_features=A_test['race'])\n\nprint(\"From my function :\", from_myacc)\nprint(\"From pregenerated:\", from_pregen)\nassert from_myacc == from_pregen"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}