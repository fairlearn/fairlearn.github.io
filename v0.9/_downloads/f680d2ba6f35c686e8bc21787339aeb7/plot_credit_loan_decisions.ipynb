{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Credit Loan Decisions\n=====================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Package Imports\n===============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom fairlearn.metrics import MetricFrame\nfrom fairlearn.metrics import (\n    count,\n    selection_rate,\n    equalized_odds_difference,\n    false_positive_rate,\n    false_negative_rate,\n)\nfrom fairlearn.postprocessing import ThresholdOptimizer\nfrom fairlearn.reductions import ExponentiatedGradient\nfrom fairlearn.reductions import EqualizedOdds\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\nwarnings.simplefilter(\"ignore\")\n\nrand_seed = 1234\nnp.random.seed(rand_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fairness considerations of credit loan decisions\n================================================\n\nFairness and credit lending in the US\n-------------------------------------\n\nIn 2019, Apple received backlash on social media after its newly\nlaunched *Apple Card* product appeared to offer higher credit limits to\nmen compared to women `nedlund2019apple`{.interpreted-text\nrole=\"footcite\"}. In multiple cases, married couples found the husband\nreceived a credit limit that was 10-20x higher than the wife\\'s even\nwhen the couple had joint assets.\n\nFrom a regulatory perspective, financial institutions that operate\nwithin the United States are subject to *legal regulations* prohibiting\ndiscrimination on the [basis of race, gender, or other protected classes\n:footcite:\\`uscode2011title15chapter41subchapteriv]{.title-ref}. With\nthe increasing prevalence of automated decision-systems in the financial\nlending space, experts have raised concerns about whether these systems\ncould exacerabate existing inequalities in financial lending.\n\nAlthough the two concepts are intertwined, algorithmic fairness is not\nthe same concept as anti-discrimination law. An AI system can comply\nwith anti-discrimination law while exhibiting fairness-related concerns.\nOn the other hand, some fairness interventions may be illegal under\nanti-discrimination law.\n:footcite`Xiang2019legalcompatibility`{.interpreted-text role=\"cts\"}\ndiscuss the compatibilities and disconnects between anti-discrimination\nlaw and algorithmic notions of fairness. In this case study, we focus on\nfairness in financial services rather than compliance with financial\nanti-discrimination regulations.\n\nErnst & Young (EY) case study\n=============================\n\nIn this case study, we aim to replicate the work done in a white paper\n`dudik2020assessing`{.interpreted-text role=\"footcite\"}, co-authored by\n*Microsoft* and *EY*, on mitigating gender-related performance\ndisparities in financial lending decisions. In their analysis, Microsoft\nand EY demonstrated how Fairlearn could be used to measure and mitigate\nunfairness in the loan adjudication process.\n\nUsing a dataset of credit loan outcomes (whether an individual defaulted\non a credit loan), we train a fairness-unaware model to predict the\nlikelihood an individual will default on a given loan. We use the\nFairlearn toolkit for assessing the fairness of our model, according to\nseveral metrics. Finally, we perform two unfairness mitigation\nstrategies on our model and compare the results to our original model.\n\nBecause the dataset used in the white paper is not publicly available,\nwe will introduce a semi-synthetic feature into an existing publicly\navailable dataset to replicate the outcome disparity found in the\noriginal dataset.\n\nCredit decisions dataset\n------------------------\n\nAs mentioned, we will not be able to use the original loans dataset, and\ninstead will be working with a publicly available dataset of credit card\ndefaults in Taiwan collected in 2005. This dataset represents binary\ncredit card default outcomes for 30,000 applicants with information\npertaining to an applicant\\'s payment history and bill statements over a\nsix-month period from April 2005 to September 2005, as well as\ndemographic information, such as *sex*, *age*, *marital status*, and\n*education level* of the applicant. A full summary of features is\nprovided below:\n\n  features                                                                   description\n  -------------------------------------------------------------------------- --------------------------------------------\n  sex, education, marriage, age                                              demographic features\n  pay\\_0, pay\\_2, pay\\_3, pay\\_4, pay\\_5, pay\\_6                             repayment status (ordinal)\n  bill\\_amt1, bill\\_amt2, bill\\_amt3, bill\\_amt4, bill\\_amt5, bill\\_amt\\_6   bill statement amount (Taiwan dollars)\n  pay\\_amt1, pay\\_amt2, pay\\_amt3, pay\\_amt4, pay\\_amt5, pay\\_amt6           previous statement amount (Taiwan dollars)\n  default payment next month                                                 default information (1 = YES, 0 = NO)\n\nLet\\'s pretend we are a data scientist at a financial institution who is\ntasked with developing a classification model to predict whether an\napplicant will default on a personal loan. A positive prediction by the\nmodel means the applicant would default on the credit loan. *Defaulting\non a loan* means the client fails to make payments within a 30-day\nwindow, and the lender can take legal actions against the client.\n\nAlthough we do not have a dataset of loan default history, we do have\nthis data set of credit card payment history. We assume customers who\nmake monthly credit card payments on time are more *creditworthy*, and\nthus less likely to default on a personal credit loan.\n\n**Decision point: task definition**\n\n-   **Defaulting on a credit card payment** can be viewed as a proxy for\n    the fact that an applicant might not be a good candidate for a\n    personal loan.\n-   Because most customers did not default on their credit card payment,\n    we will need to take this class imbalance into account during our\n    modeling process.\n\nAs the data is read in-memory, we will change the column\n`PAY_0`{.sourceCode} to `PAY_1`{.sourceCode} to make the naming more\nconsistent with the naming of the other columns. In addition, the target\nvariable `default payment next month`{.sourceCode} is changed to\n`default`{.sourceCode} to reduce verbosity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\ndataset = (\n    pd.read_excel(io=data_url, header=1)\n    .drop(columns=[\"ID\"])\n    .rename(\n        columns={\"PAY_0\": \"PAY_1\", \"default payment next month\": \"default\"}\n    )\n)\n\ndataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the [dataset description\n:footcite:\\`yeh2009comparisons]{.title-ref}, we see there are three\ncategorical features:\n\n-   `SEX`{.sourceCode}: Sex of the applicant (as a binary feature)\n-   `EDUCATION`{.sourceCode}: Highest level of education achieved by the\n    applicant.\n-   `MARRIAGE`{.sourceCode}: Marital status of the applicant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "categorical_features = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]\n\nfor col_name in categorical_features:\n    dataset[col_name] = dataset[col_name].astype(\"category\")\n\nY, A = dataset.loc[:, \"default\"], dataset.loc[:, \"SEX\"]\nX = pd.get_dummies(dataset.drop(columns=[\"default\", \"SEX\"]))\n\nA_str = A.map({1: \"male\", 2: \"female\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset imbalances\n==================\n\nBefore we start training a classifier model, we want to explore the\ndataset for any characteristics that may lead to fairness-related harms\nlater on in the modeling process. In particular, we will focus on the\ndistribution of sensitive feature `SEX`{.sourceCode} and the target\nlabel `default`{.sourceCode}.\n\nAs part of an exploratory data analysis, let\\'s explore the distribution\nof our sensitive feature `SEX`{.sourceCode}. We see that 60% of loan\napplicants were labeled as [female]{.title-ref} and 40% as\n[male]{.title-ref}, so we do not need to worry about imbalance in this\nfeature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A_str.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let\\'s explore the distribution of the *loan default rate*\n`Y`{.sourceCode}. We see that around 78% of individuals in the dataset\ndo not default on their credit loan. While the target label does not\ndisplay extreme imbalance, we will need to account for this imbalance in\nour modeling section. As opposed to the *sensitive feature*\n`SEX`{.sourceCode}, an imbalance in the target label may result in a\nclassifier that over-optimizes for the majority class. For example, a\nclassifier that predicts an applicant will not default would achieve an\naccuracy of 78%, so we will use the `balanced_accuracy`{.sourceCode}\nscore as our evaluation metric to counteract the label imbalance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add synthetic noise that is related to the outcome and sex\n==========================================================\n\nFor the purpose of this case study, we add a synthetic feature\n`Interest`{.sourceCode} that introduces correlation between the\n`SEX`{.sourceCode} label of an applicant and the `default`{.sourceCode}\noutcome. The purpose of this feature is to replicate outcome disparities\npresent in the original dataset. We can think of this\n`Interest`{.sourceCode} feature as the *interest rate* for the\napplicant. If the applicant has a history of defaulting on credit card\npayments, the bank will lend to the applicant at a higher interest rate.\nWe also assume because banks have historically lended primarily to men,\nthere is less uncertainty (or variance) in the *interest rate* for these\napplicants.\n\nTo reflect the above reasoning, the `Interest`{.sourceCode} feature is\ndrawn from a *Gaussian distribution* with the following criterion:\n\n-   If *Male*, draw `Interest`{.sourceCode} from\n    $\\mathcal{N}(2 \\cdot \\text{Default}, 1)$\n-   If *Female*, draw `Interest`{.sourceCode} from\n    $\\mathcal{N}(2 \\cdot \\text{Default}, 2)$\n\nThis feature is drawn from a *Gaussian distribution* for computational\nsimplicity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X.loc[:, \"Interest\"] = np.random.normal(loc=2 * Y, scale=A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if this will lead to disparity in naive model\n===================================================\n\nNow that we have created our synthetic feature, let\\'s check how this\nnew feature interacts with our *sensitive\\_feature* `Sex`{.sourceCode}\nand our target label `default`{.sourceCode}. We see that for both sexes,\nthe `Interest`{.sourceCode} feature is higher for individuals who\ndefaulted on their loan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax_1, ax_2) = plt.subplots(\n    ncols=2, figsize=(10, 4), sharex=True, sharey=True\n)\nX[\"Interest\"][(A == 1) & (Y == 0)].plot(\n    kind=\"kde\", label=\"Payment on Time\", ax=ax_1, title=\"INTEREST for Men\"\n)\nX[\"Interest\"][(A == 1) & (Y == 1)].plot(\n    kind=\"kde\", label=\"Payment Default\", ax=ax_1\n)\nX[\"Interest\"][(A == 2) & (Y == 0)].plot(\n    kind=\"kde\",\n    label=\"Payment on Time\",\n    ax=ax_2,\n    legend=True,\n    title=\"INTEREST for Women\",\n)\nX[\"Interest\"][(A == 2) & (Y == 1)].plot(\n    kind=\"kde\", label=\"Payment Default\", ax=ax_2, legend=True\n).legend(bbox_to_anchor=(1.6, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training an initial model\n=========================\n\nIn this section, we will train a fairness-unaware model on the training\ndata. However because of the imbalances in the dataset, we will first\nresample the training data to produce a new balanced training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def resample_training_data(X_train, Y_train, A_train):\n    \"\"\"Down-sample the majority class in the training dataset to produce a\n    balanced dataset with a 50/50 split in the predictive labels.\n\n    Parameters:\n    X_train: The training split of the features\n    Y_train: The training split of the target labels\n    A_train: The training split of the sensitive features\n\n    Returns:\n    Tuple of X_train, Y_train, A_train where each dataset has been re-balanced.\n    \"\"\"\n    negative_ids = Y_train[Y_train == 0].index\n    positive_ids = Y_train[Y_train == 1].index\n    balanced_ids = positive_ids.union(\n        np.random.choice(a=negative_ids, size=len(positive_ids))\n    )\n\n    X_train = X_train.loc[balanced_ids, :]\n    Y_train = Y_train.loc[balanced_ids]\n    A_train = A_train.loc[balanced_ids]\n    return X_train, Y_train, A_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n    X, Y, A_str, test_size=0.35, stratify=Y\n)\n\nX_train, y_train, A_train = resample_training_data(X_train, y_train, A_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this stage, we will train a *gradient-boosted tree classifier* using\nthe `lightgbm`{.sourceCode} package on the balanced training dataset.\nWhen we evaluate the model, we will use the unbalanced testing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lgb_params = {\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 10,\n    \"max_depth\": 3,\n    \"random_state\": rand_seed,\n    \"n_jobs\": 1,\n}\n\nestimator = Pipeline(\n    steps=[\n        (\"preprocessing\", StandardScaler()),\n        (\"classifier\", lgb.LGBMClassifier(**lgb_params)),\n    ]\n)\n\nestimator.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We compute the *binary predictions* and the *prediction probabilities*\nfor the testing data points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_pred_proba = estimator.predict_proba(X_test)[:, 1]\nY_pred = estimator.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the *ROC Score*, we see the model appears to be differentiating\nbetween *true positives* and *false positives* well. This is to be\nexpected given the `INTEREST`{.sourceCode} feature provides a strong\ndiscriminant feature for the classification task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "roc_auc_score(y_test, Y_pred_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature Importance of the Unmitigated Classifier\n================================================\n\nAs a model validation check, let\\'s explore the feature importances of\nour classifier. As expected, our synthetic feature\n`INTEREST`{.sourceCode} has the highest feature importance because it is\nhighly correlated with the target variable, by construction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lgb.plot_importance(\n    estimator.named_steps[\"classifier\"],\n    height=0.6,\n    title=\"Feature Importance\",\n    importance_type=\"gain\",\n    max_num_features=15,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fairness assessment of unmitigated model\n========================================\n\nNow that we have trained our initial fairness-unaware model, let\\'s\nperform our fairness assessment for this model. When conducting a\nfairness assessment, there are three main steps we want to perform:\n\n1.  Identify who will be harmed.\n2.  Identify the types of harms we anticipate.\n3.  Define fairness metrics based on the anticipated harms.\n\nWho will be harmed?\n-------------------\n\nBased on the incident with *Apple* credit card mentioned at the\nbeginning of this notebook, we believe the model may incorrectly predict\nwomen will default on the credit loan. The system may unfairly allocate\nless loans to women and over-allocate loans to men.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Types of harm experienced\n=========================\n\nWhen discussing fairness in AI systems, the first step is understanding\nwhat types of harms we anticipate the system may produce. Using the\n`harms taxonomy in the Fairlearn User Guide <types_of_harms>`{.interpreted-text\nrole=\"ref\"}, we expect this system to produce *harms of allocation*. In\naddition, we also anticipate the long-term impact on an individual\\'s\ncredit score if an individual is unable to repay a loan they receive or\nif they are rejected for a loan application. A *harm of allocation*\noccurs when an AI system extends or withholds resources, opportunities,\ninformation. In this scenario, the AI system is extending or withholding\nfinancial assets from individuals. A review of historical incidents\nshows these types of automated lending decision systems may discriminate\nunfairly based on sex.\n\n**Negative impact of credit score**\n\nA secondary harm that is somewhat unique to credit lending decisions is\nthe long-term impact on an individual\\'s credit score. In the United\nStates, a [FICO credit\nscore](https://www.investopedia.com/terms/c/credit_score.asp) is a\nnumber between 300 and 850 that represents a customer\\'s\n*creditworthiness*. An applicant\\'s *credit score* is used by many\nfinancial institutions for lending decisions. An applicant\\'s *credit\nscore* usually increases after a successful repayment of a loan and\ndecreases if the applicant fails to repay the loan.\n\nWhen applying for a credit loan, there are three major outcomes:\n\n1.  The individual receives the credit loan and pays back the loan. In\n    this scenario, we expect the individual\\'s credit score to increase\n    as a result of the successful repayment of the loan.\n2.  The individual receives the credit loan but defaults on the loan. In\n    this scenario, the individual\\'s credit score will drop drastically\n    due to the failure to repay the loan. In the modeling process, this\n    outcome is tied to a **false negative** (the model predicts the\n    individual will repay the loan, but the individual is unsuccessful\n    in doing so).\n3.  In certain countries, such as the United States, an individual\n    receives a small drop (up to five points) to their credit score\n    after a lender performs a *hard inquiry* on the applicant\\'s credit\n    history. If the applicant applies for a loan but does not receive\n    it, the small decrease in their credit score will impact their\n    ability to successfully apply for a future loan. In the modeling\n    process, this outcome is tied to the **selection rate** (the\n    proportion of positive predictions outputted by the model).\n\n**Prevention of wealth accumulation**\n\nOne other type of harm we anticipate in this scenario is the long-term\neffects of *denying loans to applicants who would have successfully paid\nback the loan*. By receiving a loan, an applicant is able to purchase a\nhome, start a business, or pursue some other economic activity that they\nare not able to do otherwise. These outcomes are tied to **false\npositive error** rates in which the model predicts an applicant will\ndefault on the loan, but the individual would have successfully paid the\nloan back. In the United States, the practice of redlining\n`peyton2020redlining`{.interpreted-text role=\"footcite\"}, denying\nmortgage loans and other financial services to predominantly Black or\nother minority communites, has resulted in a vast racial wealth gap\nbetween white and Black Americans. Although the practice of redlining\nwas banned in 1968 with the *Fair Housing Act*, the long-term impact of\nthese practices `jan2018redlining`{.interpreted-text role=\"footcite\"} is\nreflected in the lack of economic investment in Black communities, and\nBlack applicants are denied loans at a higher rate compared to white\nAmericans.\n\nDefine fairness metrics based on harms\n--------------------------------------\n\nNow that we have identified the relevant harms we anticpate users will\nexperience, we can define our fairness metrics. In addition to the\nmetrics, we will quantify the uncertainty around each metric using\n*custom functions* to compute the *standard error* for each metric at\nthe $\\alpha=0.95$ confidence level.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_error_metric(metric_value, sample_size):\n    \"\"\"Compute standard error of a given metric based on the assumption of\n    normal distribution.\n\n    Parameters:\n    metric_value: Value of the metric\n    sample_size: Number of data points associated with the metric\n\n    Returns:\n    The standard error of the metric\n    \"\"\"\n    metric_value = metric_value / sample_size\n    return (\n        1.96\n        * np.sqrt(metric_value * (1.0 - metric_value))\n        / np.sqrt(sample_size)\n    )\n\n\ndef false_positive_error(y_true, y_pred):\n    \"\"\"Compute the standard error for the false positive rate estimate.\"\"\"\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    return compute_error_metric(fp, tn + fp)\n\n\ndef false_negative_error(y_true, y_pred):\n    \"\"\"Compute the standard error for the false negative rate estimate.\"\"\"\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    return compute_error_metric(fn, fn + tp)\n\n\ndef balanced_accuracy_error(y_true, y_pred):\n    \"\"\"Compute the standard error for the balanced accuracy estimate.\"\"\"\n    fpr_error, fnr_error = false_positive_error(\n        y_true, y_pred\n    ), false_negative_error(y_true, y_pred)\n    return np.sqrt(fnr_error**2 + fpr_error**2) / 2\n\n\nfairness_metrics = {\n    \"count\": count,\n    \"balanced_accuracy\": balanced_accuracy_score,\n    \"balanced_acc_error\": balanced_accuracy_error,\n    \"selection_rate\": selection_rate,\n    \"false_positive_rate\": false_positive_rate,\n    \"false_positive_error\": false_positive_error,\n    \"false_negative_rate\": false_negative_rate,\n    \"false_negative_error\": false_negative_error,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select a subset of metrics to report to avoid information overload\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metrics_to_report = [\n    \"balanced_accuracy\",\n    \"false_positive_rate\",\n    \"false_negative_rate\",\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compute the disaggregated performance metrics, we will use the\n`MetricFrame`{.sourceCode} object within the Fairlearn library. We will\npass in our dictionary of metrics `fairness_metrics`{.sourceCode}, along\nwith our test labels `y_test`{.sourceCode} and test predictions\n`Y_pred`{.sourceCode}. In addition, we pass in the *sensitive\\_features*\n`A_test`{.sourceCode} to disaggregate our model results.\n\nInstantiate the MetricFrame for the unmitigated model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metricframe_unmitigated = MetricFrame(\n    metrics=fairness_metrics,\n    y_true=y_test,\n    y_pred=Y_pred,\n    sensitive_features=A_test,\n)\n\nmetricframe_unmitigated.by_group[metrics_to_report]\n\nmetricframe_unmitigated.difference()[metrics_to_report]\n\nmetricframe_unmitigated.overall[metrics_to_report]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_group_metrics_with_error_bars(metricframe, metric, error_name):\n    \"\"\"Plot the disaggregated metric for each group with an associated\n    error bar. Both metric and the error bar are provided as columns in the\n    provided MetricFrame.\n\n    Parameters\n    ----------\n    metricframe : MetricFrame\n        The MetricFrame containing the metrics and their associated\n        uncertainty quantification.\n    metric : str\n        The metric to plot\n    error_name : str\n        The associated standard error for each metric in metric\n\n    Returns\n    -------\n    Matplotlib Plot of point estimates with error bars\n    \"\"\"\n    grouped_metrics = metricframe.by_group\n    point_estimates = grouped_metrics[metric]\n    error_bars = grouped_metrics[error_name]\n    lower_bounds = point_estimates - error_bars\n    upper_bounds = point_estimates + error_bars\n\n    x_axis_names = [\n        str(name) for name in error_bars.index.to_flat_index().tolist()\n    ]\n    plt.vlines(\n        x_axis_names,\n        lower_bounds,\n        upper_bounds,\n        linestyles=\"dashed\",\n        alpha=0.45,\n    )\n    plt.scatter(x_axis_names, point_estimates, s=25)\n    plt.xticks(rotation=0)\n    y_start, y_end = np.round(min(lower_bounds), decimals=2), np.round(\n        max(upper_bounds), decimals=2\n    )\n    plt.yticks(np.arange(y_start, y_end, 0.05))\n    plt.ylabel(metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_group_metrics_with_error_bars(\n    metricframe_unmitigated, \"false_positive_rate\", \"false_positive_error\"\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_group_metrics_with_error_bars(\n    metricframe_unmitigated, \"false_negative_rate\", \"false_negative_error\"\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metricframe_unmitigated.by_group[metrics_to_report].plot.bar(\n    subplots=True, layout=[1, 3], figsize=[12, 4], legend=None, rot=0\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let\\'s compute the `equalized_odds_difference`{.sourceCode} for\nthis unmitigated model. The `equalized_odds_difference`{.sourceCode} is\nthe maximum of the `false_positive_rate_difference`{.sourceCode} and\n`false_negative_rate_difference`{.sourceCode}. In our lending context,\nboth *false\\_negative\\_rate\\_disparities* and\n*false\\_positive\\_rate\\_disparities* result in fairness-related harms.\nTherefore, we attempt to minimize both of these metrics by minimizing\nthe `equalized_odds_difference`{.sourceCode}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "balanced_accuracy_unmitigated = balanced_accuracy_score(y_test, Y_pred)\nequalized_odds_unmitigated = equalized_odds_difference(\n    y_test, Y_pred, sensitive_features=A_test\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One key assumption here is we assume that *false positives* and *false\nnegatives* have the equally adverse costs to each group. In practice, we\nwould develop some weighting mechanism to assign a weight to each *false\nnegative* and *false positive* event.\n\nMitigating Unfairness in ML models\n==================================\n\nIn the previous section, we identified disparities in the model\\'s\nperformance with respect to `SEX`{.sourceCode}. In particular, we found\nthat model produces a significantly higher\n`false_negative_rate`{.sourceCode} and\n`false_positive_rate`{.sourceCode} for the applicants labeled\n`female`{.sourceCode} compared to those labeled `male`{.sourceCode}. In\nthe context of credit decision scenario, this means the model\nunder-allocates loans to *women* who would have paid the loan, but\nover-allocates loans to *women* who go on to default on their loan.\n\nIn this section, we will discuss strategies for mitigating the\nperformance disparities we found in our unmitigated model. We will apply\ntwo different mitigation strategies:\n\n-   *Postprocessing*: In the postprocessing approach, the outputs of a\n    trained classifer are transformed to satisfy some fairness\n    criterion.\n-   *Reductions*: In the reductions approach, we take in a model class\n    and iteratively create a sequence of models that optimize some\n    fairness constraint. Compared to the *postprocessing* approach, the\n    fairness constraint is satisfied during the model training time\n    rather than afterwards.\n\nPostprocessing mitigations: ThresholdOptimizer\n----------------------------------------------\n\nIn the Fairlearn package, *postprocessing* mitigation is offered through\nthe `ThresholdOptimizer`{.sourceCode} algorithm, following\n:footcite`hardt2016equality`{.interpreted-text role=\"cts\"}. The\n`ThresholdOptimizer`{.sourceCode} takes in an exisiting (possibly\npre-fit) machine learning model whose predictions acts as a scoring\nfunction to identify separate thresholds for each *sensitive feature*\ngroup. The `ThresholdOptimizer`{.sourceCode} optimizes a specified\nobjective metric (in our case, `balanced_accuracy`{.sourceCode}) subject\nto some fairness constraint ([equalized\\_odds]{.title-ref}), resulting\nin a thresholded version of the underlying machine learning model.\n\nTo instantiate our `ThresholdOptimizer`{.sourceCode}, we need to specify\nour fairness constraint as a model parameter. Because both\n`false_negative_rate`{.sourceCode} disparities and\n`false_positive_rate`{.sourceCode} disparities translate into real-world\nharms in our scenario, we will aim to minimize the\n`equalized_odds`{.sourceCode} difference as our *fairness constraint*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "postprocess_est = ThresholdOptimizer(\n    estimator=estimator,\n    constraints=\"equalized_odds\",  # Optimize FPR and FNR simultaneously\n    objective=\"balanced_accuracy_score\",\n    prefit=True,\n    predict_method=\"predict_proba\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One key limitation of the `ThresholdOptimizer`{.sourceCode} is the need\nfor sensitive features during training and prediction time. If we do not\nhave access to the `sensitive_features`{.sourceCode} during prediction\ntime, we cannot use the `ThresholdOptimizer`{.sourceCode}.\n\nWe pass in `A_train`{.sourceCode} to the `fit`{.sourceCode} function\nwith the `sensitive_features`{.sourceCode} parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "postprocess_est.fit(X=X_train, y=y_train, sensitive_features=A_train)\n\npostprocess_pred = postprocess_est.predict(X_test, sensitive_features=A_test)\n\npostprocess_pred_proba = postprocess_est._pmf_predict(\n    X_test, sensitive_features=A_test\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fairness assessment of postprocessing model\n===========================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compare_metricframe_results(mframe_1, mframe_2, metrics, names):\n    \"\"\"Concatenate the results of two MetricFrames along a subset of metrics.\n\n    Parameters\n    ----------\n    mframe_1: First MetricFrame for comparison\n    mframe_2: Second MetricFrame for comparison\n    metrics: The subset of metrics for comparison\n    names: The names of the selected metrics\n\n    Returns\n    -------\n    MetricFrame : MetricFrame\n        The concatenation of the two MetricFrames, restricted to the metrics\n        specified.\n\n    \"\"\"\n    return pd.concat(\n        [mframe_1.by_group[metrics], mframe_2.by_group[metrics]],\n        keys=names,\n        axis=1,\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bal_acc_postprocess = balanced_accuracy_score(y_test, postprocess_pred)\neq_odds_postprocess = equalized_odds_difference(\n    y_test, postprocess_pred, sensitive_features=A_test\n)\n\nmetricframe_postprocess = MetricFrame(\n    metrics=fairness_metrics,\n    y_true=y_test,\n    y_pred=postprocess_pred,\n    sensitive_features=A_test,\n)\n\nmetricframe_postprocess.overall[metrics_to_report]\n\nmetricframe_postprocess.difference()[metrics_to_report]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let\\'s compare the performance of our *thresholded* classifier with\nthe original *unmitigated* model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "compare_metricframe_results(\n    metricframe_unmitigated,\n    metricframe_postprocess,\n    metrics=metrics_to_report,\n    names=[\"Unmitigated\", \"PostProcess\"],\n)\n\nmetricframe_postprocess.by_group[metrics_to_report].plot.bar(\n    subplots=True, layout=[1, 3], figsize=[12, 4], legend=None, rot=0\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the `ThresholdOptimizer`{.sourceCode} algorithm achieves a\nmuch lower disparity between the two groups compared to the\n*unmitigated* model. However, this does come with the trade-off that the\n`ThresholdOptimizer`{.sourceCode} achieves a lower\n`balanced_accuracy`{.sourceCode} score for *male* applicants.\n\nReductions approach to unfairness mitigation\n============================================\n\nIn the previous section, we took a fairness-unaware model and used the\n`ThresholdOptimizer`{.sourceCode} to transform the model\\'s decision\nboundary to satisfy our fairness constraints. One key limitation of the\n`ThresholdOptimizer`{.sourceCode} is needing access to our\n*sensitive\\_feature* during prediction time.\n\nIn this section, we will use the *reductions* approach of Agarwal et. al\n(2018) `agarwal2018reductions`{.interpreted-text role=\"footcite\"} to\nproduce models that satisfy the fairness constraint without needing\naccess to the sensitive features at deployment time.\n\nThe main reduction algorithm in Fairlearn is\n`ExponentiatedGradient`{.sourceCode}. The algorithm creates a sequence\nof re-weighted datasets and retrains the wrapped classifier on each of\nthe datasets. This re-training process is guaranteed to find a model\nthat satisfies the fairness constraints while optimizing the performance\nmetric.\n\nThe model returned by `ExponentiatedGradient`{.sourceCode} consists of\nseveral inner models, returned by a wrapped estimator.\n\nTo instantiate an `ExponentiatedGradient`{.sourceCode} model, we pass in\ntwo parameters:\n\n-   a base `estimator`{.sourceCode} (object that supports training)\n-   fairness `constraints`{.sourceCode} (object of type\n    `fairlearn.reductions.Moment`{.interpreted-text role=\"class\"})\n\nWhen passing in a fairness *constraint* as a `Moment`{.sourceCode}, we\ncan specify an `epsilon`{.sourceCode} value representing the maximum\nallowed difference or ratio between our largest and smallest value. For\nexample, in the below code,\n`EqualizedOdds(difference_bound=epsilon)`{.sourceCode} means that we are\nusing `EqualizedOdds`{.sourceCode} as our fairness constraint, and we\nwill allow a maximal difference of `epsilon`{.sourceCode} between our\nlargest and smallest *equalized odds* value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_expgrad_models_per_epsilon(\n    estimator, epsilon, X_train, y_train, A_train\n):\n    \"\"\"Instantiate and train an ExponentiatedGradient model on the\n    balanced training dataset.\n\n    Parameters\n    ----------\n    Estimator: Base estimator to contains a fit and predict function.\n    Epsilon: Float representing maximum difference bound for the fairness Moment constraint\n\n    Returns\n    -------\n    Predictors\n        List of inner model predictors learned by the ExponentiatedGradient\n        model during the training process.\n\n    \"\"\"\n    exp_grad_est = ExponentiatedGradient(\n        estimator=estimator,\n        sample_weight_name='classifier__sample_weight',\n        constraints=EqualizedOdds(difference_bound=epsilon),\n    )\n    # Is this an issue - Re-runs\n    exp_grad_est.fit(X_train, y_train, sensitive_features=A_train)\n    predictors = exp_grad_est.predictors_\n    return predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the *performance-fairness trade-off* learned by the\n`ExponentiatedGradient`{.sourceCode} model is sensitive to our chosen\n`epsilon`{.sourceCode} value, we can treat `epsilon`{.sourceCode} as a\n*hyperparameter* and iterate over a range of potential values. Here, we\nwill train two `ExponentiatedGradient`{.sourceCode} models, one with\n`epsilon=0.01`{.sourceCode} and the second with\n`epsilon=0.02`{.sourceCode}, and store the inner models learned through\neach of the training processes.\n\nIn practice, we recommend choosing smaller values of\n`epsilon`{.sourceCode} on the order of the *square root* of the number\nof samples in the training dataset:\n$\\dfrac{1}{\\sqrt{\\text{numberSamples}}} \\approx \\dfrac{1}{\\sqrt{25000}} \\approx 0.01$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epsilons = [0.01, 0.02]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_models = {}\nfor eps in epsilons:\n    all_models[eps] = get_expgrad_models_per_epsilon(\n        estimator=estimator,\n        epsilon=eps,\n        X_train=X_train,\n        y_train=y_train,\n        A_train=A_train,\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epsilon, models in all_models.items():\n    print(\n        f\"For epsilon {epsilon}, ExponentiatedGradient learned {len(models)} inner models\"\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we can see all the inner models learned for each value of\n`epsilon`{.sourceCode}. With the `ExponentiatedGradient`{.sourceCode}\nmodel, we specify an `epsilon`{.sourceCode} parameter that represents\nthe maximal disparity in our fairness metric that our final model should\nsatisfy. For example, an `epsilon=0.02`{.sourceCode} means that the\ntraining value of the *equalized odds difference* of the returned model\nis at most `0.02`{.sourceCode} (if the algorithm converges).\n\nReviewing inner models of ExponentiatedGradient\n===============================================\n\nIn many situations due to regulation or other technical restrictions,\nthe randomized nature of `ExponentiatedGradient`{.sourceCode} algorithm\nmay be undesirable. In addition, the multiple inner models of the\nalgorithm introduce challenges for model interpretability. One potential\nworkaround to avoid these issues is to select one of the inner models\nand deploy it instead.\n\nIn the previous section, we trained multiple\n`ExponentiatedGradient`{.sourceCode} models at different\n`epsilon`{.sourceCode} levels and collected all the inner models learned\nby this process. When picking a suitable inner model, we consider\ntrade-offs between our two metrics of interest: *balanced error rate*\nand *equalized odds difference*. Since our focus is on these two\nmetrics, we will filter out the models that are outperformed in both of\nthe metrics by some other model (we refer to these as *\\\"dominated\\\"*\nmodels), and plot just the remaining *\\\"undominated\\\"* models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def is_pareto_efficient(points):\n    \"\"\"Filter a NumPy Matrix to remove rows that are strictly dominated by\n    another row in the matrix. Strictly dominated means the all the row values\n    are greater than the values of another row.\n\n    Parameters\n    ----------\n    Points: NumPy array (NxM) of model metrics.\n        Assumption that smaller values for metrics are preferred.\n\n    Returns\n    -------\n    Boolean Array\n        Nx1 boolean mask representing the non-dominated indices.\n    \"\"\"\n    n, m = points.shape\n    is_efficient = np.ones(n, dtype=bool)\n    for i, c in enumerate(points):\n        if is_efficient[i]:\n            is_efficient[is_efficient] = np.any(\n                points[is_efficient] < c, axis=1\n            )\n            is_efficient[i] = True\n    return is_efficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def filter_dominated_rows(points):\n    \"\"\"Remove rows from a DataFrame that are monotonically dominated by\n    another row in the DataFrame.\n\n    Parameters\n    ----------\n    Points: DataFrame where each row represents the summarized performance\n            (balanced accuracy, fairness metric) of an inner model.\n\n    Returns\n    -------\n    pareto mask: Boolean mask representing indices of input DataFrame that are not monotonically dominated.\n    masked_DataFrame: DataFrame with dominated rows filtered out.\n\n    \"\"\"\n    pareto_mask = is_pareto_efficient(points.to_numpy())\n    return pareto_mask, points.loc[pareto_mask, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def aggregate_predictor_performances(\n    predictors, metric, X_test, Y_test, A_test=None\n):\n    \"\"\"Compute the specified metric for all classifiers in predictors.\n    If no sensitive features are present, the metric is computed without\n    disaggregation.\n\n    Parameters\n    ----------\n    predictors: A set of classifiers to generate predictions from.\n    metric: The metric (callable) to compute for each classifier in predictor\n    X_test: The data features of the testing data set\n    Y_test: The target labels of the teting data set\n    A_test: The sensitive feature of the testing data set.\n\n    Returns\n    -------\n    List of performance scores for each classifier in predictors, for the\n    given metric.\n    \"\"\"\n    all_predictions = [predictor.predict(X_test) for predictor in predictors]\n    if A_test is not None:\n        return [\n            metric(Y_test, Y_sweep, sensitive_features=A_test)\n            for Y_sweep in all_predictions\n        ]\n    else:\n        return [metric(Y_test, Y_sweep) for Y_sweep in all_predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def model_performance_sweep(models_dict, X_test, y_test, A_test):\n    \"\"\"Compute the equalized_odds_difference and balanced_error_rate for a\n    given list of inner models learned by the ExponentiatedGradient algorithm.\n    Return a DataFrame containing the epsilon level of the model, the index\n    of the model, the equalized_odds_difference score and the balanced_error\n    for the model.\n\n    Parameters\n    ----------\n    models_dict: Dictionary mapping model ids to a model.\n    X_test: The data features of the testing data set\n    y_test: The target labels of the testing data set\n    A_test: The sensitive feature of the testing data set.\n\n    Returns\n    -------\n    DataFrame where each row represents a model (epsilon, index) and its\n    performance metrics\n    \"\"\"\n    performances = []\n    for (eps, models) in models_dict.items():\n        eq_odds_difference = aggregate_predictor_performances(\n            models, equalized_odds_difference, X_test, y_test, A_test\n        )\n        bal_acc_score = aggregate_predictor_performances(\n            models, balanced_accuracy_score, X_test, y_test\n        )\n        for (i, score) in enumerate(eq_odds_difference):\n            performances.append((eps, i, score, (1 - bal_acc_score[i])))\n    performances_df = pd.DataFrame.from_records(\n        performances,\n        columns=[\"epsilon\", \"index\", \"equalized_odds\", \"balanced_error\"],\n    )\n    return performances_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "performance_df = model_performance_sweep(all_models, X_test, y_test, A_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "performance_subset = performance_df.loc[\n    :, [\"equalized_odds\", \"balanced_error\"]\n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask, pareto_subset = filter_dominated_rows(performance_subset)\n\nperformance_df_masked = performance_df.loc[mask, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let\\'s plot the performance trade-offs between all of our models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for index, row in performance_df_masked.iterrows():\n    bal_error, eq_odds_diff = row[\"balanced_error\"], row[\"equalized_odds\"]\n    epsilon_, index_ = row[\"epsilon\"], row[\"index\"]\n    plt.scatter(\n        bal_error, eq_odds_diff, color=\"green\", label=\"ExponentiatedGradient\"\n    )\n    plt.text(\n        bal_error + 0.001,\n        eq_odds_diff + 0.0001,\n        f\"Eps: {epsilon_}, Idx: {int(index_)}\",\n        fontsize=10,\n    )\nplt.scatter(\n    1.0 - balanced_accuracy_unmitigated,\n    equalized_odds_unmitigated,\n    label=\"UnmitigatedModel\",\n)\nplt.scatter(\n    1.0 - bal_acc_postprocess, eq_odds_postprocess, label=\"PostProcess\"\n)\nplt.xlabel(\"Weighted Error Rate\")\nplt.ylabel(\"Equalized Odds\")\nplt.legend(bbox_to_anchor=(1.85, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the above plot, we can see how the performance of the non-dominated\ninner models compares to the original unmitigated model. In many cases,\nwe see that a reduction in the `equalized_odds_difference`{.sourceCode}\nis accompanied by a small increase in the *weighted error rate*.\n\nSelecting a suitable inner model\n================================\n\nOne strategy we can use to select a model is creating a *threshold*\nbased on the *balanced error rate* of the unmitigated model. Then out of\nthe filtered models, we select the model that minimizes the\n`equalized_odds_difference`{.sourceCode}. The process can be broken down\ninto the three steps below:\n\n1.  Create threshold based on `balanced_error`{.sourceCode} of the\n    unmitigated model.\n2.  Filter only models whose `balanced_error`{.sourceCode} are below the\n    threshold.\n3.  Choose the model with smallest `equalized_odds`{.sourceCode}\n    difference.\n\nWithin the context of fair lending in the United States, if a financial\ninstitution is found to be engaging in discriminatory behavior, they\nmust produce documentation that demonstrates the model chosen is the\nleast discriminatory model while satisfying profitability and other\nbusiness needs. In our approach, the business need of profitability is\nsimulated by thresholding based on the `balanced_error`{.sourceCode}\nrate of the unmitigated model, and we choose the least discriminatory\nmodel based on the smallest `equalized_odds_difference`{.sourceCode}\nvalue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def filter_models_by_unmitigiated_score(\n    all_models,\n    models_frames,\n    unmitigated_score,\n    performance_metric=\"balanced_error\",\n    fairness_metric=\"equalized_odds\",\n    threshold=0.01,\n):\n    \"\"\"Filter out models whose performance score is above the desired\n    threshold. Out of the remaining model, return the models with the best\n    score on the fairness metric.\n\n    Parameters\n    ----------\n    all_models: Dictionary (Epsilon, Index) mapping (epilson, index number) pairs to a Model object\n    models_frames: A DataFrame representing each model's performance and fairness score.\n    unmitigated_score: The performance score of the unmitigated model.\n    performance_metric: The model performance metric to threshold on.\n    fairness_metric: The fairness metric to optimize for\n    threshold: The threshold padding added to the :code:`unmitigated_score`.\n\n    \"\"\"\n    # Create threshold based on balanced_error of unmitigated model and filter\n    models_filtered = models_frames.query(\n        f\"{performance_metric} <= {unmitigated_score + threshold}\"\n    )\n    best_row = models_filtered.sort_values(by=[fairness_metric]).iloc[0]\n    # Choose the model with smallest equalized_odds difference\n    epsilon, index = best_row[[\"epsilon\", \"index\"]]\n    return {\n        \"model\": all_models[epsilon][index],\n        \"epsilon\": epsilon,\n        \"index\": index,\n    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_model = filter_models_by_unmitigiated_score(\n    all_models,\n    models_frames=performance_df,\n    unmitigated_score=(1.0 - balanced_accuracy_unmitigated),\n    threshold=0.015,\n)\n\nprint(\n    f\"Epsilon for best model: {best_model.get('epsilon')}, Index number: {best_model.get('index')}\"\n)\ninprocess_model = best_model.get(\"model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have selected our best inner model, let\\'s collect the model\\'s\npredictions on the test dataset and compute the relevant performance\nmetrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_pred_inprocess = inprocess_model.predict(X_test)\n\nbal_acc_inprocess = balanced_accuracy_score(y_test, y_pred_inprocess)\neq_odds_inprocess = equalized_odds_difference(\n    y_test, y_pred_inprocess, sensitive_features=A_test\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metricframe_inprocess = MetricFrame(\n    metrics=fairness_metrics,\n    y_true=y_test,\n    y_pred=y_pred_inprocess,\n    sensitive_features=A_test,\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metricframe_inprocess.difference()[metrics_to_report]\n\nmetricframe_inprocess.overall[metrics_to_report]\n\nmetricframe_inprocess.by_group[metrics_to_report].plot.bar(\n    subplots=True, layout=[1, 3], figsize=[12, 4], legend=None, rot=0\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discuss Performance and Trade-Offs\n==================================\n\nNow we have trained two different fairness-aware models using the\n*postprocessing* approach and the *reductions* approach. Let\\'s compare\nthe performance of these models to our original fairness-unaware model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metric_error_pairs = [\n    (\"balanced_accuracy\", \"balanced_acc_error\"),\n    (\"false_positive_rate\", \"false_positive_error\"),\n    (\"false_negative_rate\", \"false_negative_error\"),\n]\n\n\ndef create_metricframe_w_errors(mframe, metrics_to_report, metric_error_pair):\n    mframe_by_group = mframe.by_group.copy()\n    for (metric_name, error_name) in metric_error_pair:\n        mframe_by_group[metric_name] = mframe_by_group[metric_name].apply(\n            lambda x: f\"{x:.3f}\"\n        )\n        mframe_by_group[error_name] = mframe_by_group[error_name].apply(\n            lambda x: f\"{x:.3f}\"\n        )\n        mframe_by_group[metric_name] = mframe_by_group[metric_name].str.cat(\n            mframe_by_group[error_name], sep=\"\u00b1\"\n        )\n    return mframe_by_group[metrics_to_report]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Report model performance error bars for metrics\n===============================================\n\n**Unmitigated model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "create_metricframe_w_errors(\n    metricframe_unmitigated, metrics_to_report, metric_error_pairs\n)\n\nmetricframe_unmitigated.overall[metrics_to_report]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ExponentiatedGradient model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "create_metricframe_w_errors(\n    metricframe_inprocess, metrics_to_report, metric_error_pairs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ThresholdOptimizer**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metricframe_inprocess.overall[metrics_to_report]\n\ncreate_metricframe_w_errors(\n    metricframe_postprocess, metrics_to_report, metric_error_pairs\n)\n\nmetricframe_postprocess.overall[metrics_to_report]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see both of our fairness-aware models yield a slight decrease in the\n*balanced\\_accuracy* for *male applicants* compared to our\nfairness-unaware model. In the *reductions* model, we see a decrease in\nthe *false positive rate* for *female applicants*. This is accompanied\nby an increase in the *false negative rate* for *male applicants*.\nHowever overall, the *equalized odds difference* for the *reductions*\nmodels is lower than that of the original fairness-unaware model.\n\nConclusion and Discussion\n=========================\n\nIn this case study, we walked through the process of assessing a credit\ndecision model for gender-related performance disparities. Our analysis\nfollows closely the work done in the Microsoft/EY white paper\n`dudik2020assessing`{.interpreted-text role=\"footcite\"} where they used\nthe *Fairlearn* toolkit to perform an audit of a fairness-unaware\ntree-based model. We applied a *postprocessing* and *reductions*\nmitigation techniques to mitigate the *equalized odds difference* in our\nmodel.\n\nThrough the *reductions* process, we generated a model that reduces the\n*equalized odds difference* of the original model without a drastic\nincrease in the *balanced error score*. If this were a real model being\ndeveloped a financial institution, the *balanced error score* would be a\nproxy for the profitability of the model. By maintaining a relatively\nsimilar *balanced error score*, we\\'ve produced a model that preserves\nprofitability to the firm while producing more fair and equitable\noutcomes for women in this scenario.\n\nDesigning a Model Card\n----------------------\n\nA key facet of Responsible Machine Learning is responsible documentation\npractices. :footcite`mitchell2019model`{.interpreted-text role=\"ct\"}\nproposed the model card framework for documentating and reporting model\ntraining details and deployment considerations.\n\nA *model card* contains sections for documenting training and evaluation\ndataset descriptions, ethical concerns, and quantitative evaluation\nsummaries. In practice, we would ideally create a model card for our\nmodel before deploying it in production. Although we will not be\nproducing a model card in this case study, interested readers can learn\nmore about creating model cards using the *Model Card Toolkit* from the\n[Fairlearn PyCon\ntutorial](https://github.com/fairlearn/talks/tree/main/2022_pycon).\n\nFairness under unawareness\n--------------------------\n\nWhen proving credit models are compliant with fair lending laws,\npractitoners may run into the issue of not having access to the\nsensitive demographic features. As a result, financial institutions are\noften tasked with proving their models are compliant with fair lending\nlaws by imputing these demographics features. However,\n:footcite`chen2019fairness`{.interpreted-text role=\"ct\"} show these\nimputation methods often introduce new fairness-related issues.\n\nReferences\n==========\n\n::: {.footbibliography}\n:::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}