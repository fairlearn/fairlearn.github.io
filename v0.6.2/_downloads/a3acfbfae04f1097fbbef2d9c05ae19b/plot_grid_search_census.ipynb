{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GridSearch with Census Data\n===========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook shows how to use Fairlearn and the Fairness dashboard to\ngenerate predictors for the Census dataset. This dataset is a\nclassification problem - given a range of data about 32,000 individuals,\npredict whether their annual income is above or below fifty thousand\ndollars per year.\n\nFor the purposes of this notebook, we shall treat this as a loan\ndecision problem. We will pretend that the label indicates whether or\nnot each individual repaid a loan in the past. We will use the data to\ntrain a predictor to predict whether previously unseen individuals will\nrepay a loan or not. The assumption is that the model predictions are\nused to decide whether an individual should be offered a loan.\n\nWe will first train a fairness-unaware predictor and show that it leads\nto unfair decisions under a specific notion of fairness called\n*demographic parity*. We then mitigate unfairness by applying the\n`GridSearch`{.sourceCode} algorithm from the Fairlearn package.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and preprocess the data set\n================================\n\nWe download the data set using [fetch\\_adult]{.title-ref} function in\n[fairlearn.datasets]{.title-ref}. We start by importing the various\nmodules we\\'re going to use:\n\n::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nThe `FairlearnDashboard`{.sourceCode} is no longer being developed as\npart of Fairlearn. The widget itself has been moved to [the raiwidgets\npackage](https://pypi.org/project/raiwidgets/). Fairlearn will provide\nsome of the existing functionality through\n`matplotlib`{.sourceCode}-based visualizations.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fairlearn.widget import FairlearnDashboard\nfrom sklearn.model_selection import train_test_split\nfrom fairlearn.reductions import GridSearch\nfrom fairlearn.reductions import DemographicParity, ErrorRate\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nimport pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now load and inspect the data by using the\n[fairlearn.datasets]{.title-ref} module:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\ndata = fetch_openml(data_id=1590, as_frame=True)\nX_raw = data.data\nY = (data.target == '>50K') * 1\nX_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to treat the sex of each individual as a sensitive feature\n(where 0 indicates female and 1 indicates male), and in this particular\ncase we are going separate this feature out and drop it from the main\ndata. We then perform some standard data preprocessing steps to convert\nthe data into a format suitable for the ML algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A = X_raw[\"sex\"]\nX = X_raw.drop(labels=['sex'], axis=1)\nX = pd.get_dummies(X)\n\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n\nle = LabelEncoder()\nY = le.fit_transform(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we split the data into training and test sets:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_scaled,\n                                                                     Y,\n                                                                     A,\n                                                                     test_size=0.2,\n                                                                     random_state=0,\n                                                                     stratify=Y)\n\n# Work around indexing bug\nX_train = X_train.reset_index(drop=True)\nA_train = A_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\nA_test = A_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training a fairness-unaware predictor\n=====================================\n\nTo show the effect of Fairlearn we will first train a standard ML\npredictor that does not incorporate fairness. For speed of\ndemonstration, we use the simple\n`sklearn.linear_model.LogisticRegression`{.interpreted-text\nrole=\"class\"} class:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n\nunmitigated_predictor.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can load this predictor into the Fairness dashboard, and assess its\nfairness:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "FairlearnDashboard(sensitive_features=A_test, sensitive_feature_names=['sex'],\n                   y_true=Y_test,\n                   y_pred={\"unmitigated\": unmitigated_predictor.predict(X_test)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the disparity in accuracy, we see that males have an error\nabout three times greater than the females. More interesting is the\ndisparity in opportunity - males are offered loans at three times the\nrate of females.\n\nDespite the fact that we removed the feature from the training data, our\npredictor still discriminates based on sex. This demonstrates that\nsimply ignoring a sensitive feature when fitting a predictor rarely\neliminates unfairness. There will generally be enough other features\ncorrelated with the removed feature to lead to disparate impact.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mitigation with GridSearch\n==========================\n\nThe `fairlearn.reductions.GridSearch`{.interpreted-text role=\"class\"}\nclass implements a simplified version of the exponentiated gradient\nreduction of [Agarwal et al. 2018](https://arxiv.org/abs/1803.02453).\nThe user supplies a standard ML estimator, which is treated as a\nblackbox. [GridSearch]{.title-ref} works by generating a sequence of\nrelabellings and reweightings, and trains a predictor for each.\n\nFor this example, we specify demographic parity (on the sensitive\nfeature of sex) as the fairness metric. Demographic parity requires that\nindividuals are offered the opportunity (are approved for a loan in this\nexample) independent of membership in the sensitive class (i.e., females\nand males should be offered loans at the same rate). We are using this\nmetric for the sake of simplicity; in general, the appropriate fairness\nmetric will not be obvious.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n                   constraints=DemographicParity(),\n                   grid_size=71)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our algorithms provide `fit()`{.sourceCode} and `predict()`{.sourceCode}\nmethods, so they behave in a similar manner to other ML packages in\nPython. We do however have to specify two extra arguments to\n`fit()`{.sourceCode} - the column of sensitive feature labels, and also\nthe number of predictors to generate in our sweep.\n\nAfter `fit()`{.sourceCode} completes, we extract the full set of\npredictors from the `fairlearn.reductions.GridSearch`{.interpreted-text\nrole=\"class\"} object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sweep.fit(X_train, Y_train,\n          sensitive_features=A_train)\n\npredictors = sweep.predictors_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We could load these predictors into the Fairness dashboard now. However,\nthe plot would be somewhat confusing due to their number. In this case,\nwe are going to remove the predictors which are dominated in the\nerror-disparity space by others from the sweep (note that the disparity\nwill only be calculated for the sensitive feature; other potentially\nsensitive features will not be mitigated). In general, one might not\nwant to do this, since there may be other considerations beyond the\nstrict optimization of error and disparity (of the given sensitive\nfeature).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "errors, disparities = [], []\nfor m in predictors:\n    def classifier(X): return m.predict(X)\n\n    error = ErrorRate()\n    error.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train)\n    disparity = DemographicParity()\n    disparity.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train)\n\n    errors.append(error.gamma(classifier)[0])\n    disparities.append(disparity.gamma(classifier).max())\n\nall_results = pd.DataFrame({\"predictor\": predictors, \"error\": errors, \"disparity\": disparities})\n\nnon_dominated = []\nfor row in all_results.itertuples():\n    errors_for_lower_or_eq_disparity = all_results[\"error\"][all_results[\"disparity\"] <= row.disparity]\n    if row.error <= errors_for_lower_or_eq_disparity.min():\n        non_dominated.append(row.predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can put the dominant models into the Fairness dashboard,\nalong with the unmitigated model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dashboard_predicted = {\"unmitigated\": unmitigated_predictor.predict(X_test)}\nfor i in range(len(non_dominated)):\n    key = \"dominant_model_{0}\".format(i)\n    value = non_dominated[i].predict(X_test)\n    dashboard_predicted[key] = value\n\n\nFairlearnDashboard(sensitive_features=A_test, sensitive_feature_names=['sex'],\n                   y_true=Y_test,\n                   y_pred=dashboard_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see a Pareto front forming - the set of predictors which represent\noptimal tradeoffs between accuracy and disparity in predictions. In the\nideal case, we would have a predictor at (1,0) - perfectly accurate and\nwithout any unfairness under demographic parity (with respect to the\nsensitive feature \\\"sex\\\"). The Pareto front represents the closest we\ncan come to this ideal based on our data and choice of estimator. Note\nthe range of the axes - the disparity axis covers more values than the\naccuracy, so we can reduce disparity substantially for a small loss in\naccuracy.\n\nBy clicking on individual models on the plot, we can inspect their\nmetrics for disparity and accuracy in greater detail. In a real example,\nwe would then pick the model which represented the best trade-off\nbetween accuracy and disparity given the relevant business constraints.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}